{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprint 機械学習スクラッチ ロジスティック回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】仮定関数\n",
    "ロジスティック回帰の仮定関数のメソッドをScratchLogisticRegressionクラスに実装してください。\n",
    "\n",
    "ロジスティック回帰の仮定関数は、線形回帰の仮定関数を **シグモイド関数** に通したものです。シグモイド関数は以下の式で表されます。\n",
    "$$\n",
    "g(z) = \\frac{1}{1+e^{−z}}.\n",
    "$$\n",
    "線形回帰の仮定関数は次の式でした。\n",
    "$$\n",
    "h_\\theta(x) = \\theta^T \\cdot x.\n",
    "$$\n",
    "まとめて書くと、ロジスティック回帰の仮定関数は次のようになります。\n",
    "$$\n",
    "h_\\theta(x) = \\frac{1}{1+e^{−\\theta^T \\cdot x}}.\n",
    "$$\n",
    "$x$ : 特徴量ベクトル\n",
    "\n",
    "$\\theta$ : パラメータ（重み）ベクトル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 4), (100,), (50, 4), (50,), (4,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "# 訓練データを用意、100行4列\n",
    "X_train = np.random.rand(400).reshape(-1, 4)\n",
    "# thetaは未知のパラメーター\n",
    "theta = np.random.randn(4)\n",
    "# 目的変数 0,1の二値数\n",
    "y_train = np.random.binomial(1, 0.5, 100)\n",
    "\n",
    "# 同様な検証データも用意、50行4列\n",
    "X_test = np.random.rand(200).reshape(-1, 4)\n",
    "y_test = np.random.binomial(1, 0.5, 50)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape, theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _logistic_hypothesis(X, theta):\n",
    "    \"\"\"\n",
    "    ロジスティック回帰の仮定関数を計算する\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    theta : 次の形のndarray, shape (n_samples, 1)\n",
    "      係数\n",
    "    Returns\n",
    "    -------\n",
    "      次の形のndarray, shape (n_samples, 1)\n",
    "      ロジスティック回帰の仮定関数による推定結果\n",
    "      線形回帰の仮定関数をシグモイド関数に通したもの\n",
    "    \"\"\"\n",
    "    linear_hypo = np.dot(X, theta)\n",
    "    hypo = 1.0 / (1.0 + np.exp(-linear_hypo))\n",
    "    return hypo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100,), (50,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_logistic_hypothesis(X_train, theta).shape, _logistic_hypothesis(X_test, theta).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】最急降下法\n",
    "最急降下法により学習させる実装を行なってください。以下の式で表されるパラメータの更新式のメソッド_gradient_descentを追加し、fitメソッドから呼び出すようにしてください。\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}\\\\\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m}  \\sum_{i=1}^{m}(h_θ(x^{(i)}) − y^{(i)})x_j^{(i)}  ,j = 0\\\\\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\biggl(\\frac{1}{m}  \\sum_{i=1}^{m}(h_θ(x^{(i)}) − y^{(i)})x_j^{(i)} \\biggr) + \\frac{λ}{m}\\theta_j　 ,j\\geq 1\n",
    "$$\n",
    "$\\alpha$ : 学習率\n",
    "\n",
    "$i$ : サンプルのインデックス\n",
    "\n",
    "$j$ : 特徴量のインデックス\n",
    "\n",
    "$m$ : 入力されるデータの数\n",
    "\n",
    "$h_\\theta()$ : 仮定関数\n",
    "\n",
    "$x$ : 特徴量ベクトル\n",
    "\n",
    "$\\theta$ : パラメータ（重み）ベクトル\n",
    "\n",
    "$x^{(i)}$ : i番目のサンプルの特徴量ベクトル\n",
    "\n",
    "$y^{(i)}$ : i番目のサンプルの正解ラベル\n",
    "\n",
    "$\\theta_j$ : j番目のパラメータ（重み）\n",
    "\n",
    "$λ$ : 正則化パラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gradient_descent(X, y, theta):\n",
    "    \"\"\"\n",
    "    theta : 係数\n",
    "    alpha_ : 学習率\n",
    "    lambda_ : 正則化パラメータ\n",
    "    m : サンプル数\n",
    "    n　: 特徴量数\n",
    "    \"\"\"\n",
    "    alpha_ = 0.1\n",
    "    lambda_ = 0.2\n",
    "    m, n = X.shape\n",
    "    theta1 = np.dot(_logistic_hypothesis(X, theta) - y, X)\n",
    "    theta2 = lambda_ * theta\n",
    "    # biasを入れない場合はtheta2[-1] = 0　の処理を追加\n",
    "    theta -= (alpha_ / m) * (theta1 + theta2)\n",
    "    print(theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "_gradient_descent(X_train, y_train, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】推定\n",
    "\n",
    "推定する仕組みを実装してください。ScratchLogisticRegressionクラスの雛形に含まれるpredictメソッドとpredict_probaメソッドに書き加えてください。\n",
    "\n",
    "仮定関数 $h_\\theta(x)$ の出力がpredict_probaの返り値、さらにその値に閾値を設けて1と0のラベルとしたものがpredictの返り値となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 下のScratchLogisticRegressionクラスに追加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】目的関数\n",
    "以下の数式で表されるロジスティック回帰の **目的関数（損失関数）** を実装してください。そして、これを`self.loss`, `self.val_loss`に記録するようにしてください。\n",
    "\n",
    "なお、この数式には正則化項が含まれています。\n",
    "$$\n",
    "J(\\theta)=  \\frac{1}{m}  \\sum_{i=1}^{m}[−y^{(i)} log(h_θ(x^{(i)})) − (1−y^{(i)}) log(1−h_θ(x^{(i)}))] +\n",
    "\\frac{λ}{2m}\\sum_{j=1}^n\n",
    "θ^2_j.\\\\\n",
    "$$\n",
    "$m$ : 入力されるデータの数\n",
    "\n",
    "$h_\\theta()$ : 仮定関数\n",
    "\n",
    "$x$ : 特徴量ベクトル\n",
    "\n",
    "$\\theta$ : パラメータ（重み）ベクトル\n",
    "\n",
    "$x^{(i)}$ : i番目のサンプルの特徴量ベクトル\n",
    "\n",
    "$y^{(i)}$ : i番目のサンプルの正解ラベル\n",
    "\n",
    "$\\theta_j$ : j番目のパラメータ（重み）\n",
    "\n",
    "$n$ : 特徴量の数\n",
    "\n",
    "$λ$ : 正則化パラメータ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 下のScratchLogisticRegressionクラスに追加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装したスクラッチ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLogisticRegression():\n",
    "    import numpy as np\n",
    "    \"\"\"\n",
    "    ロジスティック回帰のスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    C : int or float ( >0)\n",
    "    lambda_ : Cの逆数\n",
    "      正則化のパラメーター\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self._theta : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ（self.coef_　と　self.intercept_からなる）\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "    self.intercept_ : int or float\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      訓練データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証データに対する損失の記録\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_iter, lr, C=1, no_bias=False, verbose=False, random_state=False):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.C = C\n",
    "        self.lambda_ = 1 / self.C\n",
    "        self.no_bias = no_bias\n",
    "        self.verbose = verbose\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        # 乱数のコントロール(int)\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def _logistic_hypothesis(self, X, theta):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰の仮定関数を計算する\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          訓練データ\n",
    "        theta : 次の形のndarray, shape (n_samples, 1)\n",
    "          係数\n",
    "        Returns\n",
    "        -------\n",
    "          次の形のndarray, shape (n_samples, 1)\n",
    "          ロジスティック回帰の仮定関数による推定結果\n",
    "          線形回帰の仮定関数をシグモイド関数に通したもの\n",
    "        \"\"\"\n",
    "        linear_hypo = np.dot(X, theta)\n",
    "        hypo = 1.0 / (1.0 + np.exp(-linear_hypo))\n",
    "        return hypo\n",
    "\n",
    "    def _gradient_descent(self, X, y):\n",
    "        \"\"\"\n",
    "        self.m : サンプル数\n",
    "        self._theta　: パラメーター（係数, 切片）\n",
    "        \"\"\"\n",
    "        theta1 = np.dot(_logistic_hypothesis(X, self._theta) - y, X)\n",
    "        theta2 = self.lambda_ * self._theta    \n",
    "        if self.no_bias == False:\n",
    "            theta2[-1] = 0\n",
    "        self._theta -= (self.lr / self.m) * (theta1 + theta2)\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        \n",
    "        # 変数情報を管理\n",
    "        self.X = X\n",
    "        self.m, self.n = self.X.shape\n",
    "\n",
    "        # 切片を計算する場合はXの配列の最後の列に１を追加, nの数を１追加\n",
    "        if self.no_bias is False:\n",
    "            self.n += 1\n",
    "            self.X = np.concatenate([self.X, np.ones(self.m).reshape(self.m, -1)], axis=1)\n",
    "\n",
    "        # fitが呼び出されるとtheta（係数と切片）を初期化\n",
    "        # random_stateが設定されている場合はseed設定\n",
    "        if type(self.random_state) == int:\n",
    "            np.random.seed(self.random_state)\n",
    "        self._theta = np.random.randn(self.n)\n",
    "\n",
    "        # 検証データが訓練データと同様の特徴量数（n)を持つ場合は同様の学習、予測処理を行わせたいため、flagを立てる\n",
    "        val_flag = True if type(X_val) is np.ndarray and X_val.shape[1]==X.shape[1] else False\n",
    "        # iterの回数探索を繰り返す\n",
    "        for i in range(self.iter):\n",
    "            \n",
    "            # 　self._thetaの更新\n",
    "            self._gradient_descent(self.X, y)     \n",
    "            \n",
    "            # 訓練データの予測確率、損失関数の計算し記録\n",
    "            y_pred_proba = self.predict_proba(X)\n",
    "            self.loss[i] = self._loss_func(y_pred_proba, y)\n",
    "            \n",
    "            # val_dataがある時は同様の処理\n",
    "            if val_flag:\n",
    "                y_val_pred_proba = self.predict_proba(X_val)\n",
    "                self.val_loss[i] = self._loss_func(y_val_pred_proba, y_val)\n",
    "            \n",
    "            #verboseをTrueにした際は学習過程を出力\n",
    "            if self.verbose is True:\n",
    "                print('iter:{}'.format(i+1))\n",
    "                if self.no_bias is False:\n",
    "                    print('coef_:{}'.format(self._theta[:self.n-1]))\n",
    "                    print('intercept_:{}'.format(self._theta[-1]))\n",
    "                    print('train_loss:{}'.format(self.loss[i]))\n",
    "                else:\n",
    "                    print('coef_:{}'.format(self._theta))\n",
    "                    print('train_loss:{}'.format(self.loss[i]))\n",
    "                if val_flag:\n",
    "                    print('val_loss:{}'.format(self.val_loss[i]))\n",
    "            \n",
    "        # iter回の探索後、係数と切片の変数に分ける\n",
    "        if self.no_bias is False:\n",
    "            self.coef_, self.intercept_ = self._theta[:self.n-1], self._theta[-1]\n",
    "        else:\n",
    "            self.coef_ = self._theta\n",
    "            self.intercept_ = None\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰を使いラベルを推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "        threshhold : 閾値 float\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            ロジスティック回帰による推定結果\n",
    "        \"\"\"\n",
    "        # fit　が呼び出されていない場合は　エラー\n",
    "        if self._theta is False:\n",
    "            print(\"It hasn't been fitted yet\")\n",
    "        else:\n",
    "            m, n = X.shape\n",
    "            #　切片を計算している場合はX の最後の列に１を追加\n",
    "            if self.no_bias is False:\n",
    "                X = np.concatenate([X, np.ones(m).reshape(m, -1)], axis=1)\n",
    "            y_pred_proba = self._logistic_hypothesis(X, self._theta)\n",
    "            y_pred = np.where(y_pred_proba > threshold, 1, 0)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰を使い確率を推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            ロジスティック回帰による推定結果\n",
    "        \"\"\"\n",
    "        # fit　が呼び出されていない場合は　エラー\n",
    "        if self._theta is False:\n",
    "            print(\"It hasn't been fitted yet\")\n",
    "        else:\n",
    "            m, n = X.shape\n",
    "            #　切片を計算している場合はX の最後の列に１を追加\n",
    "            if self.no_bias is False:\n",
    "                X = np.concatenate([X, np.ones(m).reshape(m, -1)], axis=1)\n",
    "            y_pred_proba = self._logistic_hypothesis(X, self._theta)\n",
    "            return y_pred_proba\n",
    "\n",
    "    def _loss_func(self, y_pred_proba, y):\n",
    "        \"\"\"\n",
    "        目的関数の計算\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : 次の形のndarray, shape (n_samples,)\n",
    "        推定した値\n",
    "        y : 次の形のndarray, shape (n_samples,)\n",
    "        正解値\n",
    "        Returns\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        # m（行数）をyから取得\n",
    "        m = y_pred_proba.shape[0]\n",
    "        # n（列数）はbias有では+1されているので場合わけ\n",
    "        n = self.n-1 if self.no_bias is False else self.n\n",
    "        a = (y * np.log(y_pred_proba))\n",
    "        b = ((1 - y) * np.log(1 - y_pred_proba))\n",
    "        c = np.power(self._theta[:n], 2).sum() * self.lambda_ / 2\n",
    "        # print('y:{}'.format(y))\n",
    "        # print('y_pred_proba:{}'.format(y_pred_proba))\n",
    "        # print(a, b, c)\n",
    "        loss_value = -((y * np.log(y_pred_proba)).sum()+ ((1 - y) * np.log(1 - y_pred_proba)).sum() - np.power(self._theta[:n], 2).sum() * self.lambda_ / 2) / m\n",
    "        return loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】学習と推定\n",
    "機械学習スクラッチ入門のSprintで用意したirisデータセットのvirgicolorとvirginicaの2値分類に対してスクラッチ実装の学習と推定を行なってください。\n",
    "\n",
    "scikit-learnによる実装と比べ、正しく動いているかを確認してください。\n",
    "\n",
    "AccuracyやPrecision、Recallなどの指標値はscikit-learnを使用してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']),\n",
       " array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "iris_data = load_iris()\n",
    "iris_data.keys(), iris_data.target_names, iris_data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>Species</th>\n",
       "      <th>Species_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width  Species  \\\n",
       "0             5.1          3.5           1.4          0.2        0   \n",
       "1             4.9          3.0           1.4          0.2        0   \n",
       "2             4.7          3.2           1.3          0.2        0   \n",
       "3             4.6          3.1           1.5          0.2        0   \n",
       "4             5.0          3.6           1.4          0.2        0   \n",
       "..            ...          ...           ...          ...      ...   \n",
       "145           6.7          3.0           5.2          2.3        2   \n",
       "146           6.3          2.5           5.0          1.9        2   \n",
       "147           6.5          3.0           5.2          2.0        2   \n",
       "148           6.2          3.4           5.4          2.3        2   \n",
       "149           5.9          3.0           5.1          1.8        2   \n",
       "\n",
       "    Species_name  \n",
       "0         setosa  \n",
       "1         setosa  \n",
       "2         setosa  \n",
       "3         setosa  \n",
       "4         setosa  \n",
       "..           ...  \n",
       "145    virginica  \n",
       "146    virginica  \n",
       "147    virginica  \n",
       "148    virginica  \n",
       "149    virginica  \n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "df_X = pd.DataFrame(iris_data.data, columns=feature_names)\n",
    "df_y = pd.DataFrame(iris_data.target, columns=['Species'])\n",
    "df = pd.concat([df_X, df_y], axis=1)\n",
    "df['Species_name'] = iris_data.target_names[df['Species']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>Species</th>\n",
       "      <th>Species_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>7.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>6.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6.9</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>5.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>6.5</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  petal_length  Species Species_name\n",
       "50            7.0           4.7        1   versicolor\n",
       "51            6.4           4.5        1   versicolor\n",
       "52            6.9           4.9        1   versicolor\n",
       "53            5.5           4.0        1   versicolor\n",
       "54            6.5           4.6        1   versicolor\n",
       "..            ...           ...      ...          ...\n",
       "145           6.7           5.2        2    virginica\n",
       "146           6.3           5.0        2    virginica\n",
       "147           6.5           5.2        2    virginica\n",
       "148           6.2           5.4        2    virginica\n",
       "149           5.9           5.1        2    virginica\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Speicies virgicolorとvirginica(1と2)を選択\n",
    "# 特徴量　sepal_lengthとpetal_lengthを選択\n",
    "X_choice = ['sepal_length', 'petal_length', 'Species', 'Species_name']\n",
    "y_choice = 'Species == 1 | Species == 2'\n",
    "df_choice = df.query(y_choice)[X_choice]\n",
    "df_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5.1, 3. ],\n",
       "        [6.3, 4.7],\n",
       "        [5.9, 5.1],\n",
       "        [6.4, 5.6],\n",
       "        [6.2, 4.8]]),\n",
       " array([0, 0, 1, 1, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "X = df_choice.iloc[:, 0:2].values\n",
    "#　目的変数を0, 1に分類するように処理\n",
    "y = df_choice.iloc[:, 2].values - 1\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0)\n",
    "X_train2[:5], y_train2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.77029021, -2.29778941],\n",
       "        [ 0.03817879, -0.24187257],\n",
       "        [-0.56464421,  0.24187257],\n",
       "        [ 0.18888454,  0.84655399],\n",
       "        [-0.11252696, -0.12093628]]),\n",
       " array([[ 7.91707539e-01, -1.20936285e-01],\n",
       "        [ 3.81787899e-02,  8.46553995e-01],\n",
       "        [ 9.42413289e-01,  2.14825997e-15],\n",
       "        [ 1.99735354e+00,  2.05591684e+00],\n",
       "        [ 1.39453054e+00,  1.33029913e+00]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 前処理、標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train2)\n",
    "X_train2_scl = scaler.transform(X_train2)\n",
    "X_test2_scl = scaler.transform(X_test2)\n",
    "X_train2_scl[:5], X_test2_scl[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指標値の計算式\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "def score_calc(y, y_pred):\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    print('Accuracy = {}'.format(accuracy))\n",
    "    print('Precision = {}'.format(precision))\n",
    "    print('Recall = {}'.format(recall))\n",
    "    print('F値 = {}'.format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:1\n",
      "coef_:[1.75100884 0.44483886]\n",
      "intercept_:0.9428120674622944\n",
      "train_loss:0.546925482377433\n",
      "val_loss:0.6880626857874281\n",
      "iter:2\n",
      "coef_:[1.73707695 0.48794614]\n",
      "intercept_:0.9088466148239761\n",
      "train_loss:0.5364291039783609\n",
      "val_loss:0.6772576707748942\n",
      "iter:3\n",
      "coef_:[1.72236614 0.52959771]\n",
      "intercept_:0.8767066633455629\n",
      "train_loss:0.5266501700199029\n",
      "val_loss:0.6670383185006267\n",
      "iter:4\n",
      "coef_:[1.70697318 0.56989936]\n",
      "intercept_:0.8462693420697001\n",
      "train_loss:0.5175081630718112\n",
      "val_loss:0.6573489933390646\n",
      "iter:5\n",
      "coef_:[1.69098384 0.60894568]\n",
      "intercept_:0.8174225789316051\n",
      "train_loss:0.5089342159547209\n",
      "val_loss:0.6481421445791323\n",
      "iter:6\n",
      "coef_:[1.67447434 0.64682147]\n",
      "intercept_:0.7900639456203893\n",
      "train_loss:0.50086917027071\n",
      "val_loss:0.6393768596056245\n",
      "iter:7\n",
      "coef_:[1.65751256 0.68360294]\n",
      "intercept_:0.7640996319051085\n",
      "train_loss:0.49326199064061343\n",
      "val_loss:0.6310177098790003\n",
      "iter:8\n",
      "coef_:[1.64015911 0.71935878]\n",
      "intercept_:0.7394435383687651\n",
      "train_loss:0.4860684653955182\n",
      "val_loss:0.6230338250704608\n",
      "iter:9\n",
      "coef_:[1.62246821 0.75415108]\n",
      "intercept_:0.7160164755988815\n",
      "train_loss:0.47925013826294155\n",
      "val_loss:0.615398145903146\n",
      "iter:10\n",
      "coef_:[1.6044885 0.7880361]\n",
      "intercept_:0.6937454580133356\n",
      "train_loss:0.47277342672974193\n",
      "val_loss:0.6080868177222654\n",
      "iter:11\n",
      "coef_:[1.58626367 0.821065  ]\n",
      "intercept_:0.6725630811861436\n",
      "train_loss:0.46660889168173714\n",
      "val_loss:0.6010786954962307\n",
      "iter:12\n",
      "coef_:[1.56783307 0.85328435]\n",
      "intercept_:0.6524069724913664\n",
      "train_loss:0.46073063001987824\n",
      "val_loss:0.5943549375370721\n",
      "iter:13\n",
      "coef_:[1.54923219 0.88473674]\n",
      "intercept_:0.6332193059315505\n",
      "train_loss:0.45511576758963\n",
      "val_loss:0.5878986702449907\n",
      "iter:14\n",
      "coef_:[1.53049307 0.91546114]\n",
      "intercept_:0.6149463730605416\n",
      "train_loss:0.44974403423124093\n",
      "val_loss:0.5816947100190069\n",
      "iter:15\n",
      "coef_:[1.51164473 0.94549334]\n",
      "intercept_:0.5975382028951925\n",
      "train_loss:0.444597406306272\n",
      "val_loss:0.5757293314240179\n",
      "iter:16\n",
      "coef_:[1.49271344 0.97486629]\n",
      "intercept_:0.5809482246103532\n",
      "train_loss:0.43965980487453443\n",
      "val_loss:0.5699900729807933\n",
      "iter:17\n",
      "coef_:[1.47372304 1.00361037]\n",
      "intercept_:0.5651329676171128\n",
      "train_loss:0.4349168399397377\n",
      "val_loss:0.5644655737114118\n",
      "iter:18\n",
      "coef_:[1.45469518 1.03175367]\n",
      "intercept_:0.550051794335671\n",
      "train_loss:0.4303555929733112\n",
      "val_loss:0.5591454349494971\n",
      "iter:19\n",
      "coef_:[1.43564953 1.05932222]\n",
      "intercept_:0.5356666615966804\n",
      "train_loss:0.42596443135959633\n",
      "val_loss:0.5540201030034726\n",
      "iter:20\n",
      "coef_:[1.416604   1.08634018]\n",
      "intercept_:0.521941907146187\n",
      "train_loss:0.42173284955678525\n",
      "val_loss:0.5490807691105652\n",
      "iter:21\n",
      "coef_:[1.39757488 1.11283002]\n",
      "intercept_:0.5088440581980473\n",
      "train_loss:0.4176513326953749\n",
      "val_loss:0.544319283791504\n",
      "iter:22\n",
      "coef_:[1.37857702 1.1388127 ]\n",
      "intercept_:0.49634165938259694\n",
      "train_loss:0.4137112390854995\n",
      "val_loss:0.5397280832503324\n",
      "iter:23\n",
      "coef_:[1.35962395 1.16430777]\n",
      "intercept_:0.48440511778953843\n",
      "train_loss:0.4099046987125132\n",
      "val_loss:0.5353001258907198\n",
      "iter:24\n",
      "coef_:[1.34072799 1.18933355]\n",
      "intercept_:0.4730065631039738\n",
      "train_loss:0.40622452529505704\n",
      "val_loss:0.5310288373628133\n",
      "iter:25\n",
      "coef_:[1.32190037 1.21390717]\n",
      "intercept_:0.462119721093856\n",
      "train_loss:0.4026641398839987\n",
      "val_loss:0.5269080628309234\n",
      "iter:26\n",
      "coef_:[1.30315135 1.23804474]\n",
      "intercept_:0.4517197989307104\n",
      "train_loss:0.3992175043118661\n",
      "val_loss:0.5229320253760117\n",
      "iter:27\n",
      "coef_:[1.28449026 1.26176139]\n",
      "intercept_:0.44178338101834513\n",
      "train_loss:0.3958790630747572\n",
      "val_loss:0.5190952896288361\n",
      "iter:28\n",
      "coef_:[1.26592559 1.28507138]\n",
      "intercept_:0.43228833417080514\n",
      "train_loss:0.3926436924534296\n",
      "val_loss:0.5153927298781489\n",
      "iter:29\n",
      "coef_:[1.24746509 1.30798813]\n",
      "intercept_:0.4232137211247838\n",
      "train_loss:0.3895066558662802\n",
      "val_loss:0.5118195020201022\n",
      "iter:30\n",
      "coef_:[1.22911579 1.33052434]\n",
      "intercept_:0.41453972149630763\n",
      "train_loss:0.3864635646013943\n",
      "val_loss:0.5083710188152579\n",
      "iter:31\n",
      "coef_:[1.21088409 1.35269201]\n",
      "intercept_:0.4062475593995034\n",
      "train_loss:0.38351034320351124\n",
      "val_loss:0.505042928002403\n",
      "iter:32\n",
      "coef_:[1.19277579 1.3745025 ]\n",
      "intercept_:0.3983194370389884\n",
      "train_loss:0.3806431988992696\n",
      "val_loss:0.5018310928870531\n",
      "iter:33\n",
      "coef_:[1.17479616 1.39596657]\n",
      "intercept_:0.3907384736689026\n",
      "train_loss:0.37785859453419834\n",
      "val_loss:0.4987315750796803\n",
      "iter:34\n",
      "coef_:[1.15694995 1.41709444]\n",
      "intercept_:0.3834886493825272\n",
      "train_loss:0.37515322457065414\n",
      "val_loss:0.49574061910643935\n",
      "iter:35\n",
      "coef_:[1.13924145 1.43789584]\n",
      "intercept_:0.3765547532582767\n",
      "train_loss:0.3725239937597412\n",
      "val_loss:0.49285463865516455\n",
      "iter:36\n",
      "coef_:[1.12167452 1.45837999]\n",
      "intercept_:0.3699223354418478\n",
      "train_loss:0.36996799815421116\n",
      "val_loss:0.49007020425304565\n",
      "iter:37\n",
      "coef_:[1.10425264 1.4785557 ]\n",
      "intercept_:0.3635776627915352\n",
      "train_loss:0.36748250817506545\n",
      "val_loss:0.48738403220075355\n",
      "iter:38\n",
      "coef_:[1.08697889 1.49843135]\n",
      "intercept_:0.35750767775508463\n",
      "train_loss:0.3650649534834534\n",
      "val_loss:0.4847929746117899\n",
      "iter:39\n",
      "coef_:[1.06985605 1.51801495]\n",
      "intercept_:0.3516999601827374\n",
      "train_loss:0.3627129094425597\n",
      "val_loss:0.4822940104261972\n",
      "iter:40\n",
      "coef_:[1.05288654 1.53731412]\n",
      "intercept_:0.34614269181299473\n",
      "train_loss:0.3604240849824596\n",
      "val_loss:0.47988423728510377\n",
      "iter:41\n",
      "coef_:[1.03607252 1.55633619]\n",
      "intercept_:0.34082462319567497\n",
      "train_loss:0.3581963117051285\n",
      "val_loss:0.47756086416736826\n",
      "iter:42\n",
      "coef_:[1.01941587 1.57508813]\n",
      "intercept_:0.3357350428415472\n",
      "train_loss:0.35602753408757537\n",
      "val_loss:0.4753212047022574\n",
      "iter:43\n",
      "coef_:[1.00291821 1.59357664]\n",
      "intercept_:0.33086374840962973\n",
      "train_loss:0.3539158006589432\n",
      "val_loss:0.4731626710829518\n",
      "iter:44\n",
      "coef_:[0.98658093 1.61180813]\n",
      "intercept_:0.3262010197625107\n",
      "train_loss:0.35185925604283447\n",
      "val_loss:0.4710827685150296\n",
      "iter:45\n",
      "coef_:[0.9704052  1.62978875]\n",
      "intercept_:0.32173759373710076\n",
      "train_loss:0.3498561337694359\n",
      "val_loss:0.4690790901421459\n",
      "iter:46\n",
      "coef_:[0.95439199 1.64752439]\n",
      "intercept_:0.31746464049334094\n",
      "train_loss:0.3479047497735532\n",
      "val_loss:0.4671493123980948\n",
      "iter:47\n",
      "coef_:[0.93854208 1.66502073]\n",
      "intercept_:0.31337374131680057\n",
      "train_loss:0.34600349650467216\n",
      "val_loss:0.4652911907404992\n",
      "iter:48\n",
      "coef_:[0.92285608 1.68228322]\n",
      "intercept_:0.3094568677630214\n",
      "train_loss:0.3441508375838698\n",
      "val_loss:0.46350255572661553\n",
      "iter:49\n",
      "coef_:[0.90733441 1.69931708]\n",
      "intercept_:0.30570636204207385\n",
      "train_loss:0.3423453029499794\n",
      "val_loss:0.4617813093963294\n",
      "iter:50\n",
      "coef_:[0.89197737 1.71612736]\n",
      "intercept_:0.302114918551247\n",
      "train_loss:0.34058548444403786\n",
      "val_loss:0.46012542193139844\n",
      "iter:51\n",
      "coef_:[0.87678511 1.73271891]\n",
      "intercept_:0.29867556647223376\n",
      "train_loss:0.3388700317868324\n",
      "val_loss:0.4585329285635048\n",
      "iter:52\n",
      "coef_:[0.86175762 1.74909641]\n",
      "intercept_:0.29538165335671646\n",
      "train_loss:0.337197648909439\n",
      "val_loss:0.45700192670673007\n",
      "iter:53\n",
      "coef_:[0.84689481 1.76526436]\n",
      "intercept_:0.29222682963100904\n",
      "train_loss:0.3355670906011013\n",
      "val_loss:0.45553057329275903\n",
      "iter:54\n",
      "coef_:[0.83219646 1.78122711]\n",
      "intercept_:0.28920503395646396\n",
      "train_loss:0.3339771594427136\n",
      "val_loss:0.45411708228948305\n",
      "iter:55\n",
      "coef_:[0.81766221 1.79698885]\n",
      "intercept_:0.2863104793877826\n",
      "train_loss:0.3324267029976246\n",
      "val_loss:0.45275972238575657\n",
      "iter:56\n",
      "coef_:[0.80329166 1.81255364]\n",
      "intercept_:0.28353764027625\n",
      "train_loss:0.3309146112345212\n",
      "val_loss:0.4514568148269012\n",
      "iter:57\n",
      "coef_:[0.78908427 1.82792538]\n",
      "intercept_:0.28088123986930924\n",
      "train_loss:0.32943981415984414\n",
      "val_loss:0.45020673138717954\n",
      "iter:58\n",
      "coef_:[0.77503943 1.84310786]\n",
      "intercept_:0.2783362385618499\n",
      "train_loss:0.32800127963956477\n",
      "val_loss:0.44900789246690226\n",
      "iter:59\n",
      "coef_:[0.76115647 1.85810473]\n",
      "intercept_:0.27589782275816105\n",
      "train_loss:0.32659801139226585\n",
      "val_loss:0.44785876530311086\n",
      "iter:60\n",
      "coef_:[0.74743461 1.87291953]\n",
      "intercept_:0.2735613943067296\n",
      "train_loss:0.32522904713733897\n",
      "val_loss:0.4467578622839141\n",
      "iter:61\n",
      "coef_:[0.73387302 1.88755568]\n",
      "intercept_:0.27132256047299\n",
      "train_loss:0.3238934568837783\n",
      "val_loss:0.4457037393575657\n",
      "iter:62\n",
      "coef_:[0.72047082 1.90201649]\n",
      "intercept_:0.2691771244177826\n",
      "train_loss:0.32259034134652786\n",
      "val_loss:0.4446949945282752\n",
      "iter:63\n",
      "coef_:[0.70722706 1.91630519]\n",
      "intercept_:0.2671210761516834\n",
      "train_loss:0.3213188304786642\n",
      "val_loss:0.44373026643153934\n",
      "iter:64\n",
      "coef_:[0.69414072 1.93042487]\n",
      "intercept_:0.2651505839375588\n",
      "train_loss:0.3200780821088667\n",
      "val_loss:0.4428082329825061\n",
      "iter:65\n",
      "coef_:[0.68121075 1.94437855]\n",
      "intercept_:0.2632619861156859\n",
      "train_loss:0.3188672806746849\n",
      "val_loss:0.44192761009151726\n",
      "iter:66\n",
      "coef_:[0.66843604 1.95816917]\n",
      "intercept_:0.261451783327599\n",
      "train_loss:0.3176856360430495\n",
      "val_loss:0.44108715044155056\n",
      "iter:67\n",
      "coef_:[0.65581547 1.97179956]\n",
      "intercept_:0.2597166311164779\n",
      "train_loss:0.3165323824103122\n",
      "val_loss:0.44028564232279804\n",
      "iter:68\n",
      "coef_:[0.64334784 1.98527247]\n",
      "intercept_:0.25805333288340937\n",
      "train_loss:0.3154067772748587\n",
      "val_loss:0.4395219085200718\n",
      "iter:69\n",
      "coef_:[0.63103193 1.99859057]\n",
      "intercept_:0.25645883318024165\n",
      "train_loss:0.3143081004760086\n",
      "val_loss:0.438794805249145\n",
      "iter:70\n",
      "coef_:[0.6188665  2.01175648]\n",
      "intercept_:0.25493021132102267\n",
      "train_loss:0.31323565329352454\n",
      "val_loss:0.4381032211385023\n",
      "iter:71\n",
      "coef_:[0.60685026 2.02477271]\n",
      "intercept_:0.2534646752951809\n",
      "train_loss:0.31218875760259945\n",
      "val_loss:0.4374460762533105\n",
      "iter:72\n",
      "coef_:[0.59498191 2.03764171]\n",
      "intercept_:0.25205955596668217\n",
      "train_loss:0.31116675507967545\n",
      "val_loss:0.43682232115871245\n",
      "iter:73\n",
      "coef_:[0.58326012 2.05036588]\n",
      "intercept_:0.2507123015443834\n",
      "train_loss:0.31016900645489176\n",
      "val_loss:0.4362309360198225\n",
      "iter:74\n",
      "coef_:[0.57168354 2.06294754]\n",
      "intercept_:0.2494204723097165\n",
      "train_loss:0.3091948908073539\n",
      "val_loss:0.43567092973603677\n",
      "iter:75\n",
      "coef_:[0.56025079 2.07538894]\n",
      "intercept_:0.24818173558867668\n",
      "train_loss:0.3082438048997724\n",
      "val_loss:0.4351413391074949\n",
      "iter:76\n",
      "coef_:[0.54896049 2.08769229]\n",
      "intercept_:0.24699386095586773\n",
      "train_loss:0.30731516254934255\n",
      "val_loss:0.43464122803171784\n",
      "iter:77\n",
      "coef_:[0.53781124 2.09985972]\n",
      "intercept_:0.24585471565907696\n",
      "train_loss:0.3064083940320251\n",
      "val_loss:0.434169686728631\n",
      "iter:78\n",
      "coef_:[0.52680161 2.11189333]\n",
      "intercept_:0.2447622602535201\n",
      "train_loss:0.30552294551765036\n",
      "val_loss:0.43372583099233303\n",
      "iter:79\n",
      "coef_:[0.51593019 2.12379514]\n",
      "intercept_:0.243714544435516\n",
      "train_loss:0.30465827853350474\n",
      "val_loss:0.43330880146811773\n",
      "iter:80\n",
      "coef_:[0.50519553 2.13556714]\n",
      "intercept_:0.2427097030659271\n",
      "train_loss:0.3038138694542696\n",
      "val_loss:0.4329177629533836\n",
      "iter:81\n",
      "coef_:[0.49459618 2.14721126]\n",
      "intercept_:0.24174595237423768\n",
      "train_loss:0.3029892090163789\n",
      "val_loss:0.4325519037211827\n",
      "iter:82\n",
      "coef_:[0.4841307  2.15872938]\n",
      "intercept_:0.2408215863346408\n",
      "train_loss:0.30218380185503296\n",
      "val_loss:0.4322104348652629\n",
      "iter:83\n",
      "coef_:[0.47379763 2.17012333]\n",
      "intercept_:0.23993497320597093\n",
      "train_loss:0.3013971660622648\n",
      "val_loss:0.4318925896655561\n",
      "iter:84\n",
      "coef_:[0.46359551 2.18139493]\n",
      "intercept_:0.23908455222775477\n",
      "train_loss:0.30062883276459684\n",
      "val_loss:0.43159762297314425\n",
      "iter:85\n",
      "coef_:[0.45352288 2.19254591]\n",
      "intercept_:0.2382688304650586\n",
      "train_loss:0.2998783457189573\n",
      "val_loss:0.43132481061381617\n",
      "iter:86\n",
      "coef_:[0.44357827 2.20357798]\n",
      "intercept_:0.23748637979519244\n",
      "train_loss:0.2991452609256373\n",
      "val_loss:0.4310734488093989\n",
      "iter:87\n",
      "coef_:[0.43376022 2.21449282]\n",
      "intercept_:0.23673583402968765\n",
      "train_loss:0.2984291462571821\n",
      "val_loss:0.43084285361610086\n",
      "iter:88\n",
      "coef_:[0.42406727 2.22529205]\n",
      "intercept_:0.2360158861652992\n",
      "train_loss:0.29772958110219794\n",
      "val_loss:0.43063236037917363\n",
      "iter:89\n",
      "coef_:[0.41449796 2.23597728]\n",
      "intercept_:0.2353252857580994\n",
      "train_loss:0.29704615602315027\n",
      "val_loss:0.43044132320323814\n",
      "iter:90\n",
      "coef_:[0.40505082 2.24655006]\n",
      "intercept_:0.23466283641502467\n",
      "train_loss:0.29637847242730003\n",
      "val_loss:0.4302691144376739\n",
      "iter:91\n",
      "coef_:[0.39572441 2.25701192]\n",
      "intercept_:0.23402739339751527\n",
      "train_loss:0.2957261422500041\n",
      "val_loss:0.43011512417651004\n",
      "iter:92\n",
      "coef_:[0.38651728 2.26736434]\n",
      "intercept_:0.23341786133215112\n",
      "train_loss:0.2950887876496612\n",
      "val_loss:0.42997875977229305\n",
      "iter:93\n",
      "coef_:[0.37742798 2.27760879]\n",
      "intercept_:0.2328331920234324\n",
      "train_loss:0.29446604071365245\n",
      "val_loss:0.4298594453634422\n",
      "iter:94\n",
      "coef_:[0.36845507 2.2877467 ]\n",
      "intercept_:0.2322723823640883\n",
      "train_loss:0.2938575431746715\n",
      "val_loss:0.42975662141463106\n",
      "iter:95\n",
      "coef_:[0.35959713 2.29777946]\n",
      "intercept_:0.231734472338517\n",
      "train_loss:0.29326294613689036\n",
      "val_loss:0.4296697442697665\n",
      "iter:96\n",
      "coef_:[0.35085274 2.30770844]\n",
      "intercept_:0.23121854311516826\n",
      "train_loss:0.29268190981145253\n",
      "val_loss:0.4295982857171555\n",
      "iter:97\n",
      "coef_:[0.34222047 2.31753498]\n",
      "intercept_:0.23072371522387727\n",
      "train_loss:0.2921141032608192\n",
      "val_loss:0.4295417325664793\n",
      "iter:98\n",
      "coef_:[0.33369892 2.32726041]\n",
      "intercept_:0.23024914681434483\n",
      "train_loss:0.29155920415153763\n",
      "val_loss:0.429499586237209\n",
      "iter:99\n",
      "coef_:[0.3252867  2.33688599]\n",
      "intercept_:0.22979403199213572\n",
      "train_loss:0.2910168985150258\n",
      "val_loss:0.4294713623581202\n",
      "iter:100\n",
      "coef_:[0.31698241 2.34641301]\n",
      "intercept_:0.22935759922873492\n",
      "train_loss:0.2904868805160039\n",
      "val_loss:0.4294565903775802\n",
      "iter:101\n",
      "coef_:[0.30878469 2.35584268]\n",
      "intercept_:0.22893910984236004\n",
      "train_loss:0.28996885222822544\n",
      "val_loss:0.42945481318429385\n",
      "iter:102\n",
      "coef_:[0.30069215 2.36517623]\n",
      "intercept_:0.2285378565463796\n",
      "train_loss:0.28946252341718803\n",
      "val_loss:0.4294655867382147\n",
      "iter:103\n",
      "coef_:[0.29270346 2.37441485]\n",
      "intercept_:0.22815316206232988\n",
      "train_loss:0.28896761132952425\n",
      "val_loss:0.42948847971133264\n",
      "iter:104\n",
      "coef_:[0.28481725 2.38355971]\n",
      "intercept_:0.2277843777946599\n",
      "train_loss:0.28848384048879594\n",
      "val_loss:0.4295230731380707\n",
      "iter:105\n",
      "coef_:[0.2770322  2.39261194]\n",
      "intercept_:0.22743088256446292\n",
      "train_loss:0.2880109424974309\n",
      "val_loss:0.4295689600750252\n",
      "iter:106\n",
      "coef_:[0.26934699 2.40157268]\n",
      "intercept_:0.22709208139957693\n",
      "train_loss:0.28754865584455913\n",
      "val_loss:0.4296257452698008\n",
      "iter:107\n",
      "coef_:[0.2617603  2.41044302]\n",
      "intercept_:0.22676740437855367\n",
      "train_loss:0.2870967257195227\n",
      "val_loss:0.4296930448386974\n",
      "iter:108\n",
      "coef_:[0.25427084 2.41922406]\n",
      "intercept_:0.22645630552610743\n",
      "train_loss:0.2866549038308433\n",
      "val_loss:0.4297704859530149\n",
      "iter:109\n",
      "coef_:[0.24687732 2.42791686]\n",
      "intercept_:0.22615826175776146\n",
      "train_loss:0.2862229482304491\n",
      "val_loss:0.42985770653375105\n",
      "iter:110\n",
      "coef_:[0.23957847 2.43652246]\n",
      "intercept_:0.2258727718715116\n",
      "train_loss:0.2858006231429707\n",
      "val_loss:0.42995435495447354\n",
      "iter:111\n",
      "coef_:[0.23237304 2.4450419 ]\n",
      "intercept_:0.22559935558442307\n",
      "train_loss:0.28538769879992887\n",
      "val_loss:0.43006008975215465\n",
      "iter:112\n",
      "coef_:[0.22525976 2.45347617]\n",
      "intercept_:0.22533755261216898\n",
      "train_loss:0.2849839512786439\n",
      "val_loss:0.43017457934576425\n",
      "iter:113\n",
      "coef_:[0.21823742 2.46182628]\n",
      "intercept_:0.22508692178960743\n",
      "train_loss:0.28458916234570975\n",
      "val_loss:0.4302975017624214\n",
      "iter:114\n",
      "coef_:[0.21130479 2.47009321]\n",
      "intercept_:0.22484704023057722\n",
      "train_loss:0.28420311930487946\n",
      "val_loss:0.4304285443709107\n",
      "iter:115\n",
      "coef_:[0.20446066 2.4782779 ]\n",
      "intercept_:0.22461750252517376\n",
      "train_loss:0.28382561484921964\n",
      "val_loss:0.43056740362237717\n",
      "iter:116\n",
      "coef_:[0.19770384 2.48638131]\n",
      "intercept_:0.22439791997284217\n",
      "train_loss:0.28345644691739685\n",
      "val_loss:0.4307137847980142\n",
      "iter:117\n",
      "coef_:[0.19103315 2.49440437]\n",
      "intercept_:0.22418791984969852\n",
      "train_loss:0.2830954185539659\n",
      "val_loss:0.4308674017635679\n",
      "iter:118\n",
      "coef_:[0.18444742 2.50234798]\n",
      "intercept_:0.22398714470855968\n",
      "train_loss:0.28274233777353547\n",
      "val_loss:0.43102797673048443\n",
      "iter:119\n",
      "coef_:[0.1779455  2.51021306]\n",
      "intercept_:0.22379525171022935\n",
      "train_loss:0.2823970174286931\n",
      "val_loss:0.4311952400235285\n",
      "iter:120\n",
      "coef_:[0.17152625 2.51800048]\n",
      "intercept_:0.22361191198465125\n",
      "train_loss:0.28205927508157497\n",
      "val_loss:0.43136892985471115\n",
      "iter:121\n",
      "coef_:[0.16518855 2.52571111]\n",
      "intercept_:0.22343681002060198\n",
      "train_loss:0.28172893287897227\n",
      "val_loss:0.4315487921033639\n",
      "iter:122\n",
      "coef_:[0.15893127 2.53334581]\n",
      "intercept_:0.22326964308265393\n",
      "train_loss:0.28140581743086873\n",
      "val_loss:0.431734580102201\n",
      "iter:123\n",
      "coef_:[0.15275332 2.54090544]\n",
      "intercept_:0.22311012065419458\n",
      "train_loss:0.2810897596923102\n",
      "val_loss:0.431926054429219\n",
      "iter:124\n",
      "coef_:[0.14665363 2.54839081]\n",
      "intercept_:0.22295796390534203\n",
      "train_loss:0.28078059484850804\n",
      "val_loss:0.4321229827052804\n",
      "iter:125\n",
      "coef_:[0.1406311  2.55580275]\n",
      "intercept_:0.22281290518464703\n",
      "train_loss:0.2804781622030846\n",
      "val_loss:0.43232513939723743\n",
      "iter:126\n",
      "coef_:[0.13468469 2.56314207]\n",
      "intercept_:0.2226746875335212\n",
      "train_loss:0.28018230506937\n",
      "val_loss:0.43253230562645195\n",
      "iter:127\n",
      "coef_:[0.12881336 2.57040956]\n",
      "intercept_:0.22254306422237743\n",
      "train_loss:0.2798928706646644\n",
      "val_loss:0.43274426898257073\n",
      "iter:128\n",
      "coef_:[0.12301606 2.57760601]\n",
      "intercept_:0.2224177983075127\n",
      "train_loss:0.2796097100073817\n",
      "val_loss:0.43296082334242153\n",
      "iter:129\n",
      "coef_:[0.11729179 2.58473219]\n",
      "intercept_:0.2222986622078072\n",
      "train_loss:0.27933267781699345\n",
      "val_loss:0.43318176869389496\n",
      "iter:130\n",
      "coef_:[0.11163953 2.59178885]\n",
      "intercept_:0.22218543730035314\n",
      "train_loss:0.27906163241669596\n",
      "val_loss:0.433406910964682\n",
      "iter:131\n",
      "coef_:[0.1060583  2.59877676]\n",
      "intercept_:0.2220779135341671\n",
      "train_loss:0.27879643563872347\n",
      "val_loss:0.43363606185574083\n",
      "iter:132\n",
      "coef_:[0.10054711 2.60569664]\n",
      "intercept_:0.22197588906117563\n",
      "train_loss:0.2785369527322349\n",
      "val_loss:0.43386903867936694\n",
      "iter:133\n",
      "coef_:[0.09510501 2.61254924]\n",
      "intercept_:0.221879169883701\n",
      "train_loss:0.2782830522737028\n",
      "val_loss:0.43410566420174673\n",
      "iter:134\n",
      "coef_:[0.08973103 2.61933527]\n",
      "intercept_:0.22178756951770667\n",
      "train_loss:0.278034606079735\n",
      "val_loss:0.43434576648987466\n",
      "iter:135\n",
      "coef_:[0.08442424 2.62605543]\n",
      "intercept_:0.22170090867109646\n",
      "train_loss:0.2777914891222639\n",
      "val_loss:0.4345891787627176\n",
      "iter:136\n",
      "coef_:[0.07918372 2.63271044]\n",
      "intercept_:0.2216190149363908\n",
      "train_loss:0.27755357944603554\n",
      "val_loss:0.43483573924651564\n",
      "iter:137\n",
      "coef_:[0.07400854 2.63930098]\n",
      "intercept_:0.221541722497135\n",
      "train_loss:0.2773207580883383\n",
      "val_loss:0.43508529103410526\n",
      "iter:138\n",
      "coef_:[0.06889781 2.64582773]\n",
      "intercept_:0.22146887184742198\n",
      "train_loss:0.27709290900090733\n",
      "val_loss:0.43533768194816025\n",
      "iter:139\n",
      "coef_:[0.06385064 2.65229136]\n",
      "intercept_:0.22140030952393996\n",
      "train_loss:0.2768699189739477\n",
      "val_loss:0.4355927644082421\n",
      "iter:140\n",
      "coef_:[0.05886615 2.65869254]\n",
      "intercept_:0.2213358878499814\n",
      "train_loss:0.27665167756221665\n",
      "val_loss:0.43585039530155945\n",
      "iter:141\n",
      "coef_:[0.05394347 2.66503193]\n",
      "intercept_:0.2212754646908745\n",
      "train_loss:0.27643807701310813\n",
      "val_loss:0.43611043585733467\n",
      "iter:142\n",
      "coef_:[0.04908176 2.67131016]\n",
      "intercept_:0.22121890322032317\n",
      "train_loss:0.2762290121966883\n",
      "val_loss:0.4363727515246802\n",
      "iter:143\n",
      "coef_:[0.04428016 2.67752789]\n",
      "intercept_:0.2211660716971633\n",
      "train_loss:0.27602438053762346\n",
      "val_loss:0.4366372118538887\n",
      "iter:144\n",
      "coef_:[0.03953786 2.68368573]\n",
      "intercept_:0.22111684325206576\n",
      "train_loss:0.27582408194895425\n",
      "val_loss:0.436903690381045\n",
      "iter:145\n",
      "coef_:[0.03485404 2.68978431]\n",
      "intercept_:0.22107109568373773\n",
      "train_loss:0.2756280187676613\n",
      "val_loss:0.4371720645158668\n",
      "iter:146\n",
      "coef_:[0.03022789 2.69582424]\n",
      "intercept_:0.22102871126419288\n",
      "train_loss:0.27543609569197575\n",
      "val_loss:0.4374422154326879\n",
      "iter:147\n",
      "coef_:[0.02565861 2.70180614]\n",
      "intercept_:0.22098957655268156\n",
      "train_loss:0.27524821972038604\n",
      "val_loss:0.4377140279644955\n",
      "iter:148\n",
      "coef_:[0.02114543 2.70773059]\n",
      "intercept_:0.22095358221788944\n",
      "train_loss:0.27506430009229554\n",
      "val_loss:0.4379873904999383\n",
      "iter:149\n",
      "coef_:[0.01668757 2.71359819]\n",
      "intercept_:0.22092062286803077\n",
      "train_loss:0.2748842482302838\n",
      "val_loss:0.4382621948832237\n",
      "iter:150\n",
      "coef_:[0.01228428 2.71940953]\n",
      "intercept_:0.22089059688847978\n",
      "train_loss:0.2747079776839299\n",
      "val_loss:0.4385383363168227\n",
      "iter:151\n",
      "coef_:[0.0079348  2.72516518]\n",
      "intercept_:0.22086340628659876\n",
      "train_loss:0.27453540407515353\n",
      "val_loss:0.4388157132669061\n",
      "iter:152\n",
      "coef_:[0.00363839 2.73086571]\n",
      "intercept_:0.22083895654343758\n",
      "train_loss:0.27436644504503255\n",
      "val_loss:0.4390942273714344\n",
      "iter:153\n",
      "coef_:[-6.05671549e-04  2.73651168e+00]\n",
      "intercept_:0.22081715647199363\n",
      "train_loss:0.2742010202020568\n",
      "val_loss:0.43937378335082883\n",
      "iter:154\n",
      "coef_:[-0.0047981   2.74210365]\n",
      "intercept_:0.22079791808173505\n",
      "train_loss:0.27403905107177806\n",
      "val_loss:0.4396542889211515\n",
      "iter:155\n",
      "coef_:[-0.00893961  2.74764217]\n",
      "intercept_:0.22078115644910395\n",
      "train_loss:0.2738804610478188\n",
      "val_loss:0.4399356547097215\n",
      "iter:156\n",
      "coef_:[-0.01303089  2.75312779]\n",
      "intercept_:0.22076678959372867\n",
      "train_loss:0.2737251753442012\n",
      "val_loss:0.44021779417310397\n",
      "iter:157\n",
      "coef_:[-0.01707263  2.75856103]\n",
      "intercept_:0.22075473836008638\n",
      "train_loss:0.27357312094896075\n",
      "val_loss:0.44050062351740094\n",
      "iter:158\n",
      "coef_:[-0.02106551  2.76394243]\n",
      "intercept_:0.22074492630436945\n",
      "train_loss:0.2734242265790088\n",
      "val_loss:0.4407840616207801\n",
      "iter:159\n",
      "coef_:[-0.02501019  2.76927252]\n",
      "intercept_:0.2207372795863196\n",
      "train_loss:0.2732784226362104\n",
      "val_loss:0.44106802995818134\n",
      "iter:160\n",
      "coef_:[-0.02890732  2.77455181]\n",
      "intercept_:0.220731726865805\n",
      "train_loss:0.27313564116464206\n",
      "val_loss:0.4413524525281362\n",
      "iter:161\n",
      "coef_:[-0.03275757  2.77978081]\n",
      "intercept_:0.2207281992039251\n",
      "train_loss:0.2729958158089995\n",
      "val_loss:0.44163725578164176\n",
      "iter:162\n",
      "coef_:[-0.03656155  2.78496003]\n",
      "intercept_:0.2207266299684387\n",
      "train_loss:0.2728588817741205\n",
      "val_loss:0.44192236855303263\n",
      "iter:163\n",
      "coef_:[-0.04031991  2.79008997]\n",
      "intercept_:0.22072695474331847\n",
      "train_loss:0.2727247757855949\n",
      "val_loss:0.4422077219927912\n",
      "iter:164\n",
      "coef_:[-0.04403326  2.79517113]\n",
      "intercept_:0.22072911124224606\n",
      "train_loss:0.27259343605143055\n",
      "val_loss:0.4424932495022443\n",
      "iter:165\n",
      "coef_:[-0.04770221  2.80020398]\n",
      "intercept_:0.22073303922586868\n",
      "train_loss:0.2724648022247447\n",
      "val_loss:0.4427788866700915\n",
      "iter:166\n",
      "coef_:[-0.05132736  2.80518903]\n",
      "intercept_:0.22073868042264672\n",
      "train_loss:0.2723388153674552\n",
      "val_loss:0.4430645712107127\n",
      "iter:167\n",
      "coef_:[-0.0549093   2.81012673]\n",
      "intercept_:0.22074597845313057\n",
      "train_loss:0.27221541791494125\n",
      "val_loss:0.44335024290420577\n",
      "iter:168\n",
      "coef_:[-0.05844861  2.81501758]\n",
      "intercept_:0.2207548787575106\n",
      "train_loss:0.27209455364164814\n",
      "val_loss:0.4436358435381036\n",
      "iter:169\n",
      "coef_:[-0.06194588  2.81986202]\n",
      "intercept_:0.22076532852629263\n",
      "train_loss:0.27197616762761023\n",
      "val_loss:0.44392131685072383\n",
      "iter:170\n",
      "coef_:[-0.06540166  2.82466053]\n",
      "intercept_:0.2207772766339575\n",
      "train_loss:0.27186020622586504\n",
      "val_loss:0.4442066084761055\n",
      "iter:171\n",
      "coef_:[-0.06881651  2.82941357]\n",
      "intercept_:0.22079067357546975\n",
      "train_loss:0.2717466170307353\n",
      "val_loss:0.4444916658904845\n",
      "iter:172\n",
      "coef_:[-0.07219098  2.83412157]\n",
      "intercept_:0.2208054714055066\n",
      "train_loss:0.2716353488469551\n",
      "val_loss:0.44477643836026864\n",
      "iter:173\n",
      "coef_:[-0.07552562  2.838785  ]\n",
      "intercept_:0.2208216236802843\n",
      "train_loss:0.2715263516596144\n",
      "val_loss:0.4450608768914651\n",
      "iter:174\n",
      "coef_:[-0.07882096  2.84340429]\n",
      "intercept_:0.2208390854018648\n",
      "train_loss:0.27141957660490257\n",
      "val_loss:0.44534493418052173\n",
      "iter:175\n",
      "coef_:[-0.08207751  2.84797988]\n",
      "intercept_:0.22085781296483054\n",
      "train_loss:0.2713149759416261\n",
      "val_loss:0.44562856456654115\n",
      "iter:176\n",
      "coef_:[-0.08529581  2.85251219]\n",
      "intercept_:0.22087776410522056\n",
      "train_loss:0.2712125030234804\n",
      "val_loss:0.4459117239848288\n",
      "iter:177\n",
      "coef_:[-0.08847635  2.85700167]\n",
      "intercept_:0.22089889785162636\n",
      "train_loss:0.27111211227205373\n",
      "val_loss:0.44619436992173506\n",
      "iter:178\n",
      "coef_:[-0.09161965  2.86144872]\n",
      "intercept_:0.2209211744783497\n",
      "train_loss:0.27101375915054515\n",
      "val_loss:0.4464764613707587\n",
      "iter:179\n",
      "coef_:[-0.09472619  2.86585378]\n",
      "intercept_:0.2209445554605299\n",
      "train_loss:0.27091740013817356\n",
      "val_loss:0.4467579587898708\n",
      "iter:180\n",
      "coef_:[-0.09779646  2.87021724]\n",
      "intercept_:0.220969003431152\n",
      "train_loss:0.27082299270526117\n",
      "val_loss:0.4470388240600287\n",
      "iter:181\n",
      "coef_:[-0.10083095  2.87453953]\n",
      "intercept_:0.22099448213985112\n",
      "train_loss:0.2707304952889719\n",
      "val_loss:0.4473190204448422\n",
      "iter:182\n",
      "coef_:[-0.10383012  2.87882104]\n",
      "intercept_:0.2210209564134325\n",
      "train_loss:0.2706398672696867\n",
      "val_loss:0.4475985125513631\n",
      "iter:183\n",
      "coef_:[-0.10679445  2.88306218]\n",
      "intercept_:0.22104839211803026\n",
      "train_loss:0.27055106894799735\n",
      "val_loss:0.4478772662919631\n",
      "iter:184\n",
      "coef_:[-0.10972439  2.88726334]\n",
      "intercept_:0.22107675612283137\n",
      "train_loss:0.2704640615223031\n",
      "val_loss:0.4481552488472701\n",
      "iter:185\n",
      "coef_:[-0.11262039  2.89142492]\n",
      "intercept_:0.22110601626529483\n",
      "train_loss:0.2703788070669928\n",
      "val_loss:0.44843242863013316\n",
      "iter:186\n",
      "coef_:[-0.11548291  2.8955473 ]\n",
      "intercept_:0.22113614131779913\n",
      "train_loss:0.27029526851119534\n",
      "val_loss:0.4487087752505869\n",
      "iter:187\n",
      "coef_:[-0.11831237  2.89963086]\n",
      "intercept_:0.22116710095565434\n",
      "train_loss:0.27021340961808504\n",
      "val_loss:0.44898425948178544\n",
      "iter:188\n",
      "coef_:[-0.12110921  2.90367598]\n",
      "intercept_:0.2211988657264177\n",
      "train_loss:0.270133194964724\n",
      "val_loss:0.44925885322688086\n",
      "iter:189\n",
      "coef_:[-0.12387387  2.90768305]\n",
      "intercept_:0.2212314070204549\n",
      "train_loss:0.270054589922429\n",
      "val_loss:0.4495325294868173\n",
      "iter:190\n",
      "coef_:[-0.12660675  2.91165243]\n",
      "intercept_:0.22126469704269147\n",
      "train_loss:0.2699775606376467\n",
      "val_loss:0.449805262329015\n",
      "iter:191\n",
      "coef_:[-0.12930828  2.91558448]\n",
      "intercept_:0.2212987087855014\n",
      "train_loss:0.26990207401332456\n",
      "val_loss:0.4500770268569205\n",
      "iter:192\n",
      "coef_:[-0.13197885  2.91947958]\n",
      "intercept_:0.22133341600268264\n",
      "train_loss:0.2698280976907622\n",
      "val_loss:0.45034779918039647\n",
      "iter:193\n",
      "coef_:[-0.13461888  2.92333808]\n",
      "intercept_:0.22136879318447117\n",
      "train_loss:0.2697556000319322\n",
      "val_loss:0.4506175563869286\n",
      "iter:194\n",
      "coef_:[-0.13722876  2.92716034]\n",
      "intercept_:0.2214048155335478\n",
      "train_loss:0.26968455010225545\n",
      "val_loss:0.45088627651362656\n",
      "iter:195\n",
      "coef_:[-0.13980887  2.93094671]\n",
      "intercept_:0.22144145894199385\n",
      "train_loss:0.2696149176538195\n",
      "val_loss:0.45115393851999513\n",
      "iter:196\n",
      "coef_:[-0.14235961  2.93469753]\n",
      "intercept_:0.22147869996915395\n",
      "train_loss:0.26954667310902797\n",
      "val_loss:0.45142052226145624\n",
      "iter:197\n",
      "coef_:[-0.14488135  2.93841316]\n",
      "intercept_:0.2215165158203658\n",
      "train_loss:0.26947978754466856\n",
      "val_loss:0.45168600846359774\n",
      "iter:198\n",
      "coef_:[-0.14737446  2.94209394]\n",
      "intercept_:0.22155488432651904\n",
      "train_loss:0.269414232676389\n",
      "val_loss:0.4519503786971308\n",
      "iter:199\n",
      "coef_:[-0.14983932  2.9457402 ]\n",
      "intercept_:0.22159378392440687\n",
      "train_loss:0.2693499808435689\n",
      "val_loss:0.45221361535353466\n",
      "iter:200\n",
      "coef_:[-0.15227628  2.94935228]\n",
      "intercept_:0.2216331936378355\n",
      "train_loss:0.26928700499457797\n",
      "val_loss:0.45247570162137035\n",
      "iter:201\n",
      "coef_:[-0.15468571  2.95293051]\n",
      "intercept_:0.22167309305945876\n",
      "train_loss:0.26922527867240814\n",
      "val_loss:0.45273662146324284\n",
      "iter:202\n",
      "coef_:[-0.15706795  2.95647521]\n",
      "intercept_:0.2217134623333057\n",
      "train_loss:0.26916477600067207\n",
      "val_loss:0.4529963595933957\n",
      "iter:203\n",
      "coef_:[-0.15942335  2.95998671]\n",
      "intercept_:0.22175428213797166\n",
      "train_loss:0.2691054716699554\n",
      "val_loss:0.45325490145591874\n",
      "iter:204\n",
      "coef_:[-0.16175225  2.96346534]\n",
      "intercept_:0.2217955336704436\n",
      "train_loss:0.2690473409245151\n",
      "val_loss:0.4535122332035517\n",
      "iter:205\n",
      "coef_:[-0.16405499  2.9669114 ]\n",
      "intercept_:0.2218371986305324\n",
      "train_loss:0.2689903595493134\n",
      "val_loss:0.45376834167706875\n",
      "iter:206\n",
      "coef_:[-0.16633191  2.97032522]\n",
      "intercept_:0.2218792592058858\n",
      "train_loss:0.2689345038573791\n",
      "val_loss:0.45402321438522497\n",
      "iter:207\n",
      "coef_:[-0.16858333  2.97370709]\n",
      "intercept_:0.22192169805755743\n",
      "train_loss:0.2688797506774865\n",
      "val_loss:0.45427683948525155\n",
      "iter:208\n",
      "coef_:[-0.17080956  2.97705734]\n",
      "intercept_:0.22196449830610732\n",
      "train_loss:0.2688260773421441\n",
      "val_loss:0.45452920576388295\n",
      "iter:209\n",
      "coef_:[-0.17301094  2.98037627]\n",
      "intercept_:0.22200764351821173\n",
      "train_loss:0.2687734616758845\n",
      "val_loss:0.45478030261890123\n",
      "iter:210\n",
      "coef_:[-0.17518777  2.98366418]\n",
      "intercept_:0.2220511176937603\n",
      "train_loss:0.26872188198384717\n",
      "val_loss:0.45503012004118426\n",
      "iter:211\n",
      "coef_:[-0.17734037  2.98692136]\n",
      "intercept_:0.22209490525341977\n",
      "train_loss:0.2686713170406469\n",
      "val_loss:0.455278648597242\n",
      "iter:212\n",
      "coef_:[-0.17946903  2.99014811]\n",
      "intercept_:0.22213899102664456\n",
      "train_loss:0.26862174607951905\n",
      "val_loss:0.45552587941222955\n",
      "iter:213\n",
      "coef_:[-0.18157405  2.99334472]\n",
      "intercept_:0.22218336024011512\n",
      "train_loss:0.26857314878173566\n",
      "val_loss:0.4557718041534207\n",
      "iter:214\n",
      "coef_:[-0.18365574  2.99651149]\n",
      "intercept_:0.22222799850658623\n",
      "train_loss:0.26852550526628394\n",
      "val_loss:0.4560164150141328\n",
      "iter:215\n",
      "coef_:[-0.18571438  2.9996487 ]\n",
      "intercept_:0.22227289181412777\n",
      "train_loss:0.268478796079801\n",
      "val_loss:0.4562597046980868\n",
      "iter:216\n",
      "coef_:[-0.18775027  3.00275664]\n",
      "intercept_:0.2223180265157417\n",
      "train_loss:0.26843300218675803\n",
      "val_loss:0.45650166640419343\n",
      "iter:217\n",
      "coef_:[-0.18976367  3.00583558]\n",
      "intercept_:0.2223633893193394\n",
      "train_loss:0.26838810495988524\n",
      "val_loss:0.45674229381175174\n",
      "iter:218\n",
      "coef_:[-0.19175488  3.0088858 ]\n",
      "intercept_:0.22240896727806456\n",
      "train_loss:0.2683440861708353\n",
      "val_loss:0.4569815810660504\n",
      "iter:219\n",
      "coef_:[-0.19372417  3.01190757]\n",
      "intercept_:0.22245474778094693\n",
      "train_loss:0.2683009279810746\n",
      "val_loss:0.45721952276435757\n",
      "iter:220\n",
      "coef_:[-0.19567182  3.01490118]\n",
      "intercept_:0.22250071854387374\n",
      "train_loss:0.2682586129329987\n",
      "val_loss:0.457456113942293\n",
      "iter:221\n",
      "coef_:[-0.19759807  3.01786689]\n",
      "intercept_:0.2225468676008653\n",
      "train_loss:0.26821712394126623\n",
      "val_loss:0.4576913500605679\n",
      "iter:222\n",
      "coef_:[-0.19950321  3.02080496]\n",
      "intercept_:0.22259318329564262\n",
      "train_loss:0.2681764442843437\n",
      "val_loss:0.4579252269920843\n",
      "iter:223\n",
      "coef_:[-0.20138749  3.02371566]\n",
      "intercept_:0.2226396542734748\n",
      "train_loss:0.2681365575962578\n",
      "val_loss:0.458157741009385\n",
      "iter:224\n",
      "coef_:[-0.20325117  3.02659926]\n",
      "intercept_:0.2226862694732952\n",
      "train_loss:0.2680974478585481\n",
      "val_loss:0.4583888887724409\n",
      "iter:225\n",
      "coef_:[-0.2050945  3.029456 ]\n",
      "intercept_:0.22273301812007526\n",
      "train_loss:0.26805909939241523\n",
      "val_loss:0.45861866731677176\n",
      "iter:226\n",
      "coef_:[-0.20691774  3.03228616]\n",
      "intercept_:0.22277988971744556\n",
      "train_loss:0.26802149685106064\n",
      "val_loss:0.4588470740418859\n",
      "iter:227\n",
      "coef_:[-0.20872111  3.03508997]\n",
      "intercept_:0.22282687404055437\n",
      "train_loss:0.26798462521221106\n",
      "val_loss:0.45907410670003373\n",
      "iter:228\n",
      "coef_:[-0.21050488  3.0378677 ]\n",
      "intercept_:0.22287396112915422\n",
      "train_loss:0.26794846977082437\n",
      "val_loss:0.4592997633852655\n",
      "iter:229\n",
      "coef_:[-0.21226927  3.04061959]\n",
      "intercept_:0.22292114128090715\n",
      "train_loss:0.2679130161319709\n",
      "val_loss:0.4595240425227846\n",
      "iter:230\n",
      "coef_:[-0.21401452  3.04334588]\n",
      "intercept_:0.2229684050449003\n",
      "train_loss:0.267878250203887\n",
      "val_loss:0.4597469428585886\n",
      "iter:231\n",
      "coef_:[-0.21574087  3.04604683]\n",
      "intercept_:0.22301574321536338\n",
      "train_loss:0.26784415819119495\n",
      "val_loss:0.45996846344939\n",
      "iter:232\n",
      "coef_:[-0.21744855  3.04872267]\n",
      "intercept_:0.2230631468255804\n",
      "train_loss:0.2678107265882865\n",
      "val_loss:0.4601886036528103\n",
      "iter:233\n",
      "coef_:[-0.21913777  3.05137364]\n",
      "intercept_:0.22311060714198758\n",
      "train_loss:0.26777794217286327\n",
      "val_loss:0.46040736311783803\n",
      "iter:234\n",
      "coef_:[-0.22080877  3.05399997]\n",
      "intercept_:0.2231581156584507\n",
      "train_loss:0.2677457919996339\n",
      "val_loss:0.46062474177554497\n",
      "iter:235\n",
      "coef_:[-0.22246176  3.05660191]\n",
      "intercept_:0.22320566409071482\n",
      "train_loss:0.2677142633941595\n",
      "val_loss:0.4608407398300534\n",
      "iter:236\n",
      "coef_:[-0.22409696  3.05917969]\n",
      "intercept_:0.22325324437101982\n",
      "train_loss:0.26768334394684684\n",
      "val_loss:0.4610553577497468\n",
      "iter:237\n",
      "coef_:[-0.22571458  3.06173352]\n",
      "intercept_:0.22330084864287525\n",
      "train_loss:0.2676530215070842\n",
      "val_loss:0.46126859625871885\n",
      "iter:238\n",
      "coef_:[-0.22731484  3.06426365]\n",
      "intercept_:0.2233484692559887\n",
      "train_loss:0.2676232841775159\n",
      "val_loss:0.46148045632845275\n",
      "iter:239\n",
      "coef_:[-0.22889794  3.0667703 ]\n",
      "intercept_:0.22339609876134178\n",
      "train_loss:0.2675941203084534\n",
      "val_loss:0.46169093916972564\n",
      "iter:240\n",
      "coef_:[-0.23046409  3.06925368]\n",
      "intercept_:0.2234437299064084\n",
      "train_loss:0.2675655184924173\n",
      "val_loss:0.46190004622473196\n",
      "iter:241\n",
      "coef_:[-0.23201348  3.07171402]\n",
      "intercept_:0.22349135563050956\n",
      "train_loss:0.26753746755880903\n",
      "val_loss:0.4621077791594195\n",
      "iter:242\n",
      "coef_:[-0.23354633  3.07415154]\n",
      "intercept_:0.2235389690603003\n",
      "train_loss:0.26750995656870796\n",
      "val_loss:0.4623141398560327\n",
      "iter:243\n",
      "coef_:[-0.23506282  3.07656646]\n",
      "intercept_:0.2235865635053834\n",
      "train_loss:0.26748297480979005\n",
      "val_loss:0.46251913040585796\n",
      "iter:244\n",
      "coef_:[-0.23656315  3.07895898]\n",
      "intercept_:0.22363413245404548\n",
      "train_loss:0.2674565117913669\n",
      "val_loss:0.46272275310216426\n",
      "iter:245\n",
      "coef_:[-0.23804751  3.08132932]\n",
      "intercept_:0.2236816695691111\n",
      "train_loss:0.26743055723954\n",
      "val_loss:0.4629250104333363\n",
      "iter:246\n",
      "coef_:[-0.23951609  3.0836777 ]\n",
      "intercept_:0.22372916868391035\n",
      "train_loss:0.26740510109246884\n",
      "val_loss:0.4631259050761918\n",
      "iter:247\n",
      "coef_:[-0.24096908  3.08600431]\n",
      "intercept_:0.22377662379835625\n",
      "train_loss:0.2673801334957493\n",
      "val_loss:0.46332543988948083\n",
      "iter:248\n",
      "coef_:[-0.24240666  3.08830937]\n",
      "intercept_:0.22382402907512775\n",
      "train_loss:0.26735564479789936\n",
      "val_loss:0.46352361790756164\n",
      "iter:249\n",
      "coef_:[-0.24382902  3.09059307]\n",
      "intercept_:0.22387137883595487\n",
      "train_loss:0.2673316255459506\n",
      "val_loss:0.46372044233424675\n",
      "iter:250\n",
      "coef_:[-0.24523632  3.09285562]\n",
      "intercept_:0.22391866755800224\n",
      "train_loss:0.2673080664811407\n",
      "val_loss:0.46391591653681663\n",
      "iter:251\n",
      "coef_:[-0.24662875  3.09509723]\n",
      "intercept_:0.22396588987034768\n",
      "train_loss:0.26728495853470724\n",
      "val_loss:0.4641100440401958\n",
      "iter:252\n",
      "coef_:[-0.24800648  3.09731808]\n",
      "intercept_:0.22401304055055252\n",
      "train_loss:0.26726229282377767\n",
      "val_loss:0.4643028285212867\n",
      "iter:253\n",
      "coef_:[-0.24936968  3.09951838]\n",
      "intercept_:0.22406011452132066\n",
      "train_loss:0.2672400606473545\n",
      "val_loss:0.46449427380345865\n",
      "iter:254\n",
      "coef_:[-0.25071853  3.10169831]\n",
      "intercept_:0.22410710684724325\n",
      "train_loss:0.267218253482393\n",
      "val_loss:0.46468438385118477\n",
      "iter:255\n",
      "coef_:[-0.25205319  3.10385807]\n",
      "intercept_:0.22415401273162613\n",
      "train_loss:0.2671968629799691\n",
      "val_loss:0.46487316276482654\n",
      "iter:256\n",
      "coef_:[-0.25337383  3.10599786]\n",
      "intercept_:0.22420082751339723\n",
      "train_loss:0.26717588096153544\n",
      "val_loss:0.46506061477555977\n",
      "iter:257\n",
      "coef_:[-0.2546806   3.10811785]\n",
      "intercept_:0.2242475466640914\n",
      "train_loss:0.2671552994152629\n",
      "val_loss:0.4652467442404385\n",
      "iter:258\n",
      "coef_:[-0.25597367  3.11021823]\n",
      "intercept_:0.22429416578491013\n",
      "train_loss:0.26713511049246585\n",
      "val_loss:0.46543155563759464\n",
      "iter:259\n",
      "coef_:[-0.2572532  3.1122992]\n",
      "intercept_:0.2243406806038535\n",
      "train_loss:0.2671153065041095\n",
      "val_loss:0.4656150535615673\n",
      "iter:260\n",
      "coef_:[-0.25851935  3.11436093]\n",
      "intercept_:0.22438708697292245\n",
      "train_loss:0.26709587991739625\n",
      "val_loss:0.46579724271876105\n",
      "iter:261\n",
      "coef_:[-0.25977226  3.1164036 ]\n",
      "intercept_:0.22443338086538858\n",
      "train_loss:0.2670768233524303\n",
      "val_loss:0.4659781279230276\n",
      "iter:262\n",
      "coef_:[-0.26101209  3.1184274 ]\n",
      "intercept_:0.22447955837312997\n",
      "train_loss:0.26705812957895747\n",
      "val_loss:0.46615771409136947\n",
      "iter:263\n",
      "coef_:[-0.262239    3.12043249]\n",
      "intercept_:0.22452561570403026\n",
      "train_loss:0.26703979151317947\n",
      "val_loss:0.4663360062397605\n",
      "iter:264\n",
      "coef_:[-0.26345313  3.12241907]\n",
      "intercept_:0.22457154917943975\n",
      "train_loss:0.26702180221464006\n",
      "val_loss:0.46651300947908214\n",
      "iter:265\n",
      "coef_:[-0.26465462  3.12438729]\n",
      "intercept_:0.22461735523169588\n",
      "train_loss:0.26700415488318124\n",
      "val_loss:0.46668872901117014\n",
      "iter:266\n",
      "coef_:[-0.26584363  3.12633734]\n",
      "intercept_:0.2246630304017019\n",
      "train_loss:0.26698684285596913\n",
      "val_loss:0.4668631701249721\n",
      "iter:267\n",
      "coef_:[-0.26702029  3.12826938]\n",
      "intercept_:0.22470857133656164\n",
      "train_loss:0.26696985960458564\n",
      "val_loss:0.4670363381928091\n",
      "iter:268\n",
      "coef_:[-0.26818474  3.13018359]\n",
      "intercept_:0.22475397478726852\n",
      "train_loss:0.2669531987321863\n",
      "val_loss:0.4672082386667417\n",
      "iter:269\n",
      "coef_:[-0.26933713  3.13208013]\n",
      "intercept_:0.22479923760644785\n",
      "train_loss:0.26693685397072175\n",
      "val_loss:0.46737887707503634\n",
      "iter:270\n",
      "coef_:[-0.27047759  3.13395916]\n",
      "intercept_:0.22484435674614997\n",
      "train_loss:0.26692081917822125\n",
      "val_loss:0.46754825901872893\n",
      "iter:271\n",
      "coef_:[-0.27160626  3.13582086]\n",
      "intercept_:0.22488932925569358\n",
      "train_loss:0.26690508833613735\n",
      "val_loss:0.46771639016828503\n",
      "iter:272\n",
      "coef_:[-0.27272327  3.13766538]\n",
      "intercept_:0.22493415227955713\n",
      "train_loss:0.2668896555467496\n",
      "val_loss:0.467883276260352\n",
      "iter:273\n",
      "coef_:[-0.27382875  3.13949288]\n",
      "intercept_:0.22497882305531736\n",
      "train_loss:0.26687451503062626\n",
      "val_loss:0.4680489230946017\n",
      "iter:274\n",
      "coef_:[-0.27492284  3.14130353]\n",
      "intercept_:0.22502333891163345\n",
      "train_loss:0.26685966112414233\n",
      "val_loss:0.468213336530662\n",
      "iter:275\n",
      "coef_:[-0.27600566  3.14309748]\n",
      "intercept_:0.22506769726627546\n",
      "train_loss:0.26684508827705344\n",
      "val_loss:0.46837652248513295\n",
      "iter:276\n",
      "coef_:[-0.27707733  3.14487489]\n",
      "intercept_:0.22511189562419592\n",
      "train_loss:0.26683079105012236\n",
      "val_loss:0.46853848692868794\n",
      "iter:277\n",
      "coef_:[-0.278138    3.14663592]\n",
      "intercept_:0.22515593157564343\n",
      "train_loss:0.2668167641127995\n",
      "val_loss:0.46869923588325507\n",
      "iter:278\n",
      "coef_:[-0.27918777  3.14838071]\n",
      "intercept_:0.22519980279431684\n",
      "train_loss:0.26680300224095344\n",
      "val_loss:0.4688587754192786\n",
      "iter:279\n",
      "coef_:[-0.28022677  3.15010943]\n",
      "intercept_:0.22524350703555943\n",
      "train_loss:0.26678950031465276\n",
      "val_loss:0.46901711165305754\n",
      "iter:280\n",
      "coef_:[-0.28125512  3.15182221]\n",
      "intercept_:0.22528704213459133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:0.26677625331599575\n",
      "val_loss:0.46917425074415886\n",
      "iter:281\n",
      "coef_:[-0.28227294  3.15351921]\n",
      "intercept_:0.22533040600477985\n",
      "train_loss:0.26676325632698855\n",
      "val_loss:0.4693301988929052\n",
      "iter:282\n",
      "coef_:[-0.28328036  3.15520058]\n",
      "intercept_:0.2253735966359462\n",
      "train_loss:0.2667505045274698\n",
      "val_loss:0.4694849623379328\n",
      "iter:283\n",
      "coef_:[-0.28427747  3.15686647]\n",
      "intercept_:0.22541661209270789\n",
      "train_loss:0.2667379931930804\n",
      "val_loss:0.46963854735382\n",
      "iter:284\n",
      "coef_:[-0.28526441  3.15851701]\n",
      "intercept_:0.225459450512856\n",
      "train_loss:0.2667257176932791\n",
      "val_loss:0.4697909602487826\n",
      "iter:285\n",
      "coef_:[-0.28624128  3.16015235]\n",
      "intercept_:0.2255021101057661\n",
      "train_loss:0.26671367348939923\n",
      "val_loss:0.469942207362436\n",
      "iter:286\n",
      "coef_:[-0.28720819  3.16177264]\n",
      "intercept_:0.22554458915084213\n",
      "train_loss:0.2667018561327505\n",
      "val_loss:0.4700922950636214\n",
      "iter:287\n",
      "coef_:[-0.28816526  3.16337801]\n",
      "intercept_:0.22558688599599266\n",
      "train_loss:0.2666902612627599\n",
      "val_loss:0.470241229748294\n",
      "iter:288\n",
      "coef_:[-0.28911259  3.1649686 ]\n",
      "intercept_:0.22562899905613826\n",
      "train_loss:0.26667888460515493\n",
      "val_loss:0.4703890178374736\n",
      "iter:289\n",
      "coef_:[-0.2900503   3.16654456]\n",
      "intercept_:0.22567092681174947\n",
      "train_loss:0.266667721970185\n",
      "val_loss:0.4705356657752537\n",
      "iter:290\n",
      "coef_:[-0.29097848  3.16810602]\n",
      "intercept_:0.22571266780741475\n",
      "train_loss:0.2666567692508824\n",
      "val_loss:0.47068118002686815\n",
      "iter:291\n",
      "coef_:[-0.29189725  3.1696531 ]\n",
      "intercept_:0.2257542206504374\n",
      "train_loss:0.2666460224213601\n",
      "val_loss:0.4708255670768156\n",
      "iter:292\n",
      "coef_:[-0.2928067   3.17118596]\n",
      "intercept_:0.22579558400946084\n",
      "train_loss:0.2666354775351474\n",
      "val_loss:0.4709688334270379\n",
      "iter:293\n",
      "coef_:[-0.29370694  3.17270472]\n",
      "intercept_:0.22583675661312175\n",
      "train_loss:0.2666251307235607\n",
      "val_loss:0.47111098559515197\n",
      "iter:294\n",
      "coef_:[-0.29459808  3.17420951]\n",
      "intercept_:0.22587773724873017\n",
      "train_loss:0.2666149781941101\n",
      "val_loss:0.47125203011273425\n",
      "iter:295\n",
      "coef_:[-0.2954802   3.17570046]\n",
      "intercept_:0.22591852476097618\n",
      "train_loss:0.2666050162289397\n",
      "val_loss:0.4713919735236564\n",
      "iter:296\n",
      "coef_:[-0.29635341  3.1771777 ]\n",
      "intercept_:0.22595911805066218\n",
      "train_loss:0.26659524118330213\n",
      "val_loss:0.4715308223824697\n",
      "iter:297\n",
      "coef_:[-0.29721781  3.17864136]\n",
      "intercept_:0.22599951607346058\n",
      "train_loss:0.2665856494840654\n",
      "val_loss:0.4716685832528378\n",
      "iter:298\n",
      "coef_:[-0.2980735   3.18009157]\n",
      "intercept_:0.22603971783869614\n",
      "train_loss:0.2665762376282524\n",
      "val_loss:0.4718052627060172\n",
      "iter:299\n",
      "coef_:[-0.29892056  3.18152845]\n",
      "intercept_:0.22607972240815224\n",
      "train_loss:0.26656700218161034\n",
      "val_loss:0.471940867319383\n",
      "iter:300\n",
      "coef_:[-0.2997591   3.18295213]\n",
      "intercept_:0.22611952889490097\n",
      "train_loss:0.2665579397772131\n",
      "val_loss:0.47207540367499873\n",
      "iter:301\n",
      "coef_:[-0.3005892   3.18436272]\n",
      "intercept_:0.22615913646215602\n",
      "train_loss:0.26654904711409094\n",
      "val_loss:0.4722088783582312\n",
      "iter:302\n",
      "coef_:[-0.30141096  3.18576035]\n",
      "intercept_:0.22619854432214814\n",
      "train_loss:0.2665403209558914\n",
      "val_loss:0.472341297956406\n",
      "iter:303\n",
      "coef_:[-0.30222446  3.18714514]\n",
      "intercept_:0.2262377517350228\n",
      "train_loss:0.2665317581295676\n",
      "val_loss:0.47247266905750496\n",
      "iter:304\n",
      "coef_:[-0.3030298   3.18851722]\n",
      "intercept_:0.22627675800775915\n",
      "train_loss:0.2665233555240952\n",
      "val_loss:0.4726029982489045\n",
      "iter:305\n",
      "coef_:[-0.30382707  3.18987669]\n",
      "intercept_:0.22631556249311024\n",
      "train_loss:0.26651511008921597\n",
      "val_loss:0.47273229211615186\n",
      "iter:306\n",
      "coef_:[-0.30461635  3.19122368]\n",
      "intercept_:0.22635416458856383\n",
      "train_loss:0.2665070188342088\n",
      "val_loss:0.4728605572417803\n",
      "iter:307\n",
      "coef_:[-0.30539773  3.19255829]\n",
      "intercept_:0.22639256373532335\n",
      "train_loss:0.2664990788266859\n",
      "val_loss:0.4729878002041612\n",
      "iter:308\n",
      "coef_:[-0.3061713   3.19388066]\n",
      "intercept_:0.2264307594173087\n",
      "train_loss:0.2664912871914151\n",
      "val_loss:0.47311402757639287\n",
      "iter:309\n",
      "coef_:[-0.30693713  3.19519089]\n",
      "intercept_:0.22646875116017637\n",
      "train_loss:0.2664836411091674\n",
      "val_loss:0.47323924592522315\n",
      "iter:310\n",
      "coef_:[-0.30769531  3.19648909]\n",
      "intercept_:0.22650653853035846\n",
      "train_loss:0.2664761378155875\n",
      "val_loss:0.47336346181000893\n",
      "iter:311\n",
      "coef_:[-0.30844593  3.19777537]\n",
      "intercept_:0.2265441211341204\n",
      "train_loss:0.26646877460008966\n",
      "val_loss:0.4734866817817057\n",
      "iter:312\n",
      "coef_:[-0.30918907  3.19904985]\n",
      "intercept_:0.22658149861663665\n",
      "train_loss:0.266461548804776\n",
      "val_loss:0.4736089123818937\n",
      "iter:313\n",
      "coef_:[-0.3099248   3.20031264]\n",
      "intercept_:0.2266186706610844\n",
      "train_loss:0.26645445782337784\n",
      "val_loss:0.47373016014183195\n",
      "iter:314\n",
      "coef_:[-0.3106532   3.20156384]\n",
      "intercept_:0.22665563698775465\n",
      "train_loss:0.2664474991002189\n",
      "val_loss:0.47385043158154644\n",
      "iter:315\n",
      "coef_:[-0.31137436  3.20280356]\n",
      "intercept_:0.22669239735318028\n",
      "train_loss:0.2664406701292009\n",
      "val_loss:0.47396973320894614\n",
      "iter:316\n",
      "coef_:[-0.31208835  3.20403191]\n",
      "intercept_:0.226728951549281\n",
      "train_loss:0.2664339684528101\n",
      "val_loss:0.4740880715189696\n",
      "iter:317\n",
      "coef_:[-0.31279524  3.20524899]\n",
      "intercept_:0.2267652994025247\n",
      "train_loss:0.2664273916611441\n",
      "val_loss:0.4742054529927601\n",
      "iter:318\n",
      "coef_:[-0.31349512  3.20645491]\n",
      "intercept_:0.2268014407731049\n",
      "train_loss:0.2664209373909603\n",
      "val_loss:0.4743218840968684\n",
      "iter:319\n",
      "coef_:[-0.31418806  3.20764978]\n",
      "intercept_:0.22683737555413389\n",
      "train_loss:0.2664146033247426\n",
      "val_loss:0.47443737128248165\n",
      "iter:320\n",
      "coef_:[-0.31487412  3.20883368]\n",
      "intercept_:0.2268731036708513\n",
      "train_loss:0.26640838718978926\n",
      "val_loss:0.4745519209846813\n",
      "iter:321\n",
      "coef_:[-0.31555339  3.21000674]\n",
      "intercept_:0.22690862507984821\n",
      "train_loss:0.2664022867573182\n",
      "val_loss:0.47466553962172425\n",
      "iter:322\n",
      "coef_:[-0.31622594  3.21116904]\n",
      "intercept_:0.22694393976830557\n",
      "train_loss:0.2663962998415922\n",
      "val_loss:0.47477823359434995\n",
      "iter:323\n",
      "coef_:[-0.31689184  3.21232068]\n",
      "intercept_:0.22697904775324762\n",
      "train_loss:0.2663904242990611\n",
      "val_loss:0.4748900092851123\n",
      "iter:324\n",
      "coef_:[-0.31755115  3.21346177]\n",
      "intercept_:0.22701394908080944\n",
      "train_loss:0.26638465802752315\n",
      "val_loss:0.4750008730577352\n",
      "iter:325\n",
      "coef_:[-0.31820395  3.21459241]\n",
      "intercept_:0.22704864382551865\n",
      "train_loss:0.2663789989653023\n",
      "val_loss:0.4751108312564904\n",
      "iter:326\n",
      "coef_:[-0.31885031  3.21571268]\n",
      "intercept_:0.2270831320895907\n",
      "train_loss:0.26637344509044397\n",
      "val_loss:0.4752198902055994\n",
      "iter:327\n",
      "coef_:[-0.31949029  3.21682269]\n",
      "intercept_:0.22711741400223776\n",
      "train_loss:0.26636799441992604\n",
      "val_loss:0.47532805620865615\n",
      "iter:328\n",
      "coef_:[-0.32012396  3.21792253]\n",
      "intercept_:0.22715148971899093\n",
      "train_loss:0.2663626450088874\n",
      "val_loss:0.47543533554807127\n",
      "iter:329\n",
      "coef_:[-0.32075139  3.21901229]\n",
      "intercept_:0.2271853594210354\n",
      "train_loss:0.2663573949498717\n",
      "val_loss:0.47554173448453807\n",
      "iter:330\n",
      "coef_:[-0.32137264  3.22009207]\n",
      "intercept_:0.22721902331455823\n",
      "train_loss:0.2663522423720866\n",
      "val_loss:0.4756472592565174\n",
      "iter:331\n",
      "coef_:[-0.32198778  3.22116196]\n",
      "intercept_:0.22725248163010883\n",
      "train_loss:0.2663471854406791\n",
      "val_loss:0.4757519160797436\n",
      "iter:332\n",
      "coef_:[-0.32259688  3.22222205]\n",
      "intercept_:0.22728573462197169\n",
      "train_loss:0.26634222235602456\n",
      "val_loss:0.47585571114674863\n",
      "iter:333\n",
      "coef_:[-0.32319999  3.22327244]\n",
      "intercept_:0.2273187825675511\n",
      "train_loss:0.26633735135303155\n",
      "val_loss:0.4759586506264059\n",
      "iter:334\n",
      "coef_:[-0.32379717  3.2243132 ]\n",
      "intercept_:0.22735162576676776\n",
      "train_loss:0.2663325707004601\n",
      "val_loss:0.47606074066349136\n",
      "iter:335\n",
      "coef_:[-0.3243885   3.22534444]\n",
      "intercept_:0.227384264541467\n",
      "train_loss:0.2663278787002543\n",
      "val_loss:0.4761619873782633\n",
      "iter:336\n",
      "coef_:[-0.32497403  3.22636624]\n",
      "intercept_:0.22741669923483848\n",
      "train_loss:0.26632327368688874\n",
      "val_loss:0.4762623968660585\n",
      "iter:337\n",
      "coef_:[-0.32555382  3.22737868]\n",
      "intercept_:0.22744893021084683\n",
      "train_loss:0.26631875402672744\n",
      "val_loss:0.4763619751969058\n",
      "iter:338\n",
      "coef_:[-0.32612794  3.22838186]\n",
      "intercept_:0.22748095785367364\n",
      "train_loss:0.2663143181173973\n",
      "val_loss:0.47646072841515663\n",
      "iter:339\n",
      "coef_:[-0.32669643  3.22937585]\n",
      "intercept_:0.22751278256717\n",
      "train_loss:0.2663099643871733\n",
      "val_loss:0.4765586625391291\n",
      "iter:340\n",
      "coef_:[-0.32725936  3.23036075]\n",
      "intercept_:0.22754440477431973\n",
      "train_loss:0.26630569129437615\n",
      "val_loss:0.4766557835607713\n",
      "iter:341\n",
      "coef_:[-0.3278168   3.23133664]\n",
      "intercept_:0.22757582491671294\n",
      "train_loss:0.2663014973267832\n",
      "val_loss:0.47675209744533636\n",
      "iter:342\n",
      "coef_:[-0.32836878  3.23230359]\n",
      "intercept_:0.22760704345403\n",
      "train_loss:0.2662973810010502\n",
      "val_loss:0.4768476101310739\n",
      "iter:343\n",
      "coef_:[-0.32891538  3.2332617 ]\n",
      "intercept_:0.22763806086353527\n",
      "train_loss:0.2662933408621458\n",
      "val_loss:0.4769423275289354\n",
      "iter:344\n",
      "coef_:[-0.32945664  3.23421105]\n",
      "intercept_:0.2276688776395809\n",
      "train_loss:0.26628937548279646\n",
      "val_loss:0.4770362555222937\n",
      "iter:345\n",
      "coef_:[-0.32999262  3.2351517 ]\n",
      "intercept_:0.22769949429312025\n",
      "train_loss:0.26628548346294406\n",
      "val_loss:0.4771293999666755\n",
      "iter:346\n",
      "coef_:[-0.33052338  3.23608376]\n",
      "intercept_:0.22772991135123077\n",
      "train_loss:0.2662816634292127\n",
      "val_loss:0.4772217666895081\n",
      "iter:347\n",
      "coef_:[-0.33104897  3.23700729]\n",
      "intercept_:0.22776012935664622\n",
      "train_loss:0.26627791403438833\n",
      "val_loss:0.47731336148987724\n",
      "iter:348\n",
      "coef_:[-0.33156945  3.23792237]\n",
      "intercept_:0.22779014886729818\n",
      "train_loss:0.2662742339569069\n",
      "val_loss:0.4774041901382995\n",
      "iter:349\n",
      "coef_:[-0.33208486  3.23882909]\n",
      "intercept_:0.2278199704558664\n",
      "train_loss:0.26627062190035466\n",
      "val_loss:0.4774942583765051\n",
      "iter:350\n",
      "coef_:[-0.33259526  3.23972751]\n",
      "intercept_:0.2278495947093381\n",
      "train_loss:0.2662670765929769\n",
      "val_loss:0.4775835719172336\n",
      "iter:351\n",
      "coef_:[-0.33310069  3.24061772]\n",
      "intercept_:0.22787902222857595\n",
      "train_loss:0.26626359678719813\n",
      "val_loss:0.4776721364440405\n",
      "iter:352\n",
      "coef_:[-0.33360122  3.2414998 ]\n",
      "intercept_:0.22790825362789452\n",
      "train_loss:0.26626018125915063\n",
      "val_loss:0.47775995761111484\n",
      "iter:353\n",
      "coef_:[-0.33409689  3.2423738 ]\n",
      "intercept_:0.22793728953464518\n",
      "train_loss:0.26625682880821366\n",
      "val_loss:0.4778470410431089\n",
      "iter:354\n",
      "coef_:[-0.33458776  3.24323982]\n",
      "intercept_:0.22796613058880927\n",
      "train_loss:0.26625353825656073\n",
      "val_loss:0.4779333923349768\n",
      "iter:355\n",
      "coef_:[-0.33507386  3.24409792]\n",
      "intercept_:0.22799477744259924\n",
      "train_loss:0.26625030844871705\n",
      "val_loss:0.47801901705182476\n",
      "iter:356\n",
      "coef_:[-0.33555525  3.24494818]\n",
      "intercept_:0.22802323076006786\n",
      "train_loss:0.26624713825112517\n",
      "val_loss:0.4781039207287698\n",
      "iter:357\n",
      "coef_:[-0.33603198  3.24579067]\n",
      "intercept_:0.22805149121672513\n",
      "train_loss:0.26624402655171964\n",
      "val_loss:0.4781881088708104\n",
      "iter:358\n",
      "coef_:[-0.33650409  3.24662546]\n",
      "intercept_:0.22807955949916298\n",
      "train_loss:0.26624097225951016\n",
      "val_loss:0.4782715869527045\n",
      "iter:359\n",
      "coef_:[-0.33697164  3.24745262]\n",
      "intercept_:0.22810743630468736\n",
      "train_loss:0.266237974304173\n",
      "val_loss:0.47835436041885715\n",
      "iter:360\n",
      "coef_:[-0.33743467  3.24827222]\n",
      "intercept_:0.22813512234095787\n",
      "train_loss:0.2662350316356507\n",
      "val_loss:0.47843643468321845\n",
      "iter:361\n",
      "coef_:[-0.33789322  3.24908434]\n",
      "intercept_:0.22816261832563464\n",
      "train_loss:0.2662321432237596\n",
      "val_loss:0.478517815129188\n",
      "iter:362\n",
      "coef_:[-0.33834734  3.24988903]\n",
      "intercept_:0.22818992498603224\n",
      "train_loss:0.26622930805780537\n",
      "val_loss:0.47859850710952967\n",
      "iter:363\n",
      "coef_:[-0.33879707  3.25068638]\n",
      "intercept_:0.2282170430587807\n",
      "train_loss:0.26622652514620615\n",
      "val_loss:0.4786785159462934\n",
      "iter:364\n",
      "coef_:[-0.33924247  3.25147644]\n",
      "intercept_:0.2282439732894935\n",
      "train_loss:0.2662237935161234\n",
      "val_loss:0.4787578469307454\n",
      "iter:365\n",
      "coef_:[-0.33968357  3.25225928]\n",
      "intercept_:0.22827071643244212\n",
      "train_loss:0.2662211122130996\n",
      "val_loss:0.4788365053233059\n",
      "iter:366\n",
      "coef_:[-0.34012041  3.25303498]\n",
      "intercept_:0.22829727325023744\n",
      "train_loss:0.26621848030070355\n",
      "val_loss:0.47891449635349453\n",
      "iter:367\n",
      "coef_:[-0.34055305  3.25380359]\n",
      "intercept_:0.22832364451351764\n",
      "train_loss:0.266215896860183\n",
      "val_loss:0.4789918252198829\n",
      "iter:368\n",
      "coef_:[-0.34098152  3.25456518]\n",
      "intercept_:0.22834983100064243\n",
      "train_loss:0.26621336099012316\n",
      "val_loss:0.47906849709005445\n",
      "iter:369\n",
      "coef_:[-0.34140587  3.25531982]\n",
      "intercept_:0.2283758334973937\n",
      "train_loss:0.2662108718061136\n",
      "val_loss:0.4791445171005708\n",
      "iter:370\n",
      "coef_:[-0.34182614  3.25606756]\n",
      "intercept_:0.22840165279668231\n",
      "train_loss:0.26620842844041986\n",
      "val_loss:0.4792198903569448\n",
      "iter:371\n",
      "coef_:[-0.34224236  3.25680848]\n",
      "intercept_:0.22842728969826095\n",
      "train_loss:0.26620603004166316\n",
      "val_loss:0.47929462193362077\n",
      "iter:372\n",
      "coef_:[-0.34265458  3.25754264]\n",
      "intercept_:0.2284527450084431\n",
      "train_loss:0.2662036757745056\n",
      "val_loss:0.47936871687396015\n",
      "iter:373\n",
      "coef_:[-0.34306284  3.25827009]\n",
      "intercept_:0.22847801953982763\n",
      "train_loss:0.26620136481934187\n",
      "val_loss:0.4794421801902331\n",
      "iter:374\n",
      "coef_:[-0.34346718  3.2589909 ]\n",
      "intercept_:0.22850311411102955\n",
      "train_loss:0.2661990963719968\n",
      "val_loss:0.4795150168636175\n",
      "iter:375\n",
      "coef_:[-0.34386764  3.25970514]\n",
      "intercept_:0.22852802954641602\n",
      "train_loss:0.2661968696434295\n",
      "val_loss:0.47958723184420166\n",
      "iter:376\n",
      "coef_:[-0.34426426  3.26041285]\n",
      "intercept_:0.2285527666758482\n",
      "train_loss:0.26619468385944256\n",
      "val_loss:0.47965883005099413\n",
      "iter:377\n",
      "coef_:[-0.34465708  3.26111411]\n",
      "intercept_:0.22857732633442848\n",
      "train_loss:0.2661925382603974\n",
      "val_loss:0.4797298163719381\n",
      "iter:378\n",
      "coef_:[-0.34504613  3.26180896]\n",
      "intercept_:0.22860170936225313\n",
      "train_loss:0.2661904321009356\n",
      "val_loss:0.4798001956639315\n",
      "iter:379\n",
      "coef_:[-0.34543145  3.26249748]\n",
      "intercept_:0.2286259166041702\n",
      "train_loss:0.26618836464970463\n",
      "val_loss:0.4798699727528518\n",
      "iter:380\n",
      "coef_:[-0.34581309  3.26317971]\n",
      "intercept_:0.22864994890954257\n",
      "train_loss:0.26618633518908996\n",
      "val_loss:0.479939152433586\n",
      "iter:381\n",
      "coef_:[-0.34619107  3.26385571]\n",
      "intercept_:0.2286738071320162\n",
      "train_loss:0.26618434301495253\n",
      "val_loss:0.48000773947006503\n",
      "iter:382\n",
      "coef_:[-0.34656543  3.26452555]\n",
      "intercept_:0.22869749212929333\n",
      "train_loss:0.26618238743637024\n",
      "val_loss:0.4800757385953031\n",
      "iter:383\n",
      "coef_:[-0.34693621  3.26518928]\n",
      "intercept_:0.22872100476291052\n",
      "train_loss:0.26618046777538573\n",
      "val_loss:0.48014315451144157\n",
      "iter:384\n",
      "coef_:[-0.34730345  3.26584695]\n",
      "intercept_:0.2287443458980217\n",
      "train_loss:0.2661785833667586\n",
      "val_loss:0.48020999188979674\n",
      "iter:385\n",
      "coef_:[-0.34766718  3.26649862]\n",
      "intercept_:0.22876751640318585\n",
      "train_loss:0.2661767335577224\n",
      "val_loss:0.4802762553709121\n",
      "iter:386\n",
      "coef_:[-0.34802744  3.26714435]\n",
      "intercept_:0.22879051715015927\n",
      "train_loss:0.26617491770774665\n",
      "val_loss:0.4803419495646147\n",
      "iter:387\n",
      "coef_:[-0.34838426  3.26778419]\n",
      "intercept_:0.2288133490136926\n",
      "train_loss:0.26617313518830316\n",
      "val_loss:0.48040707905007524\n",
      "iter:388\n",
      "coef_:[-0.34873767  3.26841819]\n",
      "intercept_:0.22883601287133218\n",
      "train_loss:0.2661713853826373\n",
      "val_loss:0.4804716483758719\n",
      "iter:389\n",
      "coef_:[-0.34908771  3.26904641]\n",
      "intercept_:0.22885850960322598\n",
      "train_loss:0.26616966768554357\n",
      "val_loss:0.4805356620600587\n",
      "iter:390\n",
      "coef_:[-0.34943442  3.2696689 ]\n",
      "intercept_:0.22888084009193374\n",
      "train_loss:0.2661679815031455\n",
      "val_loss:0.48059912459023535\n",
      "iter:391\n",
      "coef_:[-0.34977782  3.27028571]\n",
      "intercept_:0.22890300522224136\n",
      "train_loss:0.26616632625268\n",
      "val_loss:0.4806620404236233\n",
      "iter:392\n",
      "coef_:[-0.35011794  3.2708969 ]\n",
      "intercept_:0.22892500588097958\n",
      "train_loss:0.26616470136228587\n",
      "val_loss:0.4807244139871428\n",
      "iter:393\n",
      "coef_:[-0.35045483  3.27150252]\n",
      "intercept_:0.22894684295684672\n",
      "train_loss:0.26616310627079603\n",
      "val_loss:0.48078624967749434\n",
      "iter:394\n",
      "coef_:[-0.35078852  3.27210261]\n",
      "intercept_:0.2289685173402354\n",
      "train_loss:0.2661615404275347\n",
      "val_loss:0.48084755186124256\n",
      "iter:395\n",
      "coef_:[-0.35111902  3.27269723]\n",
      "intercept_:0.22899002992306333\n",
      "train_loss:0.26616000329211764\n",
      "val_loss:0.48090832487490437\n",
      "iter:396\n",
      "coef_:[-0.35144639  3.27328643]\n",
      "intercept_:0.22901138159860784\n",
      "train_loss:0.26615849433425676\n",
      "val_loss:0.4809685730250377\n",
      "iter:397\n",
      "coef_:[-0.35177064  3.27387026]\n",
      "intercept_:0.2290325732613444\n",
      "train_loss:0.2661570130335683\n",
      "val_loss:0.48102830058833546\n",
      "iter:398\n",
      "coef_:[-0.35209181  3.27444877]\n",
      "intercept_:0.22905360580678877\n",
      "train_loss:0.26615555887938513\n",
      "val_loss:0.4810875118117206\n",
      "iter:399\n",
      "coef_:[-0.35240993  3.275022  ]\n",
      "intercept_:0.22907448013134285\n",
      "train_loss:0.26615413137057187\n",
      "val_loss:0.48114621091244403\n",
      "iter:400\n",
      "coef_:[-0.35272502  3.27559001]\n",
      "intercept_:0.22909519713214418\n",
      "train_loss:0.2661527300153445\n",
      "val_loss:0.4812044020781861\n",
      "iter:401\n",
      "coef_:[-0.35303713  3.27615285]\n",
      "intercept_:0.22911575770691905\n",
      "train_loss:0.26615135433109344\n",
      "val_loss:0.48126208946715865\n",
      "iter:402\n",
      "coef_:[-0.35334628  3.27671055]\n",
      "intercept_:0.22913616275383888\n",
      "train_loss:0.2661500038442087\n",
      "val_loss:0.4813192772082108\n",
      "iter:403\n",
      "coef_:[-0.35365249  3.27726317]\n",
      "intercept_:0.2291564131713803\n",
      "train_loss:0.2661486780899106\n",
      "val_loss:0.48137596940093663\n",
      "iter:404\n",
      "coef_:[-0.3539558   3.27781075]\n",
      "intercept_:0.22917650985818838\n",
      "train_loss:0.26614737661208165\n",
      "val_loss:0.48143217011578443\n",
      "iter:405\n",
      "coef_:[-0.35425623  3.27835334]\n",
      "intercept_:0.22919645371294323\n",
      "train_loss:0.26614609896310354\n",
      "val_loss:0.4814878833941688\n",
      "iter:406\n",
      "coef_:[-0.35455382  3.27889099]\n",
      "intercept_:0.22921624563422985\n",
      "train_loss:0.26614484470369554\n",
      "val_loss:0.48154311324858406\n",
      "iter:407\n",
      "coef_:[-0.35484859  3.27942374]\n",
      "intercept_:0.22923588652041121\n",
      "train_loss:0.2661436134027573\n",
      "val_loss:0.48159786366272017\n",
      "iter:408\n",
      "coef_:[-0.35514056  3.27995164]\n",
      "intercept_:0.22925537726950426\n",
      "train_loss:0.2661424046372142\n",
      "val_loss:0.48165213859157974\n",
      "iter:409\n",
      "coef_:[-0.35542978  3.28047472]\n",
      "intercept_:0.22927471877905925\n",
      "train_loss:0.2661412179918658\n",
      "val_loss:0.48170594196159766\n",
      "iter:410\n",
      "coef_:[-0.35571626  3.28099304]\n",
      "intercept_:0.22929391194604187\n",
      "train_loss:0.26614005305923694\n",
      "val_loss:0.4817592776707616\n",
      "iter:411\n",
      "coef_:[-0.35600002  3.28150664]\n",
      "intercept_:0.22931295766671844\n",
      "train_loss:0.26613890943943214\n",
      "val_loss:0.4818121495887342\n",
      "iter:412\n",
      "coef_:[-0.35628111  3.28201556]\n",
      "intercept_:0.2293318568365439\n",
      "train_loss:0.26613778673999244\n",
      "val_loss:0.48186456155697777\n",
      "iter:413\n",
      "coef_:[-0.35655954  3.28251984]\n",
      "intercept_:0.22935061035005275\n",
      "train_loss:0.2661366845757552\n",
      "val_loss:0.48191651738887886\n",
      "iter:414\n",
      "coef_:[-0.35683534  3.28301953]\n",
      "intercept_:0.22936921910075272\n",
      "train_loss:0.26613560256871654\n",
      "val_loss:0.4819680208698757\n",
      "iter:415\n",
      "coef_:[-0.35710853  3.28351467]\n",
      "intercept_:0.22938768398102125\n",
      "train_loss:0.2661345403478965\n",
      "val_loss:0.48201907575758585\n",
      "iter:416\n",
      "coef_:[-0.35737914  3.28400529]\n",
      "intercept_:0.2294060058820044\n",
      "train_loss:0.2661334975492065\n",
      "val_loss:0.482069685781936\n",
      "iter:417\n",
      "coef_:[-0.3576472   3.28449145]\n",
      "intercept_:0.2294241856935188\n",
      "train_loss:0.2661324738153198\n",
      "val_loss:0.4821198546452923\n",
      "iter:418\n",
      "coef_:[-0.35791273  3.28497318]\n",
      "intercept_:0.22944222430395586\n",
      "train_loss:0.2661314687955441\n",
      "val_loss:0.4821695860225924\n",
      "iter:419\n",
      "coef_:[-0.35817576  3.28545053]\n",
      "intercept_:0.22946012260018864\n",
      "train_loss:0.2661304821456968\n",
      "val_loss:0.48221888356147796\n",
      "iter:420\n",
      "coef_:[-0.3584363   3.28592352]\n",
      "intercept_:0.2294778814674812\n",
      "train_loss:0.26612951352798236\n",
      "val_loss:0.4822677508824289\n",
      "iter:421\n",
      "coef_:[-0.35869439  3.28639221]\n",
      "intercept_:0.22949550178940034\n",
      "train_loss:0.2661285626108723\n",
      "val_loss:0.4823161915788981\n",
      "iter:422\n",
      "coef_:[-0.35895005  3.28685663]\n",
      "intercept_:0.22951298444772994\n",
      "train_loss:0.26612762906898757\n",
      "val_loss:0.4823642092174471\n",
      "iter:423\n",
      "coef_:[-0.3592033   3.28731683]\n",
      "intercept_:0.22953033032238726\n",
      "train_loss:0.2661267125829827\n",
      "val_loss:0.48241180733788314\n",
      "iter:424\n",
      "coef_:[-0.35945416  3.28777283]\n",
      "intercept_:0.22954754029134197\n",
      "train_loss:0.2661258128394329\n",
      "val_loss:0.48245898945339616\n",
      "iter:425\n",
      "coef_:[-0.35970267  3.28822468]\n",
      "intercept_:0.22956461523053714\n",
      "train_loss:0.26612492953072214\n",
      "val_loss:0.4825057590506979\n",
      "iter:426\n",
      "coef_:[-0.35994883  3.28867242]\n",
      "intercept_:0.22958155601381255\n",
      "train_loss:0.26612406235493513\n",
      "val_loss:0.4825521195901598\n",
      "iter:427\n",
      "coef_:[-0.36019268  3.28911608]\n",
      "intercept_:0.22959836351283025\n",
      "train_loss:0.26612321101574954\n",
      "val_loss:0.48259807450595404\n",
      "iter:428\n",
      "coef_:[-0.36043423  3.2895557 ]\n",
      "intercept_:0.22961503859700214\n",
      "train_loss:0.2661223752223314\n",
      "val_loss:0.4826436272061924\n",
      "iter:429\n",
      "coef_:[-0.36067352  3.28999132]\n",
      "intercept_:0.22963158213341966\n",
      "train_loss:0.2661215546892323\n",
      "val_loss:0.48268878107306884\n",
      "iter:430\n",
      "coef_:[-0.36091055  3.29042297]\n",
      "intercept_:0.2296479949867856\n",
      "train_loss:0.26612074913628836\n",
      "val_loss:0.48273353946299963\n",
      "iter:431\n",
      "coef_:[-0.36114536  3.29085069]\n",
      "intercept_:0.22966427801934777\n",
      "train_loss:0.2661199582885211\n",
      "val_loss:0.4827779057067661\n",
      "iter:432\n",
      "coef_:[-0.36137796  3.29127452]\n",
      "intercept_:0.2296804320908349\n",
      "train_loss:0.2661191818760402\n",
      "val_loss:0.48282188310965696\n",
      "iter:433\n",
      "coef_:[-0.36160837  3.29169449]\n",
      "intercept_:0.22969645805839423\n",
      "train_loss:0.266118419633949\n",
      "val_loss:0.48286547495161114\n",
      "iter:434\n",
      "coef_:[-0.36183663  3.29211064]\n",
      "intercept_:0.22971235677653107\n",
      "train_loss:0.2661176713022499\n",
      "val_loss:0.48290868448736135\n",
      "iter:435\n",
      "coef_:[-0.36206274  3.29252301]\n",
      "intercept_:0.22972812909705026\n",
      "train_loss:0.26611693662575353\n",
      "val_loss:0.48295151494657756\n",
      "iter:436\n",
      "coef_:[-0.36228673  3.29293162]\n",
      "intercept_:0.22974377586899944\n",
      "train_loss:0.2661162153539884\n",
      "val_loss:0.48299396953401086\n",
      "iter:437\n",
      "coef_:[-0.36250862  3.29333651]\n",
      "intercept_:0.22975929793861408\n",
      "train_loss:0.2661155072411129\n",
      "val_loss:0.4830360514296384\n",
      "iter:438\n",
      "coef_:[-0.36272843  3.29373771]\n",
      "intercept_:0.22977469614926424\n",
      "train_loss:0.2661148120458285\n",
      "val_loss:0.4830777637888069\n",
      "iter:439\n",
      "coef_:[-0.36294617  3.29413527]\n",
      "intercept_:0.22978997134140314\n",
      "train_loss:0.26611412953129515\n",
      "val_loss:0.4831191097423782\n",
      "iter:440\n",
      "coef_:[-0.36316188  3.2945292 ]\n",
      "intercept_:0.2298051243525173\n",
      "train_loss:0.2661134594650477\n",
      "val_loss:0.48316009239687446\n",
      "iter:441\n",
      "coef_:[-0.36337556  3.29491956]\n",
      "intercept_:0.22982015601707834\n",
      "train_loss:0.2661128016189145\n",
      "val_loss:0.483200714834622\n",
      "iter:442\n",
      "coef_:[-0.36358725  3.29530635]\n",
      "intercept_:0.22983506716649654\n",
      "train_loss:0.2661121557689367\n",
      "val_loss:0.48324098011389777\n",
      "iter:443\n",
      "coef_:[-0.36379695  3.29568964]\n",
      "intercept_:0.22984985862907584\n",
      "train_loss:0.2661115216952904\n",
      "val_loss:0.4832808912690744\n",
      "iter:444\n",
      "coef_:[-0.36400469  3.29606943]\n",
      "intercept_:0.2298645312299703\n",
      "train_loss:0.2661108991822086\n",
      "val_loss:0.48332045131076506\n",
      "iter:445\n",
      "coef_:[-0.36421048  3.29644577]\n",
      "intercept_:0.22987908579114238\n",
      "train_loss:0.2661102880179065\n",
      "val_loss:0.48335966322596957\n",
      "iter:446\n",
      "coef_:[-0.36441435  3.29681868]\n",
      "intercept_:0.2298935231313224\n",
      "train_loss:0.26610968799450624\n",
      "val_loss:0.4833985299782193\n",
      "iter:447\n",
      "coef_:[-0.36461631  3.2971882 ]\n",
      "intercept_:0.2299078440659697\n",
      "train_loss:0.2661090989079648\n",
      "val_loss:0.4834370545077232\n",
      "iter:448\n",
      "coef_:[-0.36481638  3.29755436]\n",
      "intercept_:0.229922049407235\n",
      "train_loss:0.2661085205580021\n",
      "val_loss:0.4834752397315125\n",
      "iter:449\n",
      "coef_:[-0.36501458  3.29791719]\n",
      "intercept_:0.2299361399639244\n",
      "train_loss:0.2661079527480314\n",
      "val_loss:0.48351308854358693\n",
      "iter:450\n",
      "coef_:[-0.36521093  3.29827671]\n",
      "intercept_:0.2299501165414645\n",
      "train_loss:0.2661073952850897\n",
      "val_loss:0.48355060381505865\n",
      "iter:451\n",
      "coef_:[-0.36540544  3.29863297]\n",
      "intercept_:0.22996397994186907\n",
      "train_loss:0.266106847979771\n",
      "val_loss:0.4835877883942991\n",
      "iter:452\n",
      "coef_:[-0.36559814  3.29898598]\n",
      "intercept_:0.22997773096370694\n",
      "train_loss:0.26610631064615975\n",
      "val_loss:0.4836246451070825\n",
      "iter:453\n",
      "coef_:[-0.36578904  3.29933579]\n",
      "intercept_:0.22999137040207115\n",
      "train_loss:0.26610578310176586\n",
      "val_loss:0.4836611767567314\n",
      "iter:454\n",
      "coef_:[-0.36597815  3.29968241]\n",
      "intercept_:0.23000489904854948\n",
      "train_loss:0.2661052651674611\n",
      "val_loss:0.4836973861242615\n",
      "iter:455\n",
      "coef_:[-0.3661655   3.30002588]\n",
      "intercept_:0.23001831769119604\n",
      "train_loss:0.2661047566674165\n",
      "val_loss:0.48373327596852567\n",
      "iter:456\n",
      "coef_:[-0.3663511   3.30036623]\n",
      "intercept_:0.2300316271145042\n",
      "train_loss:0.26610425742904087\n",
      "val_loss:0.48376884902635836\n",
      "iter:457\n",
      "coef_:[-0.36653497  3.30070348]\n",
      "intercept_:0.23004482809938068\n",
      "train_loss:0.266103767282921\n",
      "val_loss:0.48380410801271995\n",
      "iter:458\n",
      "coef_:[-0.36671712  3.30103766]\n",
      "intercept_:0.23005792142312073\n",
      "train_loss:0.26610328606276196\n",
      "val_loss:0.4838390556208399\n",
      "iter:459\n",
      "coef_:[-0.36689757  3.3013688 ]\n",
      "intercept_:0.23007090785938453\n",
      "train_loss:0.26610281360532995\n",
      "val_loss:0.483873694522361\n",
      "iter:460\n",
      "coef_:[-0.36707635  3.30169694]\n",
      "intercept_:0.23008378817817465\n",
      "train_loss:0.2661023497503946\n",
      "val_loss:0.483908027367482\n",
      "iter:461\n",
      "coef_:[-0.36725345  3.30202208]\n",
      "intercept_:0.23009656314581461\n",
      "train_loss:0.26610189434067383\n",
      "val_loss:0.4839420567851006\n",
      "iter:462\n",
      "coef_:[-0.36742891  3.30234427]\n",
      "intercept_:0.23010923352492846\n",
      "train_loss:0.2661014472217788\n",
      "val_loss:0.4839757853829564\n",
      "iter:463\n",
      "coef_:[-0.36760272  3.30266354]\n",
      "intercept_:0.23012180007442143\n",
      "train_loss:0.26610100824216026\n",
      "val_loss:0.4840092157477727\n",
      "iter:464\n",
      "coef_:[-0.36777492  3.30297989]\n",
      "intercept_:0.23013426354946168\n",
      "train_loss:0.2661005772530559\n",
      "val_loss:0.4840423504453984\n",
      "iter:465\n",
      "coef_:[-0.36794552  3.30329337]\n",
      "intercept_:0.23014662470146285\n",
      "train_loss:0.2661001541084387\n",
      "val_loss:0.48407519202094995\n",
      "iter:466\n",
      "coef_:[-0.36811453  3.30360401]\n",
      "intercept_:0.23015888427806777\n",
      "train_loss:0.26609973866496583\n",
      "val_loss:0.4841077429989521\n",
      "iter:467\n",
      "coef_:[-0.36828196  3.30391181]\n",
      "intercept_:0.23017104302313304\n",
      "train_loss:0.26609933078192943\n",
      "val_loss:0.4841400058834788\n",
      "iter:468\n",
      "coef_:[-0.36844784  3.30421682]\n",
      "intercept_:0.23018310167671446\n",
      "train_loss:0.26609893032120735\n",
      "val_loss:0.48417198315829313\n",
      "iter:469\n",
      "coef_:[-0.36861217  3.30451905]\n",
      "intercept_:0.2301950609750536\n",
      "train_loss:0.26609853714721543\n",
      "val_loss:0.48420367728698793\n",
      "iter:470\n",
      "coef_:[-0.36877498  3.30481854]\n",
      "intercept_:0.23020692165056492\n",
      "train_loss:0.2660981511268604\n",
      "val_loss:0.4842350907131246\n",
      "iter:471\n",
      "coef_:[-0.36893626  3.3051153 ]\n",
      "intercept_:0.23021868443182414\n",
      "train_loss:0.2660977721294934\n",
      "val_loss:0.48426622586037227\n",
      "iter:472\n",
      "coef_:[-0.36909605  3.30540937]\n",
      "intercept_:0.23023035004355713\n",
      "train_loss:0.26609740002686555\n",
      "val_loss:0.484297085132647\n",
      "iter:473\n",
      "coef_:[-0.36925436  3.30570076]\n",
      "intercept_:0.23024191920662984\n",
      "train_loss:0.2660970346930826\n",
      "val_loss:0.48432767091424866\n",
      "iter:474\n",
      "coef_:[-0.36941119  3.3059895 ]\n",
      "intercept_:0.23025339263803893\n",
      "train_loss:0.2660966760045618\n",
      "val_loss:0.48435798556999926\n",
      "iter:475\n",
      "coef_:[-0.36956657  3.30627562]\n",
      "intercept_:0.2302647710509033\n",
      "train_loss:0.2660963238399892\n",
      "val_loss:0.4843880314453803\n",
      "iter:476\n",
      "coef_:[-0.3697205   3.30655914]\n",
      "intercept_:0.23027605515445618\n",
      "train_loss:0.2660959780802773\n",
      "val_loss:0.4844178108666688\n",
      "iter:477\n",
      "coef_:[-0.369873    3.30684008]\n",
      "intercept_:0.23028724565403833\n",
      "train_loss:0.2660956386085241\n",
      "val_loss:0.4844473261410736\n",
      "iter:478\n",
      "coef_:[-0.37002408  3.30711846]\n",
      "intercept_:0.23029834325109158\n",
      "train_loss:0.26609530530997266\n",
      "val_loss:0.4844765795568709\n",
      "iter:479\n",
      "coef_:[-0.37017377  3.30739432]\n",
      "intercept_:0.23030934864315344\n",
      "train_loss:0.26609497807197147\n",
      "val_loss:0.48450557338353945\n",
      "iter:480\n",
      "coef_:[-0.37032206  3.30766766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept_:0.23032026252385215\n",
      "train_loss:0.26609465678393546\n",
      "val_loss:0.4845343098718952\n",
      "iter:481\n",
      "coef_:[-0.37046897  3.30793853]\n",
      "intercept_:0.23033108558290255\n",
      "train_loss:0.2660943413373075\n",
      "val_loss:0.4845627912542244\n",
      "iter:482\n",
      "coef_:[-0.37061453  3.30820693]\n",
      "intercept_:0.23034181850610266\n",
      "train_loss:0.2660940316255217\n",
      "val_loss:0.48459101974441793\n",
      "iter:483\n",
      "coef_:[-0.37075873  3.30847289]\n",
      "intercept_:0.2303524619753308\n",
      "train_loss:0.2660937275439657\n",
      "val_loss:0.48461899753810384\n",
      "iter:484\n",
      "coef_:[-0.37090159  3.30873643]\n",
      "intercept_:0.23036301666854342\n",
      "train_loss:0.26609342898994537\n",
      "val_loss:0.4846467268127795\n",
      "iter:485\n",
      "coef_:[-0.37104313  3.30899758]\n",
      "intercept_:0.23037348325977355\n",
      "train_loss:0.26609313586264877\n",
      "val_loss:0.48467420972794306\n",
      "iter:486\n",
      "coef_:[-0.37118336  3.30925636]\n",
      "intercept_:0.23038386241912986\n",
      "train_loss:0.2660928480631118\n",
      "val_loss:0.48470144842522545\n",
      "iter:487\n",
      "coef_:[-0.37132228  3.30951278]\n",
      "intercept_:0.23039415481279632\n",
      "train_loss:0.26609256549418386\n",
      "val_loss:0.4847284450285194\n",
      "iter:488\n",
      "coef_:[-0.37145992  3.30976687]\n",
      "intercept_:0.2304043611030324\n",
      "train_loss:0.26609228806049456\n",
      "val_loss:0.48475520164411123\n",
      "iter:489\n",
      "coef_:[-0.37159629  3.31001866]\n",
      "intercept_:0.23041448194817388\n",
      "train_loss:0.26609201566842056\n",
      "val_loss:0.48478172036080835\n",
      "iter:490\n",
      "coef_:[-0.37173139  3.31026815]\n",
      "intercept_:0.2304245180026342\n",
      "train_loss:0.26609174822605375\n",
      "val_loss:0.484808003250069\n",
      "iter:491\n",
      "coef_:[-0.37186523  3.31051538]\n",
      "intercept_:0.23043446991690633\n",
      "train_loss:0.2660914856431693\n",
      "val_loss:0.48483405236613036\n",
      "iter:492\n",
      "coef_:[-0.37199784  3.31076036]\n",
      "intercept_:0.23044433833756525\n",
      "train_loss:0.2660912278311947\n",
      "val_loss:0.48485986974613565\n",
      "iter:493\n",
      "coef_:[-0.37212922  3.31100311]\n",
      "intercept_:0.2304541239072708\n",
      "train_loss:0.2660909747031793\n",
      "val_loss:0.4848854574102608\n",
      "iter:494\n",
      "coef_:[-0.37225939  3.31124366]\n",
      "intercept_:0.23046382726477113\n",
      "train_loss:0.26609072617376456\n",
      "val_loss:0.48491081736184155\n",
      "iter:495\n",
      "coef_:[-0.37238834  3.31148203]\n",
      "intercept_:0.2304734490449066\n",
      "train_loss:0.26609048215915426\n",
      "val_loss:0.48493595158749797\n",
      "iter:496\n",
      "coef_:[-0.37251611  3.31171822]\n",
      "intercept_:0.2304829898786142\n",
      "train_loss:0.2660902425770863\n",
      "val_loss:0.4849608620572603\n",
      "iter:497\n",
      "coef_:[-0.37264269  3.31195227]\n",
      "intercept_:0.2304924503929324\n",
      "train_loss:0.26609000734680394\n",
      "val_loss:0.48498555072469285\n",
      "iter:498\n",
      "coef_:[-0.3727681  3.3121842]\n",
      "intercept_:0.23050183121100634\n",
      "train_loss:0.2660897763890282\n",
      "val_loss:0.48501001952701744\n",
      "iter:499\n",
      "coef_:[-0.37289235  3.31241402]\n",
      "intercept_:0.23051113295209372\n",
      "train_loss:0.26608954962593095\n",
      "val_loss:0.48503427038523694\n",
      "iter:500\n",
      "coef_:[-0.37301546  3.31264174]\n",
      "intercept_:0.23052035623157086\n",
      "train_loss:0.2660893269811077\n",
      "val_loss:0.48505830520425774\n",
      "iter:501\n",
      "coef_:[-0.37313742  3.3128674 ]\n",
      "intercept_:0.23052950166093925\n",
      "train_loss:0.26608910837955185\n",
      "val_loss:0.4850821258730106\n",
      "iter:502\n",
      "coef_:[-0.37325826  3.31309101]\n",
      "intercept_:0.23053856984783264\n",
      "train_loss:0.2660888937476285\n",
      "val_loss:0.48510573426457254\n",
      "iter:503\n",
      "coef_:[-0.37337797  3.31331259]\n",
      "intercept_:0.23054756139602428\n",
      "train_loss:0.26608868301304944\n",
      "val_loss:0.4851291322362875\n",
      "iter:504\n",
      "coef_:[-0.37349659  3.31353216]\n",
      "intercept_:0.23055647690543477\n",
      "train_loss:0.2660884761048486\n",
      "val_loss:0.485152321629885\n",
      "iter:505\n",
      "coef_:[-0.3736141   3.31374973]\n",
      "intercept_:0.23056531697214014\n",
      "train_loss:0.26608827295335735\n",
      "val_loss:0.4851753042716\n",
      "iter:506\n",
      "coef_:[-0.37373053  3.31396532]\n",
      "intercept_:0.23057408218838032\n",
      "train_loss:0.26608807349018054\n",
      "val_loss:0.48519808197229125\n",
      "iter:507\n",
      "coef_:[-0.37384588  3.31417895]\n",
      "intercept_:0.23058277314256803\n",
      "train_loss:0.2660878776481737\n",
      "val_loss:0.48522065652755947\n",
      "iter:508\n",
      "coef_:[-0.37396017  3.31439065]\n",
      "intercept_:0.23059139041929788\n",
      "train_loss:0.2660876853614194\n",
      "val_loss:0.4852430297178634\n",
      "iter:509\n",
      "coef_:[-0.37407341  3.31460042]\n",
      "intercept_:0.23059993459935593\n",
      "train_loss:0.2660874965652049\n",
      "val_loss:0.4852652033086372\n",
      "iter:510\n",
      "coef_:[-0.37418559  3.31480828]\n",
      "intercept_:0.2306084062597295\n",
      "train_loss:0.2660873111960002\n",
      "val_loss:0.48528717905040536\n",
      "iter:511\n",
      "coef_:[-0.37429674  3.31501426]\n",
      "intercept_:0.2306168059736173\n",
      "train_loss:0.26608712919143596\n",
      "val_loss:0.4853089586788991\n",
      "iter:512\n",
      "coef_:[-0.37440687  3.31521836]\n",
      "intercept_:0.23062513431043988\n",
      "train_loss:0.26608695049028236\n",
      "val_loss:0.4853305439151693\n",
      "iter:513\n",
      "coef_:[-0.37451598  3.31542061]\n",
      "intercept_:0.23063339183585038\n",
      "train_loss:0.2660867750324283\n",
      "val_loss:0.4853519364657011\n",
      "iter:514\n",
      "coef_:[-0.37462408  3.31562103]\n",
      "intercept_:0.23064157911174551\n",
      "train_loss:0.2660866027588604\n",
      "val_loss:0.4853731380225269\n",
      "iter:515\n",
      "coef_:[-0.37473118  3.31581962]\n",
      "intercept_:0.23064969669627694\n",
      "train_loss:0.26608643361164347\n",
      "val_loss:0.48539415026333865\n",
      "iter:516\n",
      "coef_:[-0.3748373   3.31601641]\n",
      "intercept_:0.23065774514386272\n",
      "train_loss:0.2660862675338999\n",
      "val_loss:0.4854149748515997\n",
      "iter:517\n",
      "coef_:[-0.37494244  3.31621141]\n",
      "intercept_:0.23066572500519927\n",
      "train_loss:0.26608610446979125\n",
      "val_loss:0.48543561343665514\n",
      "iter:518\n",
      "coef_:[-0.3750466   3.31640464]\n",
      "intercept_:0.23067363682727335\n",
      "train_loss:0.26608594436449806\n",
      "val_loss:0.48545606765384375\n",
      "iter:519\n",
      "coef_:[-0.37514981  3.31659612]\n",
      "intercept_:0.23068148115337436\n",
      "train_loss:0.26608578716420217\n",
      "val_loss:0.4854763391246059\n",
      "iter:520\n",
      "coef_:[-0.37525206  3.31678586]\n",
      "intercept_:0.230689258523107\n",
      "train_loss:0.2660856328160675\n",
      "val_loss:0.4854964294565937\n",
      "iter:521\n",
      "coef_:[-0.37535338  3.31697387]\n",
      "intercept_:0.230696969472404\n",
      "train_loss:0.2660854812682226\n",
      "val_loss:0.48551634024377904\n",
      "iter:522\n",
      "coef_:[-0.37545375  3.31716017]\n",
      "intercept_:0.2307046145335391\n",
      "train_loss:0.26608533246974275\n",
      "val_loss:0.4855360730665612\n",
      "iter:523\n",
      "coef_:[-0.37555321  3.31734479]\n",
      "intercept_:0.23071219423514028\n",
      "train_loss:0.2660851863706324\n",
      "val_loss:0.48555562949187403\n",
      "iter:524\n",
      "coef_:[-0.37565174  3.31752772]\n",
      "intercept_:0.23071970910220324\n",
      "train_loss:0.2660850429218085\n",
      "val_loss:0.48557501107329215\n",
      "iter:525\n",
      "coef_:[-0.37574937  3.317709  ]\n",
      "intercept_:0.23072715965610494\n",
      "train_loss:0.26608490207508356\n",
      "val_loss:0.48559421935113634\n",
      "iter:526\n",
      "coef_:[-0.3758461   3.31788863]\n",
      "intercept_:0.23073454641461746\n",
      "train_loss:0.266084763783149\n",
      "val_loss:0.48561325585257903\n",
      "iter:527\n",
      "coef_:[-0.37594194  3.31806662]\n",
      "intercept_:0.23074186989192205\n",
      "train_loss:0.2660846279995596\n",
      "val_loss:0.48563212209174833\n",
      "iter:528\n",
      "coef_:[-0.37603689  3.318243  ]\n",
      "intercept_:0.2307491305986232\n",
      "train_loss:0.2660844946787172\n",
      "val_loss:0.4856508195698318\n",
      "iter:529\n",
      "coef_:[-0.37613097  3.31841778]\n",
      "intercept_:0.23075632904176305\n",
      "train_loss:0.2660843637758551\n",
      "val_loss:0.4856693497751789\n",
      "iter:530\n",
      "coef_:[-0.37622419  3.31859097]\n",
      "intercept_:0.23076346572483591\n",
      "train_loss:0.2660842352470233\n",
      "val_loss:0.48568771418340406\n",
      "iter:531\n",
      "coef_:[-0.37631654  3.31876259]\n",
      "intercept_:0.23077054114780296\n",
      "train_loss:0.26608410904907326\n",
      "val_loss:0.485705914257488\n",
      "iter:532\n",
      "coef_:[-0.37640805  3.31893265]\n",
      "intercept_:0.23077755580710702\n",
      "train_loss:0.2660839851396431\n",
      "val_loss:0.4857239514478783\n",
      "iter:533\n",
      "coef_:[-0.37649871  3.31910117]\n",
      "intercept_:0.23078451019568758\n",
      "train_loss:0.26608386347714325\n",
      "val_loss:0.4857418271925901\n",
      "iter:534\n",
      "coef_:[-0.37658854  3.31926815]\n",
      "intercept_:0.23079140480299587\n",
      "train_loss:0.26608374402074264\n",
      "val_loss:0.48575954291730594\n",
      "iter:535\n",
      "coef_:[-0.37667755  3.31943362]\n",
      "intercept_:0.23079824011501016\n",
      "train_loss:0.26608362673035413\n",
      "val_loss:0.48577710003547403\n",
      "iter:536\n",
      "coef_:[-0.37676573  3.31959759]\n",
      "intercept_:0.2308050166142511\n",
      "train_loss:0.2660835115666217\n",
      "val_loss:0.48579449994840723\n",
      "iter:537\n",
      "coef_:[-0.3768531   3.31976007]\n",
      "intercept_:0.2308117347797973\n",
      "train_loss:0.26608339849090623\n",
      "val_loss:0.4858117440453802\n",
      "iter:538\n",
      "coef_:[-0.37693967  3.31992107]\n",
      "intercept_:0.23081839508730093\n",
      "train_loss:0.266083287465273\n",
      "val_loss:0.485828833703727\n",
      "iter:539\n",
      "coef_:[-0.37702544  3.32008061]\n",
      "intercept_:0.23082499800900338\n",
      "train_loss:0.26608317845247864\n",
      "val_loss:0.4858457702889364\n",
      "iter:540\n",
      "coef_:[-0.37711042  3.3202387 ]\n",
      "intercept_:0.23083154401375122\n",
      "train_loss:0.26608307141595827\n",
      "val_loss:0.4858625551547489\n",
      "iter:541\n",
      "coef_:[-0.37719463  3.32039535]\n",
      "intercept_:0.23083803356701216\n",
      "train_loss:0.2660829663198132\n",
      "val_loss:0.48587918964325055\n",
      "iter:542\n",
      "coef_:[-0.37727805  3.32055059]\n",
      "intercept_:0.23084446713089096\n",
      "train_loss:0.2660828631287989\n",
      "val_loss:0.4858956750849679\n",
      "iter:543\n",
      "coef_:[-0.37736072  3.32070441]\n",
      "intercept_:0.23085084516414578\n",
      "train_loss:0.2660827618083128\n",
      "val_loss:0.48591201279896185\n",
      "iter:544\n",
      "coef_:[-0.37744262  3.32085684]\n",
      "intercept_:0.23085716812220428\n",
      "train_loss:0.26608266232438277\n",
      "val_loss:0.48592820409292026\n",
      "iter:545\n",
      "coef_:[-0.37752376  3.32100788]\n",
      "intercept_:0.23086343645718\n",
      "train_loss:0.26608256464365515\n",
      "val_loss:0.4859442502632507\n",
      "iter:546\n",
      "coef_:[-0.37760417  3.32115755]\n",
      "intercept_:0.23086965061788872\n",
      "train_loss:0.26608246873338404\n",
      "val_loss:0.4859601525951724\n",
      "iter:547\n",
      "coef_:[-0.37768383  3.32130586]\n",
      "intercept_:0.23087581104986502\n",
      "train_loss:0.2660823745614198\n",
      "val_loss:0.4859759123628069\n",
      "iter:548\n",
      "coef_:[-0.37776276  3.32145283]\n",
      "intercept_:0.23088191819537873\n",
      "train_loss:0.2660822820961981\n",
      "val_loss:0.4859915308292689\n",
      "iter:549\n",
      "coef_:[-0.37784097  3.32159846]\n",
      "intercept_:0.23088797249345167\n",
      "train_loss:0.26608219130672944\n",
      "val_loss:0.4860070092467562\n",
      "iter:550\n",
      "coef_:[-0.37791846  3.32174277]\n",
      "intercept_:0.2308939743798742\n",
      "train_loss:0.26608210216258876\n",
      "val_loss:0.48602234885663903\n",
      "iter:551\n",
      "coef_:[-0.37799523  3.32188577]\n",
      "intercept_:0.230899924287222\n",
      "train_loss:0.26608201463390446\n",
      "val_loss:0.48603755088954786\n",
      "iter:552\n",
      "coef_:[-0.3780713   3.32202747]\n",
      "intercept_:0.230905822644873\n",
      "train_loss:0.266081928691349\n",
      "val_loss:0.48605261656546234\n",
      "iter:553\n",
      "coef_:[-0.37814668  3.32216788]\n",
      "intercept_:0.230911669879024\n",
      "train_loss:0.2660818443061287\n",
      "val_loss:0.4860675470937984\n",
      "iter:554\n",
      "coef_:[-0.37822136  3.32230702]\n",
      "intercept_:0.2309174664127078\n",
      "train_loss:0.266081761449974\n",
      "val_loss:0.48608234367349484\n",
      "iter:555\n",
      "coef_:[-0.37829535  3.3224449 ]\n",
      "intercept_:0.23092321266580995\n",
      "train_loss:0.26608168009512967\n",
      "val_loss:0.48609700749309925\n",
      "iter:556\n",
      "coef_:[-0.37836867  3.32258152]\n",
      "intercept_:0.23092890905508584\n",
      "train_loss:0.2660816002143458\n",
      "val_loss:0.48611153973085414\n",
      "iter:557\n",
      "coef_:[-0.37844131  3.32271691]\n",
      "intercept_:0.23093455599417767\n",
      "train_loss:0.26608152178086814\n",
      "val_loss:0.4861259415547811\n",
      "iter:558\n",
      "coef_:[-0.37851328  3.32285106]\n",
      "intercept_:0.23094015389363154\n",
      "train_loss:0.26608144476842954\n",
      "val_loss:0.48614021412276565\n",
      "iter:559\n",
      "coef_:[-0.3785846  3.322984 ]\n",
      "intercept_:0.2309457031609145\n",
      "train_loss:0.2660813691512405\n",
      "val_loss:0.4861543585826402\n",
      "iter:560\n",
      "coef_:[-0.37865526  3.32311573]\n",
      "intercept_:0.2309512042004317\n",
      "train_loss:0.2660812949039809\n",
      "val_loss:0.4861683760722682\n",
      "iter:561\n",
      "coef_:[-0.37872527  3.32324626]\n",
      "intercept_:0.2309566574135434\n",
      "train_loss:0.2660812220017913\n",
      "val_loss:0.4861822677196254\n",
      "iter:562\n",
      "coef_:[-0.37879464  3.32337561]\n",
      "intercept_:0.2309620631985823\n",
      "train_loss:0.2660811504202643\n",
      "val_loss:0.4861960346428823\n",
      "iter:563\n",
      "coef_:[-0.37886338  3.32350379]\n",
      "intercept_:0.2309674219508706\n",
      "train_loss:0.2660810801354367\n",
      "val_loss:0.4862096779504856\n",
      "iter:564\n",
      "coef_:[-0.37893148  3.3236308 ]\n",
      "intercept_:0.23097273406273722\n",
      "train_loss:0.2660810111237809\n",
      "val_loss:0.48622319874123804\n",
      "iter:565\n",
      "coef_:[-0.37899896  3.32375665]\n",
      "intercept_:0.23097799992353493\n",
      "train_loss:0.2660809433621974\n",
      "val_loss:0.4862365981043796\n",
      "iter:566\n",
      "coef_:[-0.37906582  3.32388137]\n",
      "intercept_:0.23098321991965767\n",
      "train_loss:0.2660808768280068\n",
      "val_loss:0.4862498771196658\n",
      "iter:567\n",
      "coef_:[-0.37913207  3.32400495]\n",
      "intercept_:0.2309883944345577\n",
      "train_loss:0.26608081149894197\n",
      "val_loss:0.48626303685744715\n",
      "iter:568\n",
      "coef_:[-0.3791977   3.32412742]\n",
      "intercept_:0.2309935238487628\n",
      "train_loss:0.26608074735314097\n",
      "val_loss:0.48627607837874764\n",
      "iter:569\n",
      "coef_:[-0.37926274  3.32424877]\n",
      "intercept_:0.23099860853989346\n",
      "train_loss:0.266080684369139\n",
      "val_loss:0.48628900273534215\n",
      "iter:570\n",
      "coef_:[-0.37932718  3.32436901]\n",
      "intercept_:0.2310036488826802\n",
      "train_loss:0.2660806225258618\n",
      "val_loss:0.48630181096983316\n",
      "iter:571\n",
      "coef_:[-0.37939103  3.32448817]\n",
      "intercept_:0.23100864524898074\n",
      "train_loss:0.2660805618026182\n",
      "val_loss:0.48631450411572835\n",
      "iter:572\n",
      "coef_:[-0.3794543   3.32460625]\n",
      "intercept_:0.2310135980077971\n",
      "train_loss:0.2660805021790931\n",
      "val_loss:0.48632708319751555\n",
      "iter:573\n",
      "coef_:[-0.37951698  3.32472325]\n",
      "intercept_:0.23101850752529293\n",
      "train_loss:0.26608044363534084\n",
      "val_loss:0.48633954923073874\n",
      "iter:574\n",
      "coef_:[-0.3795791   3.32483919]\n",
      "intercept_:0.2310233741648106\n",
      "train_loss:0.2660803861517781\n",
      "val_loss:0.4863519032220728\n",
      "iter:575\n",
      "coef_:[-0.37964064  3.32495408]\n",
      "intercept_:0.23102819828688848\n",
      "train_loss:0.26608032970917767\n",
      "val_loss:0.4863641461693973\n",
      "iter:576\n",
      "coef_:[-0.37970162  3.32506793]\n",
      "intercept_:0.2310329802492779\n",
      "train_loss:0.2660802742886618\n",
      "val_loss:0.4863762790618711\n",
      "iter:577\n",
      "coef_:[-0.37976204  3.32518074]\n",
      "intercept_:0.2310377204069605\n",
      "train_loss:0.2660802198716958\n",
      "val_loss:0.4863883028800046\n",
      "iter:578\n",
      "coef_:[-0.3798219   3.32529253]\n",
      "intercept_:0.2310424191121652\n",
      "train_loss:0.2660801664400817\n",
      "val_loss:0.48640021859573296\n",
      "iter:579\n",
      "coef_:[-0.37988122  3.3254033 ]\n",
      "intercept_:0.23104707671438535\n",
      "train_loss:0.2660801139759524\n",
      "val_loss:0.4864120271724879\n",
      "iter:580\n",
      "coef_:[-0.37993999  3.32551307]\n",
      "intercept_:0.23105169356039582\n",
      "train_loss:0.2660800624617653\n",
      "val_loss:0.48642372956526914\n",
      "iter:581\n",
      "coef_:[-0.37999823  3.32562184]\n",
      "intercept_:0.23105626999426998\n",
      "train_loss:0.26608001188029656\n",
      "val_loss:0.48643532672071543\n",
      "iter:582\n",
      "coef_:[-0.38005593  3.32572963]\n",
      "intercept_:0.23106080635739676\n",
      "train_loss:0.26607996221463537\n",
      "val_loss:0.48644681957717495\n",
      "iter:583\n",
      "coef_:[-0.38011311  3.32583643]\n",
      "intercept_:0.23106530298849765\n",
      "train_loss:0.26607991344817816\n",
      "val_loss:0.48645820906477544\n",
      "iter:584\n",
      "coef_:[-0.38016976  3.32594227]\n",
      "intercept_:0.23106976022364356\n",
      "train_loss:0.2660798655646228\n",
      "val_loss:0.48646949610549284\n",
      "iter:585\n",
      "coef_:[-0.38022589  3.32604715]\n",
      "intercept_:0.2310741783962719\n",
      "train_loss:0.26607981854796353\n",
      "val_loss:0.4864806816132207\n",
      "iter:586\n",
      "coef_:[-0.38028151  3.32615107]\n",
      "intercept_:0.2310785578372033\n",
      "train_loss:0.2660797723824852\n",
      "val_loss:0.48649176649383846\n",
      "iter:587\n",
      "coef_:[-0.38033662  3.32625405]\n",
      "intercept_:0.23108289887465855\n",
      "train_loss:0.266079727052758\n",
      "val_loss:0.48650275164527856\n",
      "iter:588\n",
      "coef_:[-0.38039122  3.3263561 ]\n",
      "intercept_:0.23108720183427545\n",
      "train_loss:0.26607968254363273\n",
      "val_loss:0.4865136379575945\n",
      "iter:589\n",
      "coef_:[-0.38044533  3.32645722]\n",
      "intercept_:0.23109146703912542\n",
      "train_loss:0.2660796388402349\n",
      "val_loss:0.4865244263130268\n",
      "iter:590\n",
      "coef_:[-0.38049894  3.32655742]\n",
      "intercept_:0.23109569480973044\n",
      "train_loss:0.2660795959279604\n",
      "val_loss:0.48653511758606965\n",
      "iter:591\n",
      "coef_:[-0.38055205  3.32665672]\n",
      "intercept_:0.23109988546407959\n",
      "train_loss:0.2660795537924702\n",
      "val_loss:0.48654571264353613\n",
      "iter:592\n",
      "coef_:[-0.38060469  3.32675511]\n",
      "intercept_:0.23110403931764573\n",
      "train_loss:0.2660795124196858\n",
      "val_loss:0.4865562123446241\n",
      "iter:593\n",
      "coef_:[-0.38065684  3.32685261]\n",
      "intercept_:0.23110815668340212\n",
      "train_loss:0.2660794717957842\n",
      "val_loss:0.4865666175409798\n",
      "iter:594\n",
      "coef_:[-0.38070851  3.32694922]\n",
      "intercept_:0.231112237871839\n",
      "train_loss:0.26607943190719335\n",
      "val_loss:0.486576929076763\n",
      "iter:595\n",
      "coef_:[-0.38075972  3.32704496]\n",
      "intercept_:0.23111628319098\n",
      "train_loss:0.26607939274058784\n",
      "val_loss:0.4865871477887099\n",
      "iter:596\n",
      "coef_:[-0.38081045  3.32713983]\n",
      "intercept_:0.23112029294639866\n",
      "train_loss:0.26607935428288393\n",
      "val_loss:0.4865972745061964\n",
      "iter:597\n",
      "coef_:[-0.38086072  3.32723383]\n",
      "intercept_:0.23112426744123485\n",
      "train_loss:0.26607931652123556\n",
      "val_loss:0.48660731005130126\n",
      "iter:598\n",
      "coef_:[-0.38091053  3.32732699]\n",
      "intercept_:0.23112820697621106\n",
      "train_loss:0.26607927944302984\n",
      "val_loss:0.48661725523886745\n",
      "iter:599\n",
      "coef_:[-0.38095988  3.3274193 ]\n",
      "intercept_:0.2311321118496488\n",
      "train_loss:0.26607924303588265\n",
      "val_loss:0.48662711087656446\n",
      "iter:600\n",
      "coef_:[-0.38100879  3.32751077]\n",
      "intercept_:0.23113598235748473\n",
      "train_loss:0.266079207287635\n",
      "val_loss:0.4866368777649488\n",
      "iter:601\n",
      "coef_:[-0.38105724  3.32760141]\n",
      "intercept_:0.23113981879328693\n",
      "train_loss:0.26607917218634836\n",
      "val_loss:0.4866465566975256\n",
      "iter:602\n",
      "coef_:[-0.38110525  3.32769123]\n",
      "intercept_:0.23114362144827102\n",
      "train_loss:0.2660791377203009\n",
      "val_loss:0.48665614846080774\n",
      "iter:603\n",
      "coef_:[-0.38115283  3.32778023]\n",
      "intercept_:0.23114739061131626\n",
      "train_loss:0.26607910387798356\n",
      "val_loss:0.48666565383437627\n",
      "iter:604\n",
      "coef_:[-0.38119997  3.32786843]\n",
      "intercept_:0.23115112656898146\n",
      "train_loss:0.2660790706480962\n",
      "val_loss:0.4866750735909395\n",
      "iter:605\n",
      "coef_:[-0.38124667  3.32795582]\n",
      "intercept_:0.23115482960552108\n",
      "train_loss:0.2660790380195438\n",
      "val_loss:0.48668440849639183\n",
      "iter:606\n",
      "coef_:[-0.38129295  3.32804243]\n",
      "intercept_:0.2311585000029011\n",
      "train_loss:0.2660790059814323\n",
      "val_loss:0.48669365930987185\n",
      "iter:607\n",
      "coef_:[-0.38133881  3.32812824]\n",
      "intercept_:0.23116213804081487\n",
      "train_loss:0.26607897452306584\n",
      "val_loss:0.4867028267838201\n",
      "iter:608\n",
      "coef_:[-0.38138425  3.32821328]\n",
      "intercept_:0.23116574399669876\n",
      "train_loss:0.26607894363394224\n",
      "val_loss:0.48671191166403704\n",
      "iter:609\n",
      "coef_:[-0.38142927  3.32829754]\n",
      "intercept_:0.23116931814574807\n",
      "train_loss:0.2660789133037501\n",
      "val_loss:0.4867209146897393\n",
      "iter:610\n",
      "coef_:[-0.38147389  3.32838105]\n",
      "intercept_:0.23117286076093257\n",
      "train_loss:0.266078883522365\n",
      "val_loss:0.48672983659361646\n",
      "iter:611\n",
      "coef_:[-0.38151809  3.32846379]\n",
      "intercept_:0.23117637211301215\n",
      "train_loss:0.2660788542798462\n",
      "val_loss:0.48673867810188687\n",
      "iter:612\n",
      "coef_:[-0.38156189  3.32854578]\n",
      "intercept_:0.23117985247055228\n",
      "train_loss:0.26607882556643325\n",
      "val_loss:0.4867474399343537\n",
      "iter:613\n",
      "coef_:[-0.38160529  3.32862703]\n",
      "intercept_:0.23118330209993954\n",
      "train_loss:0.266078797372543\n",
      "val_loss:0.48675612280445935\n",
      "iter:614\n",
      "coef_:[-0.38164829  3.32870754]\n",
      "intercept_:0.231186721265397\n",
      "train_loss:0.26607876968876576\n",
      "val_loss:0.48676472741934035\n",
      "iter:615\n",
      "coef_:[-0.3816909   3.32878732]\n",
      "intercept_:0.23119011022899952\n",
      "train_loss:0.26607874250586294\n",
      "val_loss:0.486773254479882\n",
      "iter:616\n",
      "coef_:[-0.38173313  3.32886638]\n",
      "intercept_:0.23119346925068907\n",
      "train_loss:0.2660787158147632\n",
      "val_loss:0.48678170468077164\n",
      "iter:617\n",
      "coef_:[-0.38177496  3.32894472]\n",
      "intercept_:0.23119679858828984\n",
      "train_loss:0.26607868960655995\n",
      "val_loss:0.48679007871055235\n",
      "iter:618\n",
      "coef_:[-0.38181642  3.32902235]\n",
      "intercept_:0.23120009849752352\n",
      "train_loss:0.26607866387250795\n",
      "val_loss:0.48679837725167546\n",
      "iter:619\n",
      "coef_:[-0.38185749  3.32909927]\n",
      "intercept_:0.23120336923202417\n",
      "train_loss:0.26607863860402065\n",
      "val_loss:0.4868066009805534\n",
      "iter:620\n",
      "coef_:[-0.38189819  3.3291755 ]\n",
      "intercept_:0.23120661104335333\n",
      "train_loss:0.2660786137926673\n",
      "val_loss:0.48681475056761186\n",
      "iter:621\n",
      "coef_:[-0.38193852  3.32925103]\n",
      "intercept_:0.23120982418101493\n",
      "train_loss:0.2660785894301699\n",
      "val_loss:0.486822826677341\n",
      "iter:622\n",
      "coef_:[-0.38197848  3.32932588]\n",
      "intercept_:0.23121300889247015\n",
      "train_loss:0.26607856550840064\n",
      "val_loss:0.4868308299683467\n",
      "iter:623\n",
      "coef_:[-0.38201807  3.32940005]\n",
      "intercept_:0.2312161654231521\n",
      "train_loss:0.26607854201937914\n",
      "val_loss:0.486838761093402\n",
      "iter:624\n",
      "coef_:[-0.38205731  3.32947354]\n",
      "intercept_:0.23121929401648067\n",
      "train_loss:0.2660785189552697\n",
      "val_loss:0.4868466206994968\n",
      "iter:625\n",
      "coef_:[-0.38209618  3.32954637]\n",
      "intercept_:0.23122239491387708\n",
      "train_loss:0.2660784963083786\n",
      "val_loss:0.48685440942788816\n",
      "iter:626\n",
      "coef_:[-0.3821347   3.32961854]\n",
      "intercept_:0.2312254683547785\n",
      "train_loss:0.26607847407115165\n",
      "val_loss:0.48686212791414946\n",
      "iter:627\n",
      "coef_:[-0.38217287  3.32969006]\n",
      "intercept_:0.23122851457665247\n",
      "train_loss:0.26607845223617177\n",
      "val_loss:0.4868697767882203\n",
      "iter:628\n",
      "coef_:[-0.38221069  3.32976092]\n",
      "intercept_:0.23123153381501138\n",
      "train_loss:0.2660784307961562\n",
      "val_loss:0.4868773566744546\n",
      "iter:629\n",
      "coef_:[-0.38224817  3.32983114]\n",
      "intercept_:0.23123452630342684\n",
      "train_loss:0.2660784097439542\n",
      "val_loss:0.48688486819166926\n",
      "iter:630\n",
      "coef_:[-0.3822853   3.32990073]\n",
      "intercept_:0.23123749227354384\n",
      "train_loss:0.2660783890725448\n",
      "val_loss:0.48689231195319216\n",
      "iter:631\n",
      "coef_:[-0.3823221   3.32996968]\n",
      "intercept_:0.2312404319550951\n",
      "train_loss:0.26607836877503416\n",
      "val_loss:0.4868996885669095\n",
      "iter:632\n",
      "coef_:[-0.38235856  3.33003801]\n",
      "intercept_:0.23124334557591514\n",
      "train_loss:0.26607834884465353\n",
      "val_loss:0.48690699863531306\n",
      "iter:633\n",
      "coef_:[-0.38239468  3.33010572]\n",
      "intercept_:0.2312462333619543\n",
      "train_loss:0.2660783292747567\n",
      "val_loss:0.4869142427555473\n",
      "iter:634\n",
      "coef_:[-0.38243048  3.33017281]\n",
      "intercept_:0.2312490955372927\n",
      "train_loss:0.2660783100588181\n",
      "val_loss:0.48692142151945506\n",
      "iter:635\n",
      "coef_:[-0.38246595  3.33023929]\n",
      "intercept_:0.23125193232415428\n",
      "train_loss:0.26607829119043025\n",
      "val_loss:0.4869285355136242\n",
      "iter:636\n",
      "coef_:[-0.38250109  3.33030518]\n",
      "intercept_:0.2312547439429205\n",
      "train_loss:0.2660782726633021\n",
      "val_loss:0.4869355853194334\n",
      "iter:637\n",
      "coef_:[-0.38253592  3.33037046]\n",
      "intercept_:0.23125753061214416\n",
      "train_loss:0.26607825447125627\n",
      "val_loss:0.48694257151309656\n",
      "iter:638\n",
      "coef_:[-0.38257043  3.33043515]\n",
      "intercept_:0.23126029254856303\n",
      "train_loss:0.26607823660822766\n",
      "val_loss:0.4869494946657083\n",
      "iter:639\n",
      "coef_:[-0.38260462  3.33049926]\n",
      "intercept_:0.23126302996711354\n",
      "train_loss:0.26607821906826074\n",
      "val_loss:0.4869563553432888\n",
      "iter:640\n",
      "coef_:[-0.3826385   3.33056278]\n",
      "intercept_:0.2312657430809442\n",
      "train_loss:0.2660782018455081\n",
      "val_loss:0.4869631541068267\n",
      "iter:641\n",
      "coef_:[-0.38267207  3.33062572]\n",
      "intercept_:0.23126843210142917\n",
      "train_loss:0.2660781849342285\n",
      "val_loss:0.4869698915123244\n",
      "iter:642\n",
      "coef_:[-0.38270534  3.3306881 ]\n",
      "intercept_:0.23127109723818157\n",
      "train_loss:0.2660781683287844\n",
      "val_loss:0.4869765681108406\n",
      "iter:643\n",
      "coef_:[-0.3827383   3.33074991]\n",
      "intercept_:0.2312737386990668\n",
      "train_loss:0.26607815202364044\n",
      "val_loss:0.4869831844485333\n",
      "iter:644\n",
      "coef_:[-0.38277096  3.33081116]\n",
      "intercept_:0.23127635669021582\n",
      "train_loss:0.2660781360133618\n",
      "val_loss:0.4869897410667029\n",
      "iter:645\n",
      "coef_:[-0.38280332  3.33087185]\n",
      "intercept_:0.2312789514160382\n",
      "train_loss:0.2660781202926119\n",
      "val_loss:0.4869962385018345\n",
      "iter:646\n",
      "coef_:[-0.38283539  3.33093199]\n",
      "intercept_:0.23128152307923527\n",
      "train_loss:0.266078104856151\n",
      "val_loss:0.4870026772856393\n",
      "iter:647\n",
      "coef_:[-0.38286717  3.33099159]\n",
      "intercept_:0.23128407188081318\n",
      "train_loss:0.26607808969883434\n",
      "val_loss:0.48700905794509697\n",
      "iter:648\n",
      "coef_:[-0.38289866  3.33105064]\n",
      "intercept_:0.23128659802009569\n",
      "train_loss:0.2660780748156101\n",
      "val_loss:0.4870153810024967\n",
      "iter:649\n",
      "coef_:[-0.38292985  3.33110916]\n",
      "intercept_:0.23128910169473707\n",
      "train_loss:0.2660780602015182\n",
      "val_loss:0.487021646975478\n",
      "iter:650\n",
      "coef_:[-0.38296077  3.33116715]\n",
      "intercept_:0.23129158310073497\n",
      "train_loss:0.2660780458516884\n",
      "val_loss:0.48702785637707136\n",
      "iter:651\n",
      "coef_:[-0.3829914   3.33122462]\n",
      "intercept_:0.23129404243244298\n",
      "train_loss:0.26607803176133876\n",
      "val_loss:0.48703400971573857\n",
      "iter:652\n",
      "coef_:[-0.38302176  3.33128156]\n",
      "intercept_:0.23129647988258337\n",
      "train_loss:0.26607801792577374\n",
      "val_loss:0.48704010749541254\n",
      "iter:653\n",
      "coef_:[-0.38305183  3.33133798]\n",
      "intercept_:0.23129889564225956\n",
      "train_loss:0.26607800434038303\n",
      "val_loss:0.4870461502155372\n",
      "iter:654\n",
      "coef_:[-0.38308164  3.33139389]\n",
      "intercept_:0.2313012899009686\n",
      "train_loss:0.2660779910006398\n",
      "val_loss:0.48705213837110634\n",
      "iter:655\n",
      "coef_:[-0.38311117  3.3314493 ]\n",
      "intercept_:0.23130366284661363\n",
      "train_loss:0.26607797790209914\n",
      "val_loss:0.4870580724527025\n",
      "iter:656\n",
      "coef_:[-0.38314043  3.3315042 ]\n",
      "intercept_:0.23130601466551615\n",
      "train_loss:0.2660779650403968\n",
      "val_loss:0.4870639529465363\n",
      "iter:657\n",
      "coef_:[-0.38316942  3.33155861]\n",
      "intercept_:0.23130834554242827\n",
      "train_loss:0.2660779524112476\n",
      "val_loss:0.48706978033448395\n",
      "iter:658\n",
      "coef_:[-0.38319815  3.33161252]\n",
      "intercept_:0.23131065566054484\n",
      "train_loss:0.26607794001044366\n",
      "val_loss:0.48707555509412503\n",
      "iter:659\n",
      "coef_:[-0.38322662  3.33166594]\n",
      "intercept_:0.23131294520151566\n",
      "train_loss:0.2660779278338537\n",
      "val_loss:0.4870812776987814\n",
      "iter:660\n",
      "coef_:[-0.38325483  3.33171888]\n",
      "intercept_:0.23131521434545738\n",
      "train_loss:0.26607791587742097\n",
      "val_loss:0.4870869486175529\n",
      "iter:661\n",
      "coef_:[-0.38328279  3.33177134]\n",
      "intercept_:0.23131746327096547\n",
      "train_loss:0.2660779041371624\n",
      "val_loss:0.4870925683153555\n",
      "iter:662\n",
      "coef_:[-0.38331048  3.33182332]\n",
      "intercept_:0.23131969215512618\n",
      "train_loss:0.266077892609167\n",
      "val_loss:0.4870981372529575\n",
      "iter:663\n",
      "coef_:[-0.38333793  3.33187483]\n",
      "intercept_:0.23132190117352824\n",
      "train_loss:0.2660778812895944\n",
      "val_loss:0.48710365588701604\n",
      "iter:664\n",
      "coef_:[-0.38336512  3.33192587]\n",
      "intercept_:0.23132409050027458\n",
      "train_loss:0.26607787017467405\n",
      "val_loss:0.487109124670113\n",
      "iter:665\n",
      "coef_:[-0.38339207  3.33197645]\n",
      "intercept_:0.23132626030799402\n",
      "train_loss:0.26607785926070365\n",
      "val_loss:0.48711454405079124\n",
      "iter:666\n",
      "coef_:[-0.38341877  3.33202657]\n",
      "intercept_:0.23132841076785282\n",
      "train_loss:0.2660778485440478\n",
      "val_loss:0.4871199144735893\n",
      "iter:667\n",
      "coef_:[-0.38344523  3.33207623]\n",
      "intercept_:0.23133054204956616\n",
      "train_loss:0.2660778380211369\n",
      "val_loss:0.48712523637907723\n",
      "iter:668\n",
      "coef_:[-0.38347145  3.33212545]\n",
      "intercept_:0.2313326543214096\n",
      "train_loss:0.2660778276884663\n",
      "val_loss:0.48713051020389114\n",
      "iter:669\n",
      "coef_:[-0.38349743  3.33217422]\n",
      "intercept_:0.2313347477502304\n",
      "train_loss:0.26607781754259446\n",
      "val_loss:0.48713573638076774\n",
      "iter:670\n",
      "coef_:[-0.38352317  3.33222254]\n",
      "intercept_:0.23133682250145882\n",
      "train_loss:0.2660778075801422\n",
      "val_loss:0.48714091533857906\n",
      "iter:671\n",
      "coef_:[-0.38354868  3.33227043]\n",
      "intercept_:0.23133887873911924\n",
      "train_loss:0.2660777977977918\n",
      "val_loss:0.4871460475023656\n",
      "iter:672\n",
      "coef_:[-0.38357395  3.33231788]\n",
      "intercept_:0.23134091662584141\n",
      "train_loss:0.26607778819228534\n",
      "val_loss:0.48715113329337095\n",
      "iter:673\n",
      "coef_:[-0.383599    3.33236491]\n",
      "intercept_:0.23134293632287142\n",
      "train_loss:0.26607777876042393\n",
      "val_loss:0.4871561731290744\n",
      "iter:674\n",
      "coef_:[-0.38362381  3.3324115 ]\n",
      "intercept_:0.23134493799008268\n",
      "train_loss:0.2660777694990667\n",
      "val_loss:0.48716116742322463\n",
      "iter:675\n",
      "coef_:[-0.3836484   3.33245768]\n",
      "intercept_:0.23134692178598687\n",
      "train_loss:0.2660777604051295\n",
      "val_loss:0.4871661165858723\n",
      "iter:676\n",
      "coef_:[-0.38367277  3.33250343]\n",
      "intercept_:0.23134888786774474\n",
      "train_loss:0.2660777514755841\n",
      "val_loss:0.4871710210234025\n",
      "iter:677\n",
      "coef_:[-0.38369692  3.33254877]\n",
      "intercept_:0.2313508363911769\n",
      "train_loss:0.2660777427074571\n",
      "val_loss:0.4871758811385674\n",
      "iter:678\n",
      "coef_:[-0.38372084  3.3325937 ]\n",
      "intercept_:0.23135276751077447\n",
      "train_loss:0.266077734097829\n",
      "val_loss:0.48718069733051794\n",
      "iter:679\n",
      "coef_:[-0.38374455  3.33263822]\n",
      "intercept_:0.23135468137970977\n",
      "train_loss:0.2660777256438328\n",
      "val_loss:0.48718546999483564\n",
      "iter:680\n",
      "coef_:[-0.38376804  3.33268234]\n",
      "intercept_:0.23135657814984678\n",
      "train_loss:0.26607771734265395\n",
      "val_loss:0.4871901995235639\n",
      "iter:681\n",
      "coef_:[-0.38379132  3.33272605]\n",
      "intercept_:0.23135845797175167\n",
      "train_loss:0.26607770919152823\n",
      "val_loss:0.48719488630524\n",
      "iter:682\n",
      "coef_:[-0.38381438  3.33276937]\n",
      "intercept_:0.23136032099470316\n",
      "train_loss:0.2660777011877421\n",
      "val_loss:0.48719953072492467\n",
      "iter:683\n",
      "coef_:[-0.38383724  3.3328123 ]\n",
      "intercept_:0.23136216736670293\n",
      "train_loss:0.2660776933286306\n",
      "val_loss:0.4872041331642338\n",
      "iter:684\n",
      "coef_:[-0.38385988  3.33285484]\n",
      "intercept_:0.23136399723448578\n",
      "train_loss:0.2660776856115773\n",
      "val_loss:0.4872086940013686\n",
      "iter:685\n",
      "coef_:[-0.38388233  3.33289699]\n",
      "intercept_:0.23136581074352994\n",
      "train_loss:0.2660776780340131\n",
      "val_loss:0.4872132136111458\n",
      "iter:686\n",
      "coef_:[-0.38390456  3.33293876]\n",
      "intercept_:0.23136760803806705\n",
      "train_loss:0.2660776705934152\n",
      "val_loss:0.4872176923650268\n",
      "iter:687\n",
      "coef_:[-0.3839266   3.33298015]\n",
      "intercept_:0.23136938926109232\n",
      "train_loss:0.2660776632873067\n",
      "val_loss:0.48722213063114855\n",
      "iter:688\n",
      "coef_:[-0.38394843  3.33302116]\n",
      "intercept_:0.2313711545543745\n",
      "train_loss:0.2660776561132554\n",
      "val_loss:0.48722652877435196\n",
      "iter:689\n",
      "coef_:[-0.38397007  3.3330618 ]\n",
      "intercept_:0.2313729040584657\n",
      "train_loss:0.26607764906887327\n",
      "val_loss:0.4872308871562113\n",
      "iter:690\n",
      "coef_:[-0.3839915   3.33310208]\n",
      "intercept_:0.23137463791271135\n",
      "train_loss:0.26607764215181523\n",
      "val_loss:0.4872352061350632\n",
      "iter:691\n",
      "coef_:[-0.38401275  3.33314199]\n",
      "intercept_:0.2313763562552599\n",
      "train_loss:0.2660776353597788\n",
      "val_loss:0.4872394860660351\n",
      "iter:692\n",
      "coef_:[-0.3840338   3.33318153]\n",
      "intercept_:0.23137805922307259\n",
      "train_loss:0.26607762869050333\n",
      "val_loss:0.4872437273010736\n",
      "iter:693\n",
      "coef_:[-0.38405465  3.33322072]\n",
      "intercept_:0.231379746951933\n",
      "train_loss:0.2660776221417688\n",
      "val_loss:0.4872479301889729\n",
      "iter:694\n",
      "coef_:[-0.38407532  3.33325955]\n",
      "intercept_:0.23138141957645667\n",
      "train_loss:0.2660776157113954\n",
      "val_loss:0.4872520950754023\n",
      "iter:695\n",
      "coef_:[-0.3840958   3.33329803]\n",
      "intercept_:0.2313830772301006\n",
      "train_loss:0.2660776093972429\n",
      "val_loss:0.4872562223029339\n",
      "iter:696\n",
      "coef_:[-0.38411609  3.33333616]\n",
      "intercept_:0.23138472004517266\n",
      "train_loss:0.26607760319720986\n",
      "val_loss:0.4872603122110702\n",
      "iter:697\n",
      "coef_:[-0.3841362   3.33337395]\n",
      "intercept_:0.23138634815284098\n",
      "train_loss:0.2660775971092327\n",
      "val_loss:0.48726436513627147\n",
      "iter:698\n",
      "coef_:[-0.38415613  3.33341139]\n",
      "intercept_:0.23138796168314327\n",
      "train_loss:0.2660775911312851\n",
      "val_loss:0.48726838141198187\n",
      "iter:699\n",
      "coef_:[-0.38417587  3.33344849]\n",
      "intercept_:0.231389560764996\n",
      "train_loss:0.2660775852613776\n",
      "val_loss:0.4872723613686572\n",
      "iter:700\n",
      "coef_:[-0.38419544  3.33348526]\n",
      "intercept_:0.23139114552620357\n",
      "train_loss:0.2660775794975567\n",
      "val_loss:0.4872763053337904\n",
      "iter:701\n",
      "coef_:[-0.38421482  3.33352169]\n",
      "intercept_:0.23139271609346745\n",
      "train_loss:0.2660775738379043\n",
      "val_loss:0.48728021363193863\n",
      "iter:702\n",
      "coef_:[-0.38423403  3.33355779]\n",
      "intercept_:0.2313942725923952\n",
      "train_loss:0.2660775682805368\n",
      "val_loss:0.48728408658474875\n",
      "iter:703\n",
      "coef_:[-0.38425307  3.33359357]\n",
      "intercept_:0.2313958151475094\n",
      "train_loss:0.26607756282360495\n",
      "val_loss:0.4872879245109833\n",
      "iter:704\n",
      "coef_:[-0.38427193  3.33362902]\n",
      "intercept_:0.23139734388225663\n",
      "train_loss:0.2660775574652926\n",
      "val_loss:0.48729172772654605\n",
      "iter:705\n",
      "coef_:[-0.38429062  3.33366415]\n",
      "intercept_:0.23139885891901624\n",
      "train_loss:0.2660775522038168\n",
      "val_loss:0.4872954965445075\n",
      "iter:706\n",
      "coef_:[-0.38430914  3.33369896]\n",
      "intercept_:0.23140036037910916\n",
      "train_loss:0.26607754703742675\n",
      "val_loss:0.48729923127512953\n",
      "iter:707\n",
      "coef_:[-0.3843275   3.33373345]\n",
      "intercept_:0.23140184838280659\n",
      "train_loss:0.2660775419644033\n",
      "val_loss:0.4873029322258909\n",
      "iter:708\n",
      "coef_:[-0.38434568  3.33376763]\n",
      "intercept_:0.23140332304933867\n",
      "train_loss:0.2660775369830583\n",
      "val_loss:0.4873065997015116\n",
      "iter:709\n",
      "coef_:[-0.3843637  3.3338015]\n",
      "intercept_:0.23140478449690305\n",
      "train_loss:0.26607753209173446\n",
      "val_loss:0.48731023400397744\n",
      "iter:710\n",
      "coef_:[-0.38438156  3.33383507]\n",
      "intercept_:0.2314062328426734\n",
      "train_loss:0.26607752728880407\n",
      "val_loss:0.4873138354325644\n",
      "iter:711\n",
      "coef_:[-0.38439925  3.33386833]\n",
      "intercept_:0.2314076682028079\n",
      "train_loss:0.2660775225726692\n",
      "val_loss:0.48731740428386244\n",
      "iter:712\n",
      "coef_:[-0.38441678  3.33390128]\n",
      "intercept_:0.23140909069245758\n",
      "train_loss:0.2660775179417606\n",
      "val_loss:0.4873209408517995\n",
      "iter:713\n",
      "coef_:[-0.38443415  3.33393394]\n",
      "intercept_:0.23141050042577474\n",
      "train_loss:0.26607751339453745\n",
      "val_loss:0.48732444542766545\n",
      "iter:714\n",
      "coef_:[-0.38445137  3.3339663 ]\n",
      "intercept_:0.23141189751592114\n",
      "train_loss:0.2660775089294869\n",
      "val_loss:0.48732791830013467\n",
      "iter:715\n",
      "coef_:[-0.38446843  3.33399837]\n",
      "intercept_:0.23141328207507622\n",
      "train_loss:0.26607750454512347\n",
      "val_loss:0.4873313597552903\n",
      "iter:716\n",
      "coef_:[-0.38448533  3.33403015]\n",
      "intercept_:0.2314146542144453\n",
      "train_loss:0.26607750023998844\n",
      "val_loss:0.48733477007664655\n",
      "iter:717\n",
      "coef_:[-0.38450208  3.33406164]\n",
      "intercept_:0.23141601404426762\n",
      "train_loss:0.2660774960126496\n",
      "val_loss:0.48733814954517213\n",
      "iter:718\n",
      "coef_:[-0.38451868  3.33409285]\n",
      "intercept_:0.2314173616738244\n",
      "train_loss:0.2660774918617004\n",
      "val_loss:0.4873414984393119\n",
      "iter:719\n",
      "coef_:[-0.38453512  3.33412377]\n",
      "intercept_:0.23141869721144673\n",
      "train_loss:0.2660774877857601\n",
      "val_loss:0.48734481703501087\n",
      "iter:720\n",
      "coef_:[-0.38455142  3.33415441]\n",
      "intercept_:0.2314200207645236\n",
      "train_loss:0.2660774837834726\n",
      "val_loss:0.4873481056057345\n",
      "iter:721\n",
      "coef_:[-0.38456757  3.33418477]\n",
      "intercept_:0.23142133243950963\n",
      "train_loss:0.2660774798535066\n",
      "val_loss:0.4873513644224925\n",
      "iter:722\n",
      "coef_:[-0.38458357  3.33421486]\n",
      "intercept_:0.23142263234193297\n",
      "train_loss:0.26607747599455467\n",
      "val_loss:0.4873545937538597\n",
      "iter:723\n",
      "coef_:[-0.38459942  3.33424467]\n",
      "intercept_:0.23142392057640293\n",
      "train_loss:0.2660774722053332\n",
      "val_loss:0.48735779386599787\n",
      "iter:724\n",
      "coef_:[-0.38461513  3.33427421]\n",
      "intercept_:0.23142519724661767\n",
      "train_loss:0.2660774684845815\n",
      "val_loss:0.48736096502267756\n",
      "iter:725\n",
      "coef_:[-0.3846307   3.33430349]\n",
      "intercept_:0.23142646245537188\n",
      "train_loss:0.26607746483106215\n",
      "val_loss:0.4873641074852988\n",
      "iter:726\n",
      "coef_:[-0.38464613  3.3343325 ]\n",
      "intercept_:0.23142771630456427\n",
      "train_loss:0.2660774612435597\n",
      "val_loss:0.4873672215129126\n",
      "iter:727\n",
      "coef_:[-0.38466142  3.33436125]\n",
      "intercept_:0.23142895889520507\n",
      "train_loss:0.26607745772088087\n",
      "val_loss:0.48737030736224196\n",
      "iter:728\n",
      "coef_:[-0.38467656  3.33438973]\n",
      "intercept_:0.23143019032742349\n",
      "train_loss:0.2660774542618539\n",
      "val_loss:0.48737336528770214\n",
      "iter:729\n",
      "coef_:[-0.38469157  3.33441796]\n",
      "intercept_:0.2314314107004751\n",
      "train_loss:0.2660774508653283\n",
      "val_loss:0.4873763955414219\n",
      "iter:730\n",
      "coef_:[-0.38470645  3.33444593]\n",
      "intercept_:0.23143262011274915\n",
      "train_loss:0.26607744753017426\n",
      "val_loss:0.4873793983732631\n",
      "iter:731\n",
      "coef_:[-0.38472119  3.33447365]\n",
      "intercept_:0.23143381866177587\n",
      "train_loss:0.26607744425528246\n",
      "val_loss:0.4873823740308413\n",
      "iter:732\n",
      "coef_:[-0.38473579  3.33450112]\n",
      "intercept_:0.23143500644423362\n",
      "train_loss:0.26607744103956377\n",
      "val_loss:0.4873853227595459\n",
      "iter:733\n",
      "coef_:[-0.38475026  3.33452834]\n",
      "intercept_:0.23143618355595613\n",
      "train_loss:0.2660774378819486\n",
      "val_loss:0.4873882448025599\n",
      "iter:734\n",
      "coef_:[-0.3847646   3.33455531]\n",
      "intercept_:0.23143735009193953\n",
      "train_loss:0.2660774347813866\n",
      "val_loss:0.4873911404008791\n",
      "iter:735\n",
      "coef_:[-0.38477881  3.33458203]\n",
      "intercept_:0.23143850614634945\n",
      "train_loss:0.2660774317368468\n",
      "val_loss:0.48739400979333275\n",
      "iter:736\n",
      "coef_:[-0.38479289  3.33460852]\n",
      "intercept_:0.23143965181252799\n",
      "train_loss:0.26607742874731644\n",
      "val_loss:0.4873968532166012\n",
      "iter:737\n",
      "coef_:[-0.38480684  3.33463476]\n",
      "intercept_:0.23144078718300065\n",
      "train_loss:0.2660774258118013\n",
      "val_loss:0.4873996709052365\n",
      "iter:738\n",
      "coef_:[-0.38482067  3.33466076]\n",
      "intercept_:0.23144191234948325\n",
      "train_loss:0.26607742292932524\n",
      "val_loss:0.48740246309168084\n",
      "iter:739\n",
      "coef_:[-0.38483437  3.33468653]\n",
      "intercept_:0.23144302740288872\n",
      "train_loss:0.2660774200989295\n",
      "val_loss:0.48740523000628544\n",
      "iter:740\n",
      "coef_:[-0.38484794  3.33471207]\n",
      "intercept_:0.23144413243333395\n",
      "train_loss:0.2660774173196728\n",
      "val_loss:0.4874079718773291\n",
      "iter:741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coef_:[-0.3848614   3.33473737]\n",
      "intercept_:0.23144522753014643\n",
      "train_loss:0.26607741459063095\n",
      "val_loss:0.48741068893103656\n",
      "iter:742\n",
      "coef_:[-0.38487473  3.33476245]\n",
      "intercept_:0.231446312781871\n",
      "train_loss:0.26607741191089634\n",
      "val_loss:0.48741338139159734\n",
      "iter:743\n",
      "coef_:[-0.38488793  3.3347873 ]\n",
      "intercept_:0.23144738827627642\n",
      "train_loss:0.26607740927957785\n",
      "val_loss:0.48741604948118356\n",
      "iter:744\n",
      "coef_:[-0.38490102  3.33481192]\n",
      "intercept_:0.23144845410036197\n",
      "train_loss:0.26607740669580043\n",
      "val_loss:0.4874186934199679\n",
      "iter:745\n",
      "coef_:[-0.38491399  3.33483632]\n",
      "intercept_:0.2314495103403639\n",
      "train_loss:0.2660774041587049\n",
      "val_loss:0.4874213134261414\n",
      "iter:746\n",
      "coef_:[-0.38492684  3.33486049]\n",
      "intercept_:0.23145055708176204\n",
      "train_loss:0.2660774016674476\n",
      "val_loss:0.48742390971593175\n",
      "iter:747\n",
      "coef_:[-0.38493958  3.33488445]\n",
      "intercept_:0.23145159440928612\n",
      "train_loss:0.26607739922120005\n",
      "val_loss:0.48742648250362\n",
      "iter:748\n",
      "coef_:[-0.3849522   3.33490819]\n",
      "intercept_:0.23145262240692208\n",
      "train_loss:0.2660773968191488\n",
      "val_loss:0.48742903200155846\n",
      "iter:749\n",
      "coef_:[-0.3849647   3.33493172]\n",
      "intercept_:0.2314536411579185\n",
      "train_loss:0.2660773944604952\n",
      "val_loss:0.48743155842018765\n",
      "iter:750\n",
      "coef_:[-0.38497709  3.33495503]\n",
      "intercept_:0.23145465074479282\n",
      "train_loss:0.26607739214445497\n",
      "val_loss:0.4874340619680541\n",
      "iter:751\n",
      "coef_:[-0.38498937  3.33497813]\n",
      "intercept_:0.23145565124933756\n",
      "train_loss:0.26607738987025786\n",
      "val_loss:0.48743654285182586\n",
      "iter:752\n",
      "coef_:[-0.38500153  3.33500102]\n",
      "intercept_:0.23145664275262648\n",
      "train_loss:0.2660773876371479\n",
      "val_loss:0.4874390012763108\n",
      "iter:753\n",
      "coef_:[-0.38501359  3.3350237 ]\n",
      "intercept_:0.2314576253350207\n",
      "train_loss:0.26607738544438236\n",
      "val_loss:0.48744143744447244\n",
      "iter:754\n",
      "coef_:[-0.38502554  3.33504618]\n",
      "intercept_:0.23145859907617478\n",
      "train_loss:0.2660773832912322\n",
      "val_loss:0.4874438515574464\n",
      "iter:755\n",
      "coef_:[-0.38503737  3.33506845]\n",
      "intercept_:0.2314595640550427\n",
      "train_loss:0.2660773811769816\n",
      "val_loss:0.4874462438145571\n",
      "iter:756\n",
      "coef_:[-0.3850491   3.33509052]\n",
      "intercept_:0.2314605203498839\n",
      "train_loss:0.26607737910092755\n",
      "val_loss:0.4874486144133337\n",
      "iter:757\n",
      "coef_:[-0.38506073  3.33511239]\n",
      "intercept_:0.2314614680382692\n",
      "train_loss:0.26607737706237977\n",
      "val_loss:0.4874509635495262\n",
      "iter:758\n",
      "coef_:[-0.38507225  3.33513407]\n",
      "intercept_:0.23146240719708658\n",
      "train_loss:0.26607737506066054\n",
      "val_loss:0.4874532914171216\n",
      "iter:759\n",
      "coef_:[-0.38508366  3.33515554]\n",
      "intercept_:0.23146333790254714\n",
      "train_loss:0.26607737309510426\n",
      "val_loss:0.4874555982083592\n",
      "iter:760\n",
      "coef_:[-0.38509497  3.33517683]\n",
      "intercept_:0.2314642602301908\n",
      "train_loss:0.2660773711650575\n",
      "val_loss:0.4874578841137469\n",
      "iter:761\n",
      "coef_:[-0.38510617  3.33519791]\n",
      "intercept_:0.23146517425489208\n",
      "train_loss:0.2660773692698785\n",
      "val_loss:0.48746014932207565\n",
      "iter:762\n",
      "coef_:[-0.38511728  3.33521881]\n",
      "intercept_:0.23146608005086577\n",
      "train_loss:0.26607736740893717\n",
      "val_loss:0.4874623940204361\n",
      "iter:763\n",
      "coef_:[-0.38512828  3.33523952]\n",
      "intercept_:0.23146697769167263\n",
      "train_loss:0.26607736558161493\n",
      "val_loss:0.487464618394233\n",
      "iter:764\n",
      "coef_:[-0.38513919  3.33526004]\n",
      "intercept_:0.23146786725022486\n",
      "train_loss:0.2660773637873043\n",
      "val_loss:0.4874668226272002\n",
      "iter:765\n",
      "coef_:[-0.38514999  3.33528037]\n",
      "intercept_:0.23146874879879187\n",
      "train_loss:0.2660773620254086\n",
      "val_loss:0.48746900690141615\n",
      "iter:766\n",
      "coef_:[-0.3851607   3.33530052]\n",
      "intercept_:0.23146962240900554\n",
      "train_loss:0.26607736029534224\n",
      "val_loss:0.4874711713973185\n",
      "iter:767\n",
      "coef_:[-0.38517131  3.33532049]\n",
      "intercept_:0.23147048815186588\n",
      "train_loss:0.2660773585965299\n",
      "val_loss:0.48747331629371843\n",
      "iter:768\n",
      "coef_:[-0.38518182  3.33534027]\n",
      "intercept_:0.2314713460977464\n",
      "train_loss:0.26607735692840695\n",
      "val_loss:0.4874754417678159\n",
      "iter:769\n",
      "coef_:[-0.38519224  3.33535988]\n",
      "intercept_:0.23147219631639943\n",
      "train_loss:0.26607735529041887\n",
      "val_loss:0.4874775479952136\n",
      "iter:770\n",
      "coef_:[-0.38520256  3.33537931]\n",
      "intercept_:0.23147303887696152\n",
      "train_loss:0.26607735368202107\n",
      "val_loss:0.48747963514993115\n",
      "iter:771\n",
      "coef_:[-0.38521279  3.33539856]\n",
      "intercept_:0.23147387384795876\n",
      "train_loss:0.2660773521026788\n",
      "val_loss:0.4874817034044198\n",
      "iter:772\n",
      "coef_:[-0.38522292  3.33541764]\n",
      "intercept_:0.2314747012973119\n",
      "train_loss:0.26607735055186715\n",
      "val_loss:0.4874837529295762\n",
      "iter:773\n",
      "coef_:[-0.38523297  3.33543654]\n",
      "intercept_:0.2314755212923417\n",
      "train_loss:0.26607734902907065\n",
      "val_loss:0.48748578389475594\n",
      "iter:774\n",
      "coef_:[-0.38524292  3.33545527]\n",
      "intercept_:0.231476333899774\n",
      "train_loss:0.266077347533783\n",
      "val_loss:0.4874877964677883\n",
      "iter:775\n",
      "coef_:[-0.38525278  3.33547384]\n",
      "intercept_:0.23147713918574495\n",
      "train_loss:0.2660773460655071\n",
      "val_loss:0.4874897908149887\n",
      "iter:776\n",
      "coef_:[-0.38526256  3.33549223]\n",
      "intercept_:0.23147793721580587\n",
      "train_loss:0.26607734462375504\n",
      "val_loss:0.4874917671011737\n",
      "iter:777\n",
      "coef_:[-0.38527224  3.33551046]\n",
      "intercept_:0.2314787280549286\n",
      "train_loss:0.2660773432080473\n",
      "val_loss:0.48749372548967296\n",
      "iter:778\n",
      "coef_:[-0.38528184  3.33552852]\n",
      "intercept_:0.23147951176751025\n",
      "train_loss:0.26607734181791365\n",
      "val_loss:0.4874956661423438\n",
      "iter:779\n",
      "coef_:[-0.38529134  3.33554642]\n",
      "intercept_:0.23148028841737825\n",
      "train_loss:0.26607734045289183\n",
      "val_loss:0.4874975892195836\n",
      "iter:780\n",
      "coef_:[-0.38530077  3.33556416]\n",
      "intercept_:0.23148105806779526\n",
      "train_loss:0.26607733911252807\n",
      "val_loss:0.48749949488034344\n",
      "iter:781\n",
      "coef_:[-0.3853101   3.33558173]\n",
      "intercept_:0.23148182078146406\n",
      "train_loss:0.26607733779637677\n",
      "val_loss:0.48750138328214065\n",
      "iter:782\n",
      "coef_:[-0.38531936  3.33559915]\n",
      "intercept_:0.2314825766205324\n",
      "train_loss:0.2660773365040006\n",
      "val_loss:0.4875032545810714\n",
      "iter:783\n",
      "coef_:[-0.38532852  3.3356164 ]\n",
      "intercept_:0.23148332564659768\n",
      "train_loss:0.26607733523497007\n",
      "val_loss:0.4875051089318251\n",
      "iter:784\n",
      "coef_:[-0.38533761  3.33563351]\n",
      "intercept_:0.23148406792071186\n",
      "train_loss:0.2660773339888632\n",
      "val_loss:0.48750694648769427\n",
      "iter:785\n",
      "coef_:[-0.38534661  3.33565045]\n",
      "intercept_:0.23148480350338613\n",
      "train_loss:0.26607733276526585\n",
      "val_loss:0.4875087674005899\n",
      "iter:786\n",
      "coef_:[-0.38535553  3.33566724]\n",
      "intercept_:0.23148553245459552\n",
      "train_loss:0.2660773315637714\n",
      "val_loss:0.48751057182105184\n",
      "iter:787\n",
      "coef_:[-0.38536437  3.33568388]\n",
      "intercept_:0.23148625483378363\n",
      "train_loss:0.26607733038398035\n",
      "val_loss:0.4875123598982619\n",
      "iter:788\n",
      "coef_:[-0.38537313  3.33570037]\n",
      "intercept_:0.2314869706998672\n",
      "train_loss:0.26607732922550065\n",
      "val_loss:0.48751413178005615\n",
      "iter:789\n",
      "coef_:[-0.38538181  3.33571671]\n",
      "intercept_:0.23148768011124066\n",
      "train_loss:0.2660773280879474\n",
      "val_loss:0.4875158876129366\n",
      "iter:790\n",
      "coef_:[-0.38539041  3.3357329 ]\n",
      "intercept_:0.23148838312578066\n",
      "train_loss:0.2660773269709423\n",
      "val_loss:0.48751762754208355\n",
      "iter:791\n",
      "coef_:[-0.38539893  3.33574895]\n",
      "intercept_:0.2314890798008506\n",
      "train_loss:0.2660773258741142\n",
      "val_loss:0.48751935171136723\n",
      "iter:792\n",
      "coef_:[-0.38540738  3.33576485]\n",
      "intercept_:0.23148977019330502\n",
      "train_loss:0.2660773247970985\n",
      "val_loss:0.4875210602633595\n",
      "iter:793\n",
      "coef_:[-0.38541574  3.3357806 ]\n",
      "intercept_:0.23149045435949409\n",
      "train_loss:0.2660773237395374\n",
      "val_loss:0.4875227533393457\n",
      "iter:794\n",
      "coef_:[-0.38542404  3.33579622]\n",
      "intercept_:0.23149113235526786\n",
      "train_loss:0.2660773227010793\n",
      "val_loss:0.48752443107933635\n",
      "iter:795\n",
      "coef_:[-0.38543225  3.33581169]\n",
      "intercept_:0.23149180423598079\n",
      "train_loss:0.26607732168137915\n",
      "val_loss:0.48752609362207794\n",
      "iter:796\n",
      "coef_:[-0.3854404   3.33582702]\n",
      "intercept_:0.23149247005649587\n",
      "train_loss:0.26607732068009793\n",
      "val_loss:0.487527741105065\n",
      "iter:797\n",
      "coef_:[-0.38544846  3.33584221]\n",
      "intercept_:0.23149312987118906\n",
      "train_loss:0.2660773196969029\n",
      "val_loss:0.487529373664551\n",
      "iter:798\n",
      "coef_:[-0.38545646  3.33585726]\n",
      "intercept_:0.23149378373395335\n",
      "train_loss:0.26607731873146745\n",
      "val_loss:0.4875309914355597\n",
      "iter:799\n",
      "coef_:[-0.38546438  3.33587218]\n",
      "intercept_:0.23149443169820313\n",
      "train_loss:0.26607731778347055\n",
      "val_loss:0.48753259455189557\n",
      "iter:800\n",
      "coef_:[-0.38547223  3.33588696]\n",
      "intercept_:0.23149507381687823\n",
      "train_loss:0.2660773168525972\n",
      "val_loss:0.487534183146156\n",
      "iter:801\n",
      "coef_:[-0.38548001  3.33590161]\n",
      "intercept_:0.2314957101424481\n",
      "train_loss:0.26607731593853806\n",
      "val_loss:0.4875357573497407\n",
      "iter:802\n",
      "coef_:[-0.38548772  3.33591612]\n",
      "intercept_:0.23149634072691594\n",
      "train_loss:0.2660773150409895\n",
      "val_loss:0.4875373172928635\n",
      "iter:803\n",
      "coef_:[-0.38549536  3.3359305 ]\n",
      "intercept_:0.23149696562182268\n",
      "train_loss:0.26607731415965297\n",
      "val_loss:0.48753886310456246\n",
      "iter:804\n",
      "coef_:[-0.38550293  3.33594476]\n",
      "intercept_:0.23149758487825106\n",
      "train_loss:0.2660773132942358\n",
      "val_loss:0.48754039491271095\n",
      "iter:805\n",
      "coef_:[-0.38551043  3.33595888]\n",
      "intercept_:0.23149819854682963\n",
      "train_loss:0.26607731244445026\n",
      "val_loss:0.4875419128440272\n",
      "iter:806\n",
      "coef_:[-0.38551786  3.33597287]\n",
      "intercept_:0.2314988066777367\n",
      "train_loss:0.26607731161001397\n",
      "val_loss:0.48754341702408593\n",
      "iter:807\n",
      "coef_:[-0.38552523  3.33598674]\n",
      "intercept_:0.23149940932070426\n",
      "train_loss:0.2660773107906498\n",
      "val_loss:0.4875449075773274\n",
      "iter:808\n",
      "coef_:[-0.38553252  3.33600048]\n",
      "intercept_:0.23150000652502187\n",
      "train_loss:0.2660773099860852\n",
      "val_loss:0.4875463846270688\n",
      "iter:809\n",
      "coef_:[-0.38553975  3.3360141 ]\n",
      "intercept_:0.23150059833954054\n",
      "train_loss:0.26607730919605305\n",
      "val_loss:0.48754784829551306\n",
      "iter:810\n",
      "coef_:[-0.38554692  3.3360276 ]\n",
      "intercept_:0.23150118481267654\n",
      "train_loss:0.2660773084202906\n",
      "val_loss:0.4875492987037601\n",
      "iter:811\n",
      "coef_:[-0.38555402  3.33604097]\n",
      "intercept_:0.2315017659924152\n",
      "train_loss:0.26607730765854015\n",
      "val_loss:0.4875507359718159\n",
      "iter:812\n",
      "coef_:[-0.38556106  3.33605422]\n",
      "intercept_:0.23150234192631472\n",
      "train_loss:0.2660773069105487\n",
      "val_loss:0.48755216021860287\n",
      "iter:813\n",
      "coef_:[-0.38556803  3.33606735]\n",
      "intercept_:0.23150291266150985\n",
      "train_loss:0.2660773061760675\n",
      "val_loss:0.48755357156196893\n",
      "iter:814\n",
      "coef_:[-0.38557494  3.33608036]\n",
      "intercept_:0.23150347824471562\n",
      "train_loss:0.26607730545485264\n",
      "val_loss:0.4875549701186979\n",
      "iter:815\n",
      "coef_:[-0.38558179  3.33609325]\n",
      "intercept_:0.23150403872223096\n",
      "train_loss:0.26607730474666436\n",
      "val_loss:0.4875563560045191\n",
      "iter:816\n",
      "coef_:[-0.38558857  3.33610603]\n",
      "intercept_:0.23150459413994243\n",
      "train_loss:0.26607730405126734\n",
      "val_loss:0.4875577293341155\n",
      "iter:817\n",
      "coef_:[-0.38559529  3.33611869]\n",
      "intercept_:0.2315051445433277\n",
      "train_loss:0.26607730336843055\n",
      "val_loss:0.4875590902211353\n",
      "iter:818\n",
      "coef_:[-0.38560196  3.33613124]\n",
      "intercept_:0.23150568997745935\n",
      "train_loss:0.26607730269792707\n",
      "val_loss:0.4875604387781993\n",
      "iter:819\n",
      "coef_:[-0.38560856  3.33614367]\n",
      "intercept_:0.23150623048700814\n",
      "train_loss:0.2660773020395341\n",
      "val_loss:0.48756177511691134\n",
      "iter:820\n",
      "coef_:[-0.3856151   3.33615599]\n",
      "intercept_:0.23150676611624674\n",
      "train_loss:0.2660773013930329\n",
      "val_loss:0.4875630993478668\n",
      "iter:821\n",
      "coef_:[-0.38562158  3.33616819]\n",
      "intercept_:0.23150729690905314\n",
      "train_loss:0.26607730075820857\n",
      "val_loss:0.4875644115806619\n",
      "iter:822\n",
      "coef_:[-0.385628    3.33618029]\n",
      "intercept_:0.2315078229089141\n",
      "train_loss:0.26607730013485026\n",
      "val_loss:0.48756571192390297\n",
      "iter:823\n",
      "coef_:[-0.38563437  3.33619228]\n",
      "intercept_:0.23150834415892863\n",
      "train_loss:0.26607729952275083\n",
      "val_loss:0.4875670004852149\n",
      "iter:824\n",
      "coef_:[-0.38564067  3.33620416]\n",
      "intercept_:0.2315088607018113\n",
      "train_loss:0.26607729892170684\n",
      "val_loss:0.4875682773712501\n",
      "iter:825\n",
      "coef_:[-0.38564692  3.33621593]\n",
      "intercept_:0.23150937257989568\n",
      "train_loss:0.26607729833151866\n",
      "val_loss:0.4875695426876975\n",
      "iter:826\n",
      "coef_:[-0.38565312  3.33622759]\n",
      "intercept_:0.23150987983513774\n",
      "train_loss:0.2660772977519901\n",
      "val_loss:0.4875707965392908\n",
      "iter:827\n",
      "coef_:[-0.38565925  3.33623915]\n",
      "intercept_:0.23151038250911904\n",
      "train_loss:0.2660772971829288\n",
      "val_loss:0.48757203902981777\n",
      "iter:828\n",
      "coef_:[-0.38566533  3.3362506 ]\n",
      "intercept_:0.23151088064305012\n",
      "train_loss:0.26607729662414537\n",
      "val_loss:0.48757327026212777\n",
      "iter:829\n",
      "coef_:[-0.38567136  3.33626195]\n",
      "intercept_:0.23151137427777366\n",
      "train_loss:0.26607729607545433\n",
      "val_loss:0.4875744903381411\n",
      "iter:830\n",
      "coef_:[-0.38567733  3.3362732 ]\n",
      "intercept_:0.23151186345376779\n",
      "train_loss:0.26607729553667336\n",
      "val_loss:0.4875756993588567\n",
      "iter:831\n",
      "coef_:[-0.38568325  3.33628434]\n",
      "intercept_:0.23151234821114927\n",
      "train_loss:0.2660772950076235\n",
      "val_loss:0.4875768974243613\n",
      "iter:832\n",
      "coef_:[-0.38568911  3.33629538]\n",
      "intercept_:0.2315128285896767\n",
      "train_loss:0.2660772944881288\n",
      "val_loss:0.48757808463383656\n",
      "iter:833\n",
      "coef_:[-0.38569492  3.33630633]\n",
      "intercept_:0.2315133046287536\n",
      "train_loss:0.2660772939780169\n",
      "val_loss:0.4875792610855678\n",
      "iter:834\n",
      "coef_:[-0.38570068  3.33631717]\n",
      "intercept_:0.23151377636743162\n",
      "train_loss:0.26607729347711817\n",
      "val_loss:0.48758042687695224\n",
      "iter:835\n",
      "coef_:[-0.38570638  3.33632792]\n",
      "intercept_:0.23151424384441352\n",
      "train_loss:0.2660772929852661\n",
      "val_loss:0.4875815821045068\n",
      "iter:836\n",
      "coef_:[-0.38571204  3.33633856]\n",
      "intercept_:0.23151470709805635\n",
      "train_loss:0.26607729250229745\n",
      "val_loss:0.4875827268638758\n",
      "iter:837\n",
      "coef_:[-0.38571764  3.33634912]\n",
      "intercept_:0.23151516616637444\n",
      "train_loss:0.26607729202805164\n",
      "val_loss:0.48758386124983916\n",
      "iter:838\n",
      "coef_:[-0.38572319  3.33635957]\n",
      "intercept_:0.23151562108704243\n",
      "train_loss:0.2660772915623712\n",
      "val_loss:0.48758498535632006\n",
      "iter:839\n",
      "coef_:[-0.38572869  3.33636993]\n",
      "intercept_:0.23151607189739826\n",
      "train_loss:0.2660772911051013\n",
      "val_loss:0.4875860992763927\n",
      "iter:840\n",
      "coef_:[-0.38573414  3.3363802 ]\n",
      "intercept_:0.23151651863444614\n",
      "train_loss:0.2660772906560901\n",
      "val_loss:0.4875872031022898\n",
      "iter:841\n",
      "coef_:[-0.38573954  3.33639037]\n",
      "intercept_:0.2315169613348595\n",
      "train_loss:0.2660772902151884\n",
      "val_loss:0.4875882969254104\n",
      "iter:842\n",
      "coef_:[-0.38574489  3.33640045]\n",
      "intercept_:0.2315174000349839\n",
      "train_loss:0.2660772897822497\n",
      "val_loss:0.48758938083632714\n",
      "iter:843\n",
      "coef_:[-0.3857502   3.33641044]\n",
      "intercept_:0.23151783477083984\n",
      "train_loss:0.2660772893571302\n",
      "val_loss:0.487590454924794\n",
      "iter:844\n",
      "coef_:[-0.38575545  3.33642034]\n",
      "intercept_:0.2315182655781258\n",
      "train_loss:0.2660772889396886\n",
      "val_loss:0.48759151927975336\n",
      "iter:845\n",
      "coef_:[-0.38576066  3.33643015]\n",
      "intercept_:0.23151869249222093\n",
      "train_loss:0.26607728852978635\n",
      "val_loss:0.4875925739893435\n",
      "iter:846\n",
      "coef_:[-0.38576582  3.33643987]\n",
      "intercept_:0.23151911554818794\n",
      "train_loss:0.2660772881272871\n",
      "val_loss:0.4875936191409059\n",
      "iter:847\n",
      "coef_:[-0.38577093  3.33644951]\n",
      "intercept_:0.23151953478077583\n",
      "train_loss:0.26607728773205724\n",
      "val_loss:0.4875946548209923\n",
      "iter:848\n",
      "coef_:[-0.385776    3.33645905]\n",
      "intercept_:0.23151995022442276\n",
      "train_loss:0.2660772873439654\n",
      "val_loss:0.48759568111537177\n",
      "iter:849\n",
      "coef_:[-0.38578102  3.33646851]\n",
      "intercept_:0.2315203619132587\n",
      "train_loss:0.2660772869628827\n",
      "val_loss:0.4875966981090379\n",
      "iter:850\n",
      "coef_:[-0.385786    3.33647788]\n",
      "intercept_:0.2315207698811082\n",
      "train_loss:0.2660772865886825\n",
      "val_loss:0.4875977058862157\n",
      "iter:851\n",
      "coef_:[-0.38579093  3.33648717]\n",
      "intercept_:0.23152117416149307\n",
      "train_loss:0.2660772862212405\n",
      "val_loss:0.4875987045303688\n",
      "iter:852\n",
      "coef_:[-0.38579581  3.33649637]\n",
      "intercept_:0.23152157478763505\n",
      "train_loss:0.26607728586043466\n",
      "val_loss:0.48759969412420573\n",
      "iter:853\n",
      "coef_:[-0.38580066  3.33650549]\n",
      "intercept_:0.23152197179245848\n",
      "train_loss:0.26607728550614507\n",
      "val_loss:0.4876006747496874\n",
      "iter:854\n",
      "coef_:[-0.38580545  3.33651453]\n",
      "intercept_:0.23152236520859296\n",
      "train_loss:0.2660772851582541\n",
      "val_loss:0.4876016464880333\n",
      "iter:855\n",
      "coef_:[-0.38581021  3.33652349]\n",
      "intercept_:0.23152275506837583\n",
      "train_loss:0.26607728481664605\n",
      "val_loss:0.48760260941972844\n",
      "iter:856\n",
      "coef_:[-0.38581492  3.33653236]\n",
      "intercept_:0.2315231414038549\n",
      "train_loss:0.2660772844812075\n",
      "val_loss:0.4876035636245302\n",
      "iter:857\n",
      "coef_:[-0.38581958  3.33654115]\n",
      "intercept_:0.231523524246791\n",
      "train_loss:0.266077284151827\n",
      "val_loss:0.4876045091814742\n",
      "iter:858\n",
      "coef_:[-0.38582421  3.33654987]\n",
      "intercept_:0.2315239036286604\n",
      "train_loss:0.26607728382839524\n",
      "val_loss:0.48760544616888163\n",
      "iter:859\n",
      "coef_:[-0.38582879  3.3365585 ]\n",
      "intercept_:0.23152427958065738\n",
      "train_loss:0.2660772835108047\n",
      "val_loss:0.4876063746643651\n",
      "iter:860\n",
      "coef_:[-0.38583334  3.33656706]\n",
      "intercept_:0.23152465213369683\n",
      "train_loss:0.2660772831989498\n",
      "val_loss:0.4876072947448354\n",
      "iter:861\n",
      "coef_:[-0.38583784  3.33657554]\n",
      "intercept_:0.23152502131841657\n",
      "train_loss:0.266077282892727\n",
      "val_loss:0.48760820648650766\n",
      "iter:862\n",
      "coef_:[-0.3858423   3.33658394]\n",
      "intercept_:0.23152538716517992\n",
      "train_loss:0.26607728259203467\n",
      "val_loss:0.48760910996490736\n",
      "iter:863\n",
      "coef_:[-0.38584672  3.33659227]\n",
      "intercept_:0.23152574970407808\n",
      "train_loss:0.26607728229677274\n",
      "val_loss:0.4876100052548772\n",
      "iter:864\n",
      "coef_:[-0.38585109  3.33660052]\n",
      "intercept_:0.2315261089649325\n",
      "train_loss:0.2660772820068432\n",
      "val_loss:0.4876108924305824\n",
      "iter:865\n",
      "coef_:[-0.38585543  3.33660869]\n",
      "intercept_:0.23152646497729737\n",
      "train_loss:0.2660772817221498\n",
      "val_loss:0.4876117715655181\n",
      "iter:866\n",
      "coef_:[-0.38585973  3.33661679]\n",
      "intercept_:0.2315268177704619\n",
      "train_loss:0.2660772814425979\n",
      "val_loss:0.48761264273251376\n",
      "iter:867\n",
      "coef_:[-0.385864    3.33662482]\n",
      "intercept_:0.23152716737345275\n",
      "train_loss:0.26607728116809465\n",
      "val_loss:0.4876135060037407\n",
      "iter:868\n",
      "coef_:[-0.38586822  3.33663278]\n",
      "intercept_:0.23152751381503625\n",
      "train_loss:0.2660772808985489\n",
      "val_loss:0.4876143614507167\n",
      "iter:869\n",
      "coef_:[-0.3858724   3.33664066]\n",
      "intercept_:0.2315278571237208\n",
      "train_loss:0.26607728063387104\n",
      "val_loss:0.48761520914431317\n",
      "iter:870\n",
      "coef_:[-0.38587655  3.33664847]\n",
      "intercept_:0.23152819732775912\n",
      "train_loss:0.2660772803739732\n",
      "val_loss:0.48761604915475987\n",
      "iter:871\n",
      "coef_:[-0.38588066  3.33665621]\n",
      "intercept_:0.23152853445515054\n",
      "train_loss:0.2660772801187691\n",
      "val_loss:0.48761688155165145\n",
      "iter:872\n",
      "coef_:[-0.38588473  3.33666388]\n",
      "intercept_:0.23152886853364324\n",
      "train_loss:0.26607727986817375\n",
      "val_loss:0.4876177064039532\n",
      "iter:873\n",
      "coef_:[-0.38588876  3.33667148]\n",
      "intercept_:0.23152919959073645\n",
      "train_loss:0.2660772796221042\n",
      "val_loss:0.4876185237800057\n",
      "iter:874\n",
      "coef_:[-0.38589276  3.33667902]\n",
      "intercept_:0.2315295276536827\n",
      "train_loss:0.2660772793804785\n",
      "val_loss:0.48761933374753197\n",
      "iter:875\n",
      "coef_:[-0.38589672  3.33668648]\n",
      "intercept_:0.23152985274948998\n",
      "train_loss:0.26607727914321644\n",
      "val_loss:0.48762013637364177\n",
      "iter:876\n",
      "coef_:[-0.38590065  3.33669387]\n",
      "intercept_:0.23153017490492392\n",
      "train_loss:0.26607727891023925\n",
      "val_loss:0.4876209317248381\n",
      "iter:877\n",
      "coef_:[-0.38590454  3.3367012 ]\n",
      "intercept_:0.23153049414650997\n",
      "train_loss:0.26607727868146946\n",
      "val_loss:0.4876217198670217\n",
      "iter:878\n",
      "coef_:[-0.38590839  3.33670847]\n",
      "intercept_:0.2315308105005355\n",
      "train_loss:0.2660772784568312\n",
      "val_loss:0.4876225008654976\n",
      "iter:879\n",
      "coef_:[-0.38591221  3.33671566]\n",
      "intercept_:0.23153112399305195\n",
      "train_loss:0.26607727823624977\n",
      "val_loss:0.4876232747849793\n",
      "iter:880\n",
      "coef_:[-0.38591599  3.33672279]\n",
      "intercept_:0.23153143464987685\n",
      "train_loss:0.2660772780196518\n",
      "val_loss:0.48762404168959533\n",
      "iter:881\n",
      "coef_:[-0.38591975  3.33672986]\n",
      "intercept_:0.231531742496596\n",
      "train_loss:0.26607727780696555\n",
      "val_loss:0.48762480164289357\n",
      "iter:882\n",
      "coef_:[-0.38592346  3.33673686]\n",
      "intercept_:0.23153204755856546\n",
      "train_loss:0.2660772775981203\n",
      "val_loss:0.487625554707847\n",
      "iter:883\n",
      "coef_:[-0.38592714  3.3367438 ]\n",
      "intercept_:0.23153234986091362\n",
      "train_loss:0.2660772773930466\n",
      "val_loss:0.4876263009468586\n",
      "iter:884\n",
      "coef_:[-0.38593079  3.33675068]\n",
      "intercept_:0.2315326494285432\n",
      "train_loss:0.26607727719167634\n",
      "val_loss:0.4876270404217668\n",
      "iter:885\n",
      "coef_:[-0.38593441  3.33675749]\n",
      "intercept_:0.23153294628613333\n",
      "train_loss:0.2660772769939428\n",
      "val_loss:0.48762777319385014\n",
      "iter:886\n",
      "coef_:[-0.38593799  3.33676424]\n",
      "intercept_:0.23153324045814141\n",
      "train_loss:0.26607727679978005\n",
      "val_loss:0.48762849932383306\n",
      "iter:887\n",
      "coef_:[-0.38594154  3.33677093]\n",
      "intercept_:0.23153353196880522\n",
      "train_loss:0.26607727660912384\n",
      "val_loss:0.4876292188718901\n",
      "iter:888\n",
      "coef_:[-0.38594506  3.33677756]\n",
      "intercept_:0.23153382084214477\n",
      "train_loss:0.26607727642191065\n",
      "val_loss:0.48762993189765125\n",
      "iter:889\n",
      "coef_:[-0.38594855  3.33678413]\n",
      "intercept_:0.23153410710196434\n",
      "train_loss:0.2660772762380784\n",
      "val_loss:0.48763063846020693\n",
      "iter:890\n",
      "coef_:[-0.385952    3.33679064]\n",
      "intercept_:0.23153439077185428\n",
      "train_loss:0.26607727605756604\n",
      "val_loss:0.48763133861811253\n",
      "iter:891\n",
      "coef_:[-0.38595543  3.33679709]\n",
      "intercept_:0.231534671875193\n",
      "train_loss:0.2660772758803135\n",
      "val_loss:0.48763203242939396\n",
      "iter:892\n",
      "coef_:[-0.38595882  3.33680349]\n",
      "intercept_:0.23153495043514882\n",
      "train_loss:0.266077275706262\n",
      "val_loss:0.4876327199515515\n",
      "iter:893\n",
      "coef_:[-0.38596218  3.33680982]\n",
      "intercept_:0.2315352264746819\n",
      "train_loss:0.2660772755353537\n",
      "val_loss:0.48763340124156523\n",
      "iter:894\n",
      "coef_:[-0.38596551  3.3368161 ]\n",
      "intercept_:0.23153550001654596\n",
      "train_loss:0.2660772753675318\n",
      "val_loss:0.4876340763558991\n",
      "iter:895\n",
      "coef_:[-0.38596881  3.33682232]\n",
      "intercept_:0.23153577108329032\n",
      "train_loss:0.26607727520274066\n",
      "val_loss:0.4876347453505069\n",
      "iter:896\n",
      "coef_:[-0.38597209  3.33682848]\n",
      "intercept_:0.23153603969726153\n",
      "train_loss:0.2660772750409255\n",
      "val_loss:0.4876354082808348\n",
      "iter:897\n",
      "coef_:[-0.38597533  3.33683459]\n",
      "intercept_:0.23153630588060528\n",
      "train_loss:0.26607727488203237\n",
      "val_loss:0.4876360652018279\n",
      "iter:898\n",
      "coef_:[-0.38597854  3.33684064]\n",
      "intercept_:0.23153656965526814\n",
      "train_loss:0.26607727472600884\n",
      "val_loss:0.48763671616793375\n",
      "iter:899\n",
      "coef_:[-0.38598172  3.33684664]\n",
      "intercept_:0.2315368310429994\n",
      "train_loss:0.26607727457280284\n",
      "val_loss:0.4876373612331068\n",
      "iter:900\n",
      "coef_:[-0.38598488  3.33685258]\n",
      "intercept_:0.23153709006535278\n",
      "train_loss:0.2660772744223637\n",
      "val_loss:0.4876380004508135\n",
      "iter:901\n",
      "coef_:[-0.385988    3.33685847]\n",
      "intercept_:0.23153734674368817\n",
      "train_loss:0.2660772742746412\n",
      "val_loss:0.48763863387403605\n",
      "iter:902\n",
      "coef_:[-0.3859911   3.33686431]\n",
      "intercept_:0.23153760109917337\n",
      "train_loss:0.26607727412958637\n",
      "val_loss:0.487639261555277\n",
      "iter:903\n",
      "coef_:[-0.38599417  3.33687009]\n",
      "intercept_:0.23153785315278586\n",
      "train_loss:0.26607727398715114\n",
      "val_loss:0.4876398835465639\n",
      "iter:904\n",
      "coef_:[-0.38599721  3.33687582]\n",
      "intercept_:0.23153810292531435\n",
      "train_loss:0.2660772738472881\n",
      "val_loss:0.4876404998994528\n",
      "iter:905\n",
      "coef_:[-0.38600022  3.3368815 ]\n",
      "intercept_:0.23153835043736062\n",
      "train_loss:0.26607727370995077\n",
      "val_loss:0.4876411106650337\n",
      "iter:906\n",
      "coef_:[-0.38600321  3.33688713]\n",
      "intercept_:0.23153859570934113\n",
      "train_loss:0.2660772735750936\n",
      "val_loss:0.48764171589393357\n",
      "iter:907\n",
      "coef_:[-0.38600617  3.33689271]\n",
      "intercept_:0.23153883876148862\n",
      "train_loss:0.2660772734426717\n",
      "val_loss:0.4876423156363211\n",
      "iter:908\n",
      "coef_:[-0.3860091   3.33689823]\n",
      "intercept_:0.23153907961385387\n",
      "train_loss:0.26607727331264125\n",
      "val_loss:0.48764290994191106\n",
      "iter:909\n",
      "coef_:[-0.386012    3.33690371]\n",
      "intercept_:0.2315393182863072\n",
      "train_loss:0.26607727318495894\n",
      "val_loss:0.4876434988599679\n",
      "iter:910\n",
      "coef_:[-0.38601488  3.33690913]\n",
      "intercept_:0.23153955479854016\n",
      "train_loss:0.2660772730595824\n",
      "val_loss:0.4876440824393102\n",
      "iter:911\n",
      "coef_:[-0.38601774  3.33691451]\n",
      "intercept_:0.2315397891700671\n",
      "train_loss:0.26607727293647004\n",
      "val_loss:0.4876446607283142\n",
      "iter:912\n",
      "coef_:[-0.38602056  3.33691984]\n",
      "intercept_:0.23154002142022673\n",
      "train_loss:0.26607727281558086\n",
      "val_loss:0.4876452337749187\n",
      "iter:913\n",
      "coef_:[-0.38602337  3.33692512]\n",
      "intercept_:0.23154025156818375\n",
      "train_loss:0.26607727269687476\n",
      "val_loss:0.4876458016266281\n",
      "iter:914\n",
      "coef_:[-0.38602614  3.33693035]\n",
      "intercept_:0.23154047963293037\n",
      "train_loss:0.26607727258031233\n",
      "val_loss:0.4876463643305166\n",
      "iter:915\n",
      "coef_:[-0.38602889  3.33693553]\n",
      "intercept_:0.23154070563328782\n",
      "train_loss:0.26607727246585483\n",
      "val_loss:0.4876469219332327\n",
      "iter:916\n",
      "coef_:[-0.38603162  3.33694067]\n",
      "intercept_:0.23154092958790792\n",
      "train_loss:0.2660772723534643\n",
      "val_loss:0.48764747448100193\n",
      "iter:917\n",
      "coef_:[-0.38603432  3.33694576]\n",
      "intercept_:0.23154115151527457\n",
      "train_loss:0.2660772722431033\n",
      "val_loss:0.4876480220196315\n",
      "iter:918\n",
      "coef_:[-0.386037   3.3369508]\n",
      "intercept_:0.2315413714337053\n",
      "train_loss:0.2660772721347353\n",
      "val_loss:0.487648564594514\n",
      "iter:919\n",
      "coef_:[-0.38603965  3.3369558 ]\n",
      "intercept_:0.23154158936135263\n",
      "train_loss:0.26607727202832426\n",
      "val_loss:0.48764910225063074\n",
      "iter:920\n",
      "coef_:[-0.38604228  3.33696076]\n",
      "intercept_:0.23154180531620577\n",
      "train_loss:0.26607727192383485\n",
      "val_loss:0.48764963503255593\n",
      "iter:921\n",
      "coef_:[-0.38604488  3.33696566]\n",
      "intercept_:0.23154201931609183\n",
      "train_loss:0.26607727182123225\n",
      "val_loss:0.4876501629844601\n",
      "iter:922\n",
      "coef_:[-0.38604746  3.33697053]\n",
      "intercept_:0.23154223137867744\n",
      "train_loss:0.2660772717204826\n",
      "val_loss:0.48765068615011375\n",
      "iter:923\n",
      "coef_:[-0.38605002  3.33697535]\n",
      "intercept_:0.2315424415214701\n",
      "train_loss:0.26607727162155226\n",
      "val_loss:0.487651204572891\n",
      "iter:924\n",
      "coef_:[-0.38605255  3.33698012]\n",
      "intercept_:0.23154264976181965\n",
      "train_loss:0.26607727152440847\n",
      "val_loss:0.4876517182957734\n",
      "iter:925\n",
      "coef_:[-0.38605507  3.33698486]\n",
      "intercept_:0.23154285611691966\n",
      "train_loss:0.26607727142901894\n",
      "val_loss:0.48765222736135316\n",
      "iter:926\n",
      "coef_:[-0.38605755  3.33698955]\n",
      "intercept_:0.23154306060380883\n",
      "train_loss:0.266077271335352\n",
      "val_loss:0.48765273181183666\n",
      "iter:927\n",
      "coef_:[-0.38606002  3.33699419]\n",
      "intercept_:0.23154326323937238\n",
      "train_loss:0.2660772712433765\n",
      "val_loss:0.4876532316890483\n",
      "iter:928\n",
      "coef_:[-0.38606246  3.3369988 ]\n",
      "intercept_:0.23154346404034343\n",
      "train_loss:0.26607727115306185\n",
      "val_loss:0.4876537270344333\n",
      "iter:929\n",
      "coef_:[-0.38606489  3.33700336]\n",
      "intercept_:0.2315436630233043\n",
      "train_loss:0.26607727106437823\n",
      "val_loss:0.4876542178890622\n",
      "iter:930\n",
      "coef_:[-0.38606728  3.33700788]\n",
      "intercept_:0.23154386020468803\n",
      "train_loss:0.26607727097729594\n",
      "val_loss:0.48765470429363333\n",
      "iter:931\n",
      "coef_:[-0.38606966  3.33701236]\n",
      "intercept_:0.23154405560077945\n",
      "train_loss:0.2660772708917863\n",
      "val_loss:0.48765518628847615\n",
      "iter:932\n",
      "coef_:[-0.38607202  3.3370168 ]\n",
      "intercept_:0.23154424922771677\n",
      "train_loss:0.2660772708078208\n",
      "val_loss:0.4876556639135551\n",
      "iter:933\n",
      "coef_:[-0.38607435  3.3370212 ]\n",
      "intercept_:0.23154444110149272\n",
      "train_loss:0.2660772707253715\n",
      "val_loss:0.4876561372084727\n",
      "iter:934\n",
      "coef_:[-0.38607667  3.33702556]\n",
      "intercept_:0.23154463123795593\n",
      "train_loss:0.26607727064441117\n",
      "val_loss:0.48765660621247275\n",
      "iter:935\n",
      "coef_:[-0.38607896  3.33702988]\n",
      "intercept_:0.23154481965281218\n",
      "train_loss:0.26607727056491287\n",
      "val_loss:0.487657070964444\n",
      "iter:936\n",
      "coef_:[-0.38608123  3.33703417]\n",
      "intercept_:0.23154500636162573\n",
      "train_loss:0.2660772704868501\n",
      "val_loss:0.4876575315029223\n",
      "iter:937\n",
      "coef_:[-0.38608348  3.33703841]\n",
      "intercept_:0.23154519137982055\n",
      "train_loss:0.26607727041019685\n",
      "val_loss:0.48765798786609493\n",
      "iter:938\n",
      "coef_:[-0.38608571  3.33704261]\n",
      "intercept_:0.2315453747226816\n",
      "train_loss:0.26607727033492795\n",
      "val_loss:0.48765844009180326\n",
      "iter:939\n",
      "coef_:[-0.38608792  3.33704678]\n",
      "intercept_:0.23154555640535604\n",
      "train_loss:0.2660772702610183\n",
      "val_loss:0.4876588882175459\n",
      "iter:940\n",
      "coef_:[-0.38609011  3.33705091]\n",
      "intercept_:0.23154573644285445\n",
      "train_loss:0.26607727018844324\n",
      "val_loss:0.48765933228048186\n",
      "iter:941\n",
      "coef_:[-0.38609228  3.337055  ]\n",
      "intercept_:0.23154591485005216\n",
      "train_loss:0.26607727011717874\n",
      "val_loss:0.48765977231743335\n",
      "iter:942\n",
      "coef_:[-0.38609443  3.33705905]\n",
      "intercept_:0.23154609164169038\n",
      "train_loss:0.26607727004720116\n",
      "val_loss:0.48766020836488944\n",
      "iter:943\n",
      "coef_:[-0.38609657  3.33706307]\n",
      "intercept_:0.23154626683237736\n",
      "train_loss:0.26607726997848724\n",
      "val_loss:0.4876606404590085\n",
      "iter:944\n",
      "coef_:[-0.38609868  3.33706705]\n",
      "intercept_:0.2315464404365897\n",
      "train_loss:0.2660772699110141\n",
      "val_loss:0.48766106863562114\n",
      "iter:945\n",
      "coef_:[-0.38610077  3.33707099]\n",
      "intercept_:0.23154661246867345\n",
      "train_loss:0.26607726984475943\n",
      "val_loss:0.4876614929302342\n",
      "iter:946\n",
      "coef_:[-0.38610284  3.3370749 ]\n",
      "intercept_:0.23154678294284525\n",
      "train_loss:0.26607726977970114\n",
      "val_loss:0.4876619133780318\n",
      "iter:947\n",
      "coef_:[-0.3861049   3.33707877]\n",
      "intercept_:0.2315469518731936\n",
      "train_loss:0.26607726971581774\n",
      "val_loss:0.4876623300138803\n",
      "iter:948\n",
      "coef_:[-0.38610694  3.33708261]\n",
      "intercept_:0.23154711927367994\n",
      "train_loss:0.2660772696530879\n",
      "val_loss:0.4876627428723299\n",
      "iter:949\n",
      "coef_:[-0.38610895  3.33708642]\n",
      "intercept_:0.23154728515813974\n",
      "train_loss:0.2660772695914909\n",
      "val_loss:0.48766315198761767\n",
      "iter:950\n",
      "coef_:[-0.38611095  3.33709018]\n",
      "intercept_:0.23154744954028375\n",
      "train_loss:0.2660772695310061\n",
      "val_loss:0.48766355739367073\n",
      "iter:951\n",
      "coef_:[-0.38611293  3.33709392]\n",
      "intercept_:0.23154761243369903\n",
      "train_loss:0.26607726947161364\n",
      "val_loss:0.4876639591241089\n",
      "iter:952\n",
      "coef_:[-0.3861149   3.33709762]\n",
      "intercept_:0.2315477738518501\n",
      "train_loss:0.26607726941329357\n",
      "val_loss:0.4876643572122474\n",
      "iter:953\n",
      "coef_:[-0.38611684  3.33710129]\n",
      "intercept_:0.23154793380807998\n",
      "train_loss:0.2660772693560267\n",
      "val_loss:0.4876647516910995\n",
      "iter:954\n",
      "coef_:[-0.38611877  3.33710492]\n",
      "intercept_:0.23154809231561138\n",
      "train_loss:0.2660772692997939\n",
      "val_loss:0.4876651425933798\n",
      "iter:955\n",
      "coef_:[-0.38612068  3.33710852]\n",
      "intercept_:0.2315482493875477\n",
      "train_loss:0.2660772692445766\n",
      "val_loss:0.4876655299515062\n",
      "iter:956\n",
      "coef_:[-0.38612257  3.33711209]\n",
      "intercept_:0.23154840503687413\n",
      "train_loss:0.26607726919035635\n",
      "val_loss:0.48766591379760316\n",
      "iter:957\n",
      "coef_:[-0.38612445  3.33711563]\n",
      "intercept_:0.2315485592764587\n",
      "train_loss:0.2660772691371151\n",
      "val_loss:0.48766629416350377\n",
      "iter:958\n",
      "coef_:[-0.38612631  3.33711913]\n",
      "intercept_:0.23154871211905334\n",
      "train_loss:0.2660772690848355\n",
      "val_loss:0.48766667108075323\n",
      "iter:959\n",
      "coef_:[-0.38612815  3.3371226 ]\n",
      "intercept_:0.23154886357729493\n",
      "train_loss:0.2660772690334998\n",
      "val_loss:0.4876670445806107\n",
      "iter:960\n",
      "coef_:[-0.38612998  3.33712604]\n",
      "intercept_:0.2315490136637063\n",
      "train_loss:0.2660772689830911\n",
      "val_loss:0.48766741469405206\n",
      "iter:961\n",
      "coef_:[-0.38613179  3.33712945]\n",
      "intercept_:0.23154916239069734\n",
      "train_loss:0.2660772689335927\n",
      "val_loss:0.48766778145177286\n",
      "iter:962\n",
      "coef_:[-0.38613358  3.33713283]\n",
      "intercept_:0.23154930977056584\n",
      "train_loss:0.2660772688849881\n",
      "val_loss:0.4876681448841903\n",
      "iter:963\n",
      "coef_:[-0.38613535  3.33713618]\n",
      "intercept_:0.23154945581549874\n",
      "train_loss:0.2660772688372612\n",
      "val_loss:0.48766850502144615\n",
      "iter:964\n",
      "coef_:[-0.38613711  3.33713949]\n",
      "intercept_:0.2315496005375729\n",
      "train_loss:0.2660772687903961\n",
      "val_loss:0.48766886189340886\n",
      "iter:965\n",
      "coef_:[-0.38613886  3.33714278]\n",
      "intercept_:0.23154974394875621\n",
      "train_loss:0.2660772687443774\n",
      "val_loss:0.4876692155296769\n",
      "iter:966\n",
      "coef_:[-0.38614059  3.33714604]\n",
      "intercept_:0.23154988606090857\n",
      "train_loss:0.2660772686991896\n",
      "val_loss:0.4876695659595797\n",
      "iter:967\n",
      "coef_:[-0.3861423   3.33714927]\n",
      "intercept_:0.23155002688578277\n",
      "train_loss:0.2660772686548178\n",
      "val_loss:0.48766991321218184\n",
      "iter:968\n",
      "coef_:[-0.386144    3.33715247]\n",
      "intercept_:0.23155016643502557\n",
      "train_loss:0.2660772686112472\n",
      "val_loss:0.487670257316284\n",
      "iter:969\n",
      "coef_:[-0.38614568  3.33715563]\n",
      "intercept_:0.2315503047201786\n",
      "train_loss:0.2660772685684635\n",
      "val_loss:0.48767059830042625\n",
      "iter:970\n",
      "coef_:[-0.38614734  3.33715878]\n",
      "intercept_:0.23155044175267925\n",
      "train_loss:0.26607726852645225\n",
      "val_loss:0.4876709361928897\n",
      "iter:971\n",
      "coef_:[-0.386149    3.33716189]\n",
      "intercept_:0.2315505775438617\n",
      "train_loss:0.26607726848519964\n",
      "val_loss:0.48767127102169966\n",
      "iter:972\n",
      "coef_:[-0.38615063  3.33716497]\n",
      "intercept_:0.2315507121049578\n",
      "train_loss:0.266077268444692\n",
      "val_loss:0.4876716028146273\n",
      "iter:973\n",
      "coef_:[-0.38615225  3.33716803]\n",
      "intercept_:0.231550845447098\n",
      "train_loss:0.26607726840491575\n",
      "val_loss:0.48767193159919214\n",
      "iter:974\n",
      "coef_:[-0.38615386  3.33717106]\n",
      "intercept_:0.23155097758131224\n",
      "train_loss:0.2660772683658578\n",
      "val_loss:0.48767225740266457\n",
      "iter:975\n",
      "coef_:[-0.38615545  3.33717406]\n",
      "intercept_:0.23155110851853086\n",
      "train_loss:0.2660772683275052\n",
      "val_loss:0.48767258025206756\n",
      "iter:976\n",
      "coef_:[-0.38615703  3.33717703]\n",
      "intercept_:0.23155123826958557\n",
      "train_loss:0.2660772682898451\n",
      "val_loss:0.48767290017417964\n",
      "iter:977\n",
      "coef_:[-0.38615859  3.33717998]\n",
      "intercept_:0.23155136684521022\n",
      "train_loss:0.26607726825286504\n",
      "val_loss:0.48767321719553636\n",
      "iter:978\n",
      "coef_:[-0.38616014  3.3371829 ]\n",
      "intercept_:0.2315514942560417\n",
      "train_loss:0.2660772682165527\n",
      "val_loss:0.487673531342433\n",
      "iter:979\n",
      "coef_:[-0.38616168  3.33718579]\n",
      "intercept_:0.23155162051262096\n",
      "train_loss:0.26607726818089616\n",
      "val_loss:0.48767384264092656\n",
      "iter:980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coef_:[-0.3861632   3.33718866]\n",
      "intercept_:0.23155174562539363\n",
      "train_loss:0.26607726814588345\n",
      "val_loss:0.48767415111683804\n",
      "iter:981\n",
      "coef_:[-0.38616471  3.3371915 ]\n",
      "intercept_:0.23155186960471108\n",
      "train_loss:0.26607726811150306\n",
      "val_loss:0.4876744567957542\n",
      "iter:982\n",
      "coef_:[-0.3861662   3.33719432]\n",
      "intercept_:0.23155199246083116\n",
      "train_loss:0.2660772680777433\n",
      "val_loss:0.48767475970303037\n",
      "iter:983\n",
      "coef_:[-0.38616768  3.33719711]\n",
      "intercept_:0.2315521142039191\n",
      "train_loss:0.26607726804459325\n",
      "val_loss:0.4876750598637916\n",
      "iter:984\n",
      "coef_:[-0.38616915  3.33719987]\n",
      "intercept_:0.23155223484404833\n",
      "train_loss:0.26607726801204196\n",
      "val_loss:0.48767535730293604\n",
      "iter:985\n",
      "coef_:[-0.3861706   3.33720261]\n",
      "intercept_:0.23155235439120125\n",
      "train_loss:0.26607726798007825\n",
      "val_loss:0.48767565204513524\n",
      "iter:986\n",
      "coef_:[-0.38617204  3.33720533]\n",
      "intercept_:0.23155247285527011\n",
      "train_loss:0.2660772679486918\n",
      "val_loss:0.487675944114838\n",
      "iter:987\n",
      "coef_:[-0.38617347  3.33720802]\n",
      "intercept_:0.23155259024605782\n",
      "train_loss:0.2660772679178721\n",
      "val_loss:0.4876762335362713\n",
      "iter:988\n",
      "coef_:[-0.38617488  3.33721068]\n",
      "intercept_:0.2315527065732787\n",
      "train_loss:0.2660772678876089\n",
      "val_loss:0.4876765203334426\n",
      "iter:989\n",
      "coef_:[-0.38617628  3.33721332]\n",
      "intercept_:0.23155282184655934\n",
      "train_loss:0.26607726785789226\n",
      "val_loss:0.4876768045301417\n",
      "iter:990\n",
      "coef_:[-0.38617767  3.33721594]\n",
      "intercept_:0.2315529360754394\n",
      "train_loss:0.26607726782871216\n",
      "val_loss:0.48767708614994304\n",
      "iter:991\n",
      "coef_:[-0.38617905  3.33721854]\n",
      "intercept_:0.23155304926937229\n",
      "train_loss:0.26607726780005897\n",
      "val_loss:0.48767736521620736\n",
      "iter:992\n",
      "coef_:[-0.38618041  3.33722111]\n",
      "intercept_:0.23155316143772603\n",
      "train_loss:0.26607726777192314\n",
      "val_loss:0.48767764175208356\n",
      "iter:993\n",
      "coef_:[-0.38618176  3.33722365]\n",
      "intercept_:0.23155327258978403\n",
      "train_loss:0.2660772677442954\n",
      "val_loss:0.4876779157805109\n",
      "iter:994\n",
      "coef_:[-0.3861831   3.33722618]\n",
      "intercept_:0.2315533827347458\n",
      "train_loss:0.2660772677171666\n",
      "val_loss:0.4876781873242207\n",
      "iter:995\n",
      "coef_:[-0.38618443  3.33722868]\n",
      "intercept_:0.23155349188172772\n",
      "train_loss:0.26607726769052764\n",
      "val_loss:0.48767845640573854\n",
      "iter:996\n",
      "coef_:[-0.38618574  3.33723116]\n",
      "intercept_:0.23155360003976383\n",
      "train_loss:0.2660772676643696\n",
      "val_loss:0.4876787230473856\n",
      "iter:997\n",
      "coef_:[-0.38618705  3.33723361]\n",
      "intercept_:0.2315537072178065\n",
      "train_loss:0.2660772676386841\n",
      "val_loss:0.48767898727128084\n",
      "iter:998\n",
      "coef_:[-0.38618834  3.33723605]\n",
      "intercept_:0.23155381342472722\n",
      "train_loss:0.2660772676134623\n",
      "val_loss:0.4876792490993428\n",
      "iter:999\n",
      "coef_:[-0.38618962  3.33723846]\n",
      "intercept_:0.23155391866931735\n",
      "train_loss:0.26607726758869593\n",
      "val_loss:0.48767950855329134\n",
      "iter:1000\n",
      "coef_:[-0.38619089  3.33724085]\n",
      "intercept_:0.2315540229602887\n",
      "train_loss:0.2660772675643768\n",
      "val_loss:0.4876797656546495\n"
     ]
    }
   ],
   "source": [
    "# Scratchの学習と推定\n",
    "slr = ScratchLogisticRegression(num_iter=1000, lr=0.3, C=1, verbose=True, random_state=0)\n",
    "slr.fit(X_train2_scl, y_train2, X_test2_scl, y_test2)\n",
    "y_slr_pred = slr.predict(X_test2_scl)\n",
    "y_slr_pred_proba = slr.predict_proba(X_test2_scl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearnの学習と推定\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train2_scl, y_train2)\n",
    "y_pred = lr.predict(X_test2_scl)\n",
    "y_pred_proba = lr.predict_proba(X_test2_scl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.38619089,  3.33724085]), 0.2315540229602887)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# スクラッチの係数、切片\n",
    "slr.coef_, slr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.92\n",
      "Precision = 0.8571428571428571\n",
      "Recall = 1.0\n",
      "F値 = 0.923076923076923\n"
     ]
    }
   ],
   "source": [
    "# スクラッチの指標値\n",
    "score_calc(y_test2, y_slr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.38631843,  3.337484  ]]), array([0.23157329]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearnの係数、切片\n",
    "lr.coef_, lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.92\n",
      "Precision = 0.8571428571428571\n",
      "Recall = 1.0\n",
      "F値 = 0.923076923076923\n"
     ]
    }
   ],
   "source": [
    "# sklearnの指標値\n",
    "score_calc(y_test2, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 特徴量に対する予測値の分布\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "features = ['sepal_length', 'petal_length']\n",
    "fig = plt.figure(figsize=(len(features) * 4, 4))\n",
    "for j in range(len(features)):\n",
    "    ax = fig.add_subplot(1, len(features), j+1)\n",
    "    ax.scatter(X_test2_scl[:, j], y_slr_pred_proba, label='Scratch', marker='+')\n",
    "    ax.scatter(X_test2_scl[:, j], y_pred_proba[:, 1], label='sklearn', marker='x')\n",
    "    ax.set_xlabel(features[j])\n",
    "    ax.set_ylabel('predict probability')\n",
    "    ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題6】学習曲線のプロット\n",
    "学習曲線を見て損失が適切に下がっているかどうか確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 損失関数のプロット\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot([i for i in range(slr.iter)], slr.loss, label='train_loss')\n",
    "plt.plot([i for i in range(slr.iter)], slr.val_loss, label='val_loss')\n",
    "plt.title('model_loss')\n",
    "plt.xlabel('iter')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題7】決定領域の可視化\n",
    "決定領域を可視化してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 決定領域の図\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "# setting\n",
    "n_class=2\n",
    "scatter_color = ['red', 'blue']\n",
    "contourf_color = ['pink', 'skyblue']\n",
    "target_names = ['virgicolor', 'virginica']\n",
    "title_names = ['train_data', 'test_data']\n",
    "X_data = X_train2_scl, X_test2_scl\n",
    "y_data = y_train2, y_test2\n",
    "fig = plt.figure(figsize=(4*n_class, 4))\n",
    "for j in range(n_class):\n",
    "    X, y = X_data[j], y_data[j]\n",
    "    # pred\n",
    "    mesh_f0, mesh_f1  = np.meshgrid(np.arange(np.min(X[:,0])-0.5, np.max(X[:,0])+0.5, 0.01),\n",
    "                                     np.arange(np.min(X[:,1])-0.5, np.max(X[:,1])+0.5, 0.01))\n",
    "    mesh = np.c_[np.ravel(mesh_f0),np.ravel(mesh_f1)]\n",
    "    mesh_pred = slr.predict(mesh).reshape(mesh_f0.shape)\n",
    "\n",
    "    # plot\n",
    "    ax = fig.add_subplot(1, 2, j+1)\n",
    "    ax.set_xlabel('sepal_length')\n",
    "    ax.set_ylabel('petal_length')\n",
    "    ax.set_title(title_names[j])\n",
    "    ax.contourf(mesh_f0, mesh_f1, mesh_pred, n_class-1, cmap=ListedColormap(contourf_color))\n",
    "    ax.contour(mesh_f0, mesh_f1, mesh_pred, n_class-1, colors='y', linewidths=3, alpha=0.5)\n",
    "    for i, target in enumerate(set(y)):\n",
    "        ax.scatter(X[y==target][:, 0], X[y==target][:, 1], s=80, color=scatter_color[i], label=target_names[i], marker='o')\n",
    "    patches = [mpatches.Patch(color=scatter_color[i], label=target_names[i]) for i in range(n_class)]\n",
    "    ax.legend(handles=patches)\n",
    "    ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題8】（アドバンス課題）重みの保存\n",
    "検証が容易になるように、学習した重みを保存および読み込みができるようにしましょう。pickleモジュールやNumPyのnp.savezを利用します。\n",
    "\n",
    "[pickle — Python オブジェクトの直列化 — Python 3.7.4 ドキュメント](https://docs.python.org/ja/3/library/pickle.html)\n",
    "\n",
    "[numpy.savez — NumPy v1.17 Manual](https://docs.scipy.org/doc/numpy/reference/generated/numpy.savez.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Sprint4_object_file', 'wb') as f:\n",
    "    pickle.dump(slr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.38619089,  3.33724085]), 0.2315540229602887)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Sprint4_object_file', 'rb') as f:\n",
    "    save_object = pickle.load(f)\n",
    "save_object.coef_, save_object.intercept_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
