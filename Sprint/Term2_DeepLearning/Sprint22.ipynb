{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 深層学習スクラッチ リカレントニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.このSprintについて\n",
    "\n",
    "### Sprintの目的\n",
    "- スクラッチを通してリカレントニューラルネットワークの基礎を理解する\n",
    "\n",
    "### どのように学ぶか\n",
    "スクラッチでリカレントニューラルネットワークの実装を行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.リカレントニューラルネットワークスクラッチ\n",
    "\n",
    "**リカレントニューラルネットワーク（RNN）** のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "フォワードプロパゲーションの実装を必須課題とし、バックプロパゲーションの実装はアドバンス課題とします。\n",
    "\n",
    "クラスの名前はScratchSimpleRNNClassifierとしてください。クラスの構造などは以前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】SimpleRNNのフォワードプロパゲーション実装\n",
    "\n",
    "SimpleRNNのクラスSimpleRNNを作成してください。基本構造はFCクラスと同じになります。\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。ndarrayのshapeがどうなるかを併記しています。\n",
    "\n",
    "バッチサイズを**batch_size**、入力の特徴量数を**n_features**、RNNのノード数を**n_nodes**として表記します。活性化関数はtanhとして進めますが、これまでのニューラルネットワーク同様にReLUなどに置き換えられます。\n",
    "\n",
    "$$\n",
    "a_t = x_{t}\\cdot W_{x} + h_{t-1}\\cdot W_{h} + B\\\\\n",
    "h_t = tanh(a_t)\n",
    "$$\n",
    "\n",
    "$a_t$ : 時刻tの活性化関数を通す前の状態 (batch_size, n_nodes)\n",
    "\n",
    "$h_t$ : 時刻tの状態・出力 (batch_size, n_nodes)\n",
    "\n",
    "$x_t$ : 時刻tの入力 (batch_size, n_features)\n",
    "\n",
    "$W_x$ : 入力に対する重み (n_features, n_nodes)\n",
    "\n",
    "$h_t-1$ : 時刻t-1の状態（前の時刻から伝わる順伝播） (batch_size, n_nodes)\n",
    "\n",
    "$W_h$ : 状態に対する重み。 (n_nodes, n_nodes)\n",
    "\n",
    "$B$ : バイアス項 (n_nodes,)\n",
    "\n",
    "初期状態 $h_0$ は全て0とすることが多いですが、任意の値を与えることも可能です。\n",
    "\n",
    "\n",
    "上記の処理を系列数n_sequences回繰り返すことになります。RNN全体への入力 \n",
    "$x$\n",
    " は(batch_size, n_sequences, n_features)のような配列で渡されることになり、そこから各時刻の配列を取り出していきます。\n",
    "\n",
    "分類問題であれば、それぞれの時刻のhに対して全結合層とソフトマックス関数（またはシグモイド関数）を使用します。タスクによっては最後の時刻のhだけを使用することもあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCell:\n",
    "    \"\"\"\n",
    "    RNNそれぞれのセル\n",
    "    Parameters\n",
    "    ----------\n",
    "    act_instance : 活性化関数のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, act_instance):\n",
    "        self.act_instance = act_instance\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        return X\n",
    "    def backward(self, dh):\n",
    "        return self.act_instance.backward(dh)\n",
    "\n",
    "class SimpleRNN:\n",
    "    \"\"\"\n",
    "    ノード数n_nodesのRNN層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_sequences : int inputのデータ数\n",
    "    n_features : int inputの特徴量数\n",
    "    n_nodes : int RNNのノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    act_instance : 活性化関数のインスタンス（縦方向）\n",
    "    cell_act_instance : cell内の活性化関数のインスタンス（横方向）\n",
    "    \"\"\"\n",
    "    def __init__(self, n_sequences, n_features, n_nodes, initializer, optimizer, act_instance, cell_act_instance):\n",
    "        self.n_sequences, self.n_features, self.n_nodes = n_sequences, n_features, n_nodes\n",
    "        #　各セルのインスタンス\n",
    "        self.cell_list = [SimpleCell(cell_act_instance) for _ in range(self.n_sequences)]\n",
    "        # 最適化手法のインスタンス\n",
    "        self.optimizer = optimizer\n",
    "        # 活性化関数のインスタンス\n",
    "        self.act_instance = act_instance\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        if initializer == None:\n",
    "            # pass\n",
    "            self.Wx = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "            self.Wh = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "            self.B = np.array([1, 1, 1, 1]) # (n_nodes,)\n",
    "        else:\n",
    "            self.Wx = initializer.W(n_features, n_nodes)\n",
    "            self.Wh = initializer.W(n_nodes, n_nodes)\n",
    "            self.B = initializer.B(n_nodes)\n",
    "            # optimazer がadagradの際に使用する\n",
    "            self.H_Wx = np.ones((n_features, n_nodes))\n",
    "            self.H_Wx = np.ones((n_nodes, n_nodes))\n",
    "            self.H_B = np.ones(n_nodes)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_sequences, n_features)\n",
    "        Returns\n",
    "        ----------\n",
    "        h_s : 次の形のndarray, shape (batch_size, n_sequences, n_nodes)\n",
    "            A : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "                出力\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.batch_size, _, _ = X.shape\n",
    "        h_s = np.zeros((self.batch_size, self.n_sequences, self.n_nodes))\n",
    "        \n",
    "        Z = np.zeros((self.batch_size, self.n_nodes))\n",
    "        for i in range(self.n_sequences):\n",
    "            # print(self.X[:, i].shape, self.Wx.shape, np.dot(self.X[:, i], self.Wx).shape,\"     \", self.Wh.shape, np.dot(self.cell_list[i].forward(Z), self.Wh).shape, \"   \", self.B.shape)\n",
    "            A = np.dot(self.X[:, i], self.Wx) + np.dot(self.cell_list[i].forward(Z), self.Wh) + self.B\n",
    "            # print(A.shape)\n",
    "            Z = self.cell_list[i].act_instance.forward(A)\n",
    "            # print(Z.shape)\n",
    "            h_s[:, i] = Z\n",
    "        \n",
    "        # 最終出力後、何らかの方法で誤差計算? 現状はゼロ\n",
    "        self.dh = 0\n",
    "        \n",
    "        return h_s\n",
    "    \n",
    "    def backward(self, dh_s):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dh_s : 次の形のndarray, shape (batch_size, n_sequences, n_nodes)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dx : 次の形のndarray, shape (batch_size, n_sequences, n_features)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # ベクトルの誤差を保管する\n",
    "        dx = np.zeros((self.batch_size, self.n_sequences, self.n_features))\n",
    "        # 各勾配を0で初期化\n",
    "        self.dB, self.dWx, self.dWh = 0, 0, 0\n",
    "        \n",
    "        for i in range(self.n_sequences)[::-1]:\n",
    "            dh_s[:, i] += self.dh\n",
    "            dA = self.cell_list[i].backward(dh_s[:, i])\n",
    "            #　前の時刻や層に流す誤差\n",
    "            self.dh = np.dot(dA, self.Wh.T)\n",
    "            dx[:, i] = np.dot(dA, self.Wx.T)\n",
    "            \n",
    "            # dB = dA 問題にシグマついていない？\n",
    "            self.dB += dA.sum(axis=0)\n",
    "            self.dWx += np.dot(self.X[:, i].T, dA)\n",
    "            self.dWh += np.dot(self.cell_list[i].X.T, dA)\n",
    "            \n",
    "        # 勾配をもとに重みを更新\n",
    "        self = self.optimizer.update_SimpleRNN(self, dA)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### class Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.001):\n",
    "        self.lr = lr\n",
    "    def update_SimpleRNN(self, layer, dA):\n",
    "        \"\"\"\n",
    "        SimpleRNN層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            rnn層の最終出力からの勾配\n",
    "        \"\"\"\n",
    "        # バイアス、重みの更新\n",
    "        layer.Wx = layer.Wx - self.lr * layer.dWx\n",
    "        layer.Wh = layer.Wh - self.lr * layer.dWh\n",
    "        layer.B = layer.B - self.lr * layer.dB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### class Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma=0.01):\n",
    "        self.sigma = sigma\n",
    "    def W(self, *parameters):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        Conv2d\n",
    "            output_channels : int 出力チャネル数\n",
    "            input_channels : int 入力チャネル数\n",
    "            filter_sizes : tuple フィルター数\n",
    "                filter_sizes_h, filter_sizes_w\n",
    "        Conv1d\n",
    "            output_channels : int 出力チャネル数\n",
    "            input_channels : int 入力チャネル数\n",
    "            filter_sizes : int フィルター数\n",
    "        Affine\n",
    "            n_nodes1 : int 前の層のノード数\n",
    "            n_nodes2 : int 後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W : shape(*parameters)\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(*parameters)\n",
    "        return W\n",
    "    def B(self, output_channels):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        Conv2d\n",
    "            output_channels : int 出力チャネル数\n",
    "        Conv1d\n",
    "            output_channels : int 出力チャネル数\n",
    "        Affine\n",
    "            n_node2 : int 後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(output_channels)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### class Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        Z = 1.0 / (1.0 + np.exp(-self.A))\n",
    "        return Z\n",
    "    def backward(self, dZ):\n",
    "        # dZとアダマール計算する値\n",
    "        tmp =  (1.0 - self.forward(self.A)) * self.forward(self.A)\n",
    "        dA = dZ * tmp\n",
    "        return dA\n",
    "\n",
    "class Tanh():\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        Z = np.tanh(self.A)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        # dZとアダマール計算する値\n",
    "        tmp = 1.0 - np.power(self.forward(self.A), 2)\n",
    "        dA = dZ * tmp\n",
    "        return dA\n",
    "\n",
    "class ReLU():\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        Z = np.maximum(self.A, 0)\n",
    "        return Z\n",
    "    def backward(self, dZ):\n",
    "        # dZとアダマール計算する値\n",
    "        tmp = np.where(self.A > 0, 1, 0)\n",
    "        dA = dZ * tmp\n",
    "        return dA\n",
    "    \n",
    "class Softmax():\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        # オーバーフローを防ぐ\n",
    "        self.A -= np.max(self.A)\n",
    "        Z = np.exp(self.A) / np.exp(self.A).sum(axis=1)[:, np.newaxis]\n",
    "        return Z\n",
    "\n",
    "    def _cross_entropy_error(self, Z, y_label_one_hot):\n",
    "        # error = -(np.log(Z) * y_label_one_hot).sum(axis=1).mean()\n",
    "        error = -(np.log(Z + 10e-10) * y_label_one_hot).sum(axis=1).mean()\n",
    "        return error\n",
    "\n",
    "    def backward(self, Z, y_label_one_hot):\n",
    "        # 交差エントロピー誤差の計算\n",
    "        loss = self._cross_entropy_error(Z, y_label_one_hot)\n",
    "        # dA　の計算\n",
    "        dA = Z - y_label_one_hot\n",
    "        # dA, loss\n",
    "        return dA, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】小さな配列でのフォワードプロパゲーションの実験\n",
    "小さな配列でフォワードプロパゲーションを考えてみます。\n",
    "\n",
    "入力x、初期状態h、重みw_xとw_h、バイアスbを次のようにします。\n",
    "\n",
    "ここで配列xの軸はバッチサイズ、系列数、特徴量数の順番です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "b = np.array([1, 1, 1, 1]) # (n_nodes,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "フォワードプロパゲーションの出力が次のようになることを作成したコードで確認してください。\n",
    "```\n",
    "h = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]]) # (batch_size, n_nodes)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.76188798, 0.76213958, 0.76239095, 0.76255841],\n",
       "        [0.792209  , 0.8141834 , 0.83404912, 0.84977719],\n",
       "        [0.79494228, 0.81839002, 0.83939649, 0.85584174]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = SimpleRNN(n_sequences, n_features, n_nodes, \n",
    "                initializer=None, optimizer=SGD(), act_instance=ReLU(), cell_act_instance=Tanh())\n",
    "rnn.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】（アドバンス課題）バックプロパゲーションの実装\n",
    "バックプロパゲーションを実装してください。\n",
    "\n",
    "RNNの内部は全結合層を組み合わせた形になっているので、更新式は全結合層などと同様です。\n",
    "\n",
    "$$\n",
    "W_x^{\\prime} = W_x - \\alpha \\frac{\\partial L}{\\partial W_x} \\\\\n",
    "W_h^{\\prime} = W_h - \\alpha \\frac{\\partial L}{\\partial W_h} \\\\\n",
    "B^{\\prime} = B - \\alpha \\frac{\\partial L}{\\partial B}\n",
    "$$\n",
    "\n",
    "$\\alpha$ : 学習率\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_x}$\n",
    " : \n",
    "$W_x$ に関する損失 $L$ の勾配\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_h}$\n",
    ": \n",
    "$W_h$ に関する損失 $L$ の勾配\n",
    "\n",
    "$\\frac{\\partial L}{\\partial B}$\n",
    ": \n",
    "$B$ に関する損失 $L$ の勾配\n",
    "\n",
    "勾配を求めるためのバックプロパゲーションの数式が以下です。\n",
    "\n",
    "$\\frac{\\partial h_t}{\\partial a_t} = \\frac{\\partial L}{\\partial h_t} ×(1-tanh^2(a_t))$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial B} = \\frac{\\partial h_t}{\\partial a_t}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_x} = x_{t}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_h} = h_{t-1}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}$\n",
    "\n",
    "＊\n",
    "$\\frac{\\partial L}{\\partial h_t}$\n",
    " は前の時刻からの状態の誤差と出力の誤差の合計です。hは順伝播時に出力と次の層に伝わる状態双方に使われているからです。\n",
    " \n",
    " 前の時刻や層に流す誤差の数式は以下です。\n",
    " \n",
    " $\n",
    " \\frac{\\partial L}{\\partial h_{t-1}} = \\frac{\\partial h_t}{\\partial a_t}\\cdot W_{h}^{T}\n",
    " $\n",
    " \n",
    " $\n",
    " \\frac{\\partial L}{\\partial x_{t}} = \\frac{\\partial h_t}{\\partial a_t}\\cdot W_{x}^{T}\n",
    " $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0.03889262, 0.05730395],\n",
       "         [0.04203426, 0.06159598],\n",
       "         [0.03946   , 0.05796616]]]),\n",
       " (1, 3, 2))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dX = rnn.backward(rnn.forward(x))\n",
    "dX, dX.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_anaconda3-2020.02)",
   "language": "python",
   "name": "conda_anaconda3-2020.02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
