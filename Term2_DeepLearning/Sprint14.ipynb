{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint ディープラーニングフレームワーク2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.このSprintについて\n",
    "\n",
    "### Sprintの目的\n",
    "- フレームワークのコードを読めるようにする\n",
    "- フレームワークを習得し続けられるようになる\n",
    "- 理論を知っている範囲をフレームワークで動かす\n",
    "\n",
    "### どのように学ぶか\n",
    "前半はTensorFlowのExampleを動かします。後半ではKerasのコードを書いていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.公式Example\n",
    "\n",
    "深層学習フレームワークには公式に様々なモデルのExampleコードが公開されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】公式チュートリアルモデルを分担して実行\n",
    "TensorFLowの公式チュートリアルモデルを分担して実行してください。\n",
    "\n",
    "以下の中から1人ひとつ選び実行し、その結果を簡単に発表してください。\n",
    "\n",
    "[models/tutorials at master · tensorflow/models](https://www.tensorflow.org/tutorials/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 実行したファイル\n",
    "[image segmentation](https://colab.research.google.com/drive/1ko1JaJFFoO64dgwML3hWOWvWpveQnyPt#scrollTo=cZCM65CBt1CJ)\n",
    "\n",
    "- ベースとなっているモデル　:  U-net [参考](https://qiita.com/hiro871_/items/871c76bf65b76ebe1dd0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題2】（アドバンス課題）様々な手法を実行\n",
    "TensorFLowやGoogle AI ResearchのGitHubリポジトリには、定番のモデルから最新のモデルまで多様なコードが公開されています。これらから興味あるものを選び実行してください。\n",
    "\n",
    "なお、これらのコードは初学者向けではないため、巨大なデータセットのダウンロードが必要な場合など、実行が簡単ではないこともあります。そういった場合は、コードリーディングを行ってください。\n",
    "\n",
    "[models/research at master · tensorflow/models](https://github.com/tensorflow/models/tree/master/research)\n",
    "\n",
    "[google-research/google-research: Google AI Research](https://github.com/google-research/google-research)\n",
    "\n",
    "更新日が古いものはPythonやTensorFlowのバージョンが古く、扱いずらい場合があります。新しいものから見ることを推奨します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.異なるフレームワークへの書き換え\n",
    "\n",
    "「ディープラーニングフレームワーク1」で作成した4種類のデータセットを扱うTensorFLowのコードを異なるフレームワークに変更していきます。\n",
    "\n",
    "- Iris（Iris-versicolorとIris-virginicaのみの2値分類）\n",
    "- Iris（3種類全ての目的変数を使用して多値分類）\n",
    "- House Prices\n",
    "- MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kerasへの書き換え\n",
    "KerasはTensorFLowに含まれるtf.kerasモジュールを使用してください。\n",
    "\n",
    "KerasにはSequentialモデルかFunctional APIかなど書き方に種類がありますが、これは指定しません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】Iris（2値分類）をKerasで学習\n",
    "TensorFlowによるIrisデータセットに対する2値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# データセットの読み込み\n",
    "dataset_path =\"/Users/ikeda/Desktop/dive/diveintocode-ml/Downlowd_data/datasets_19_420_Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units=n_hidden1, activation='relu', input_shape=(n_input, )))\n",
    "model.add(tf.keras.layers.Dense(units=n_hidden2, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=n_classes, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64 samples, validate on 16 samples\n",
      "Epoch 1/10\n",
      "64/64 - 1s - loss: 0.7791 - accuracy: 0.5312 - val_loss: 0.7333 - val_accuracy: 0.3750\n",
      "Epoch 2/10\n",
      "64/64 - 0s - loss: 0.6872 - accuracy: 0.5156 - val_loss: 0.6903 - val_accuracy: 0.3750\n",
      "Epoch 3/10\n",
      "64/64 - 0s - loss: 0.6396 - accuracy: 0.5625 - val_loss: 0.6053 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "64/64 - 0s - loss: 0.5962 - accuracy: 0.8438 - val_loss: 0.6334 - val_accuracy: 0.4375\n",
      "Epoch 5/10\n",
      "64/64 - 0s - loss: 0.6566 - accuracy: 0.5781 - val_loss: 0.5056 - val_accuracy: 0.7500\n",
      "Epoch 6/10\n",
      "64/64 - 0s - loss: 0.7548 - accuracy: 0.5000 - val_loss: 0.5520 - val_accuracy: 0.8125\n",
      "Epoch 7/10\n",
      "64/64 - 0s - loss: 0.5387 - accuracy: 0.6406 - val_loss: 0.5712 - val_accuracy: 0.5625\n",
      "Epoch 8/10\n",
      "64/64 - 0s - loss: 0.4737 - accuracy: 0.8594 - val_loss: 0.4272 - val_accuracy: 0.9375\n",
      "Epoch 9/10\n",
      "64/64 - 0s - loss: 0.4164 - accuracy: 0.9062 - val_loss: 0.4566 - val_accuracy: 0.8125\n",
      "Epoch 10/10\n",
      "64/64 - 0s - loss: 0.3132 - accuracy: 0.9375 - val_loss: 0.2662 - val_accuracy: 0.8750\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=2,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】Iris（多値分類）をKerasで学習\n",
    "TensorFlowによるIrisデータセットに対する3値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの読み込み\n",
    "dataset_path =\"/Users/ikeda/Desktop/dive/diveintocode-ml/Downlowd_data/datasets_19_420_Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-setosa'] = 0\n",
    "y[y=='Iris-versicolor'] = 1\n",
    "y[y=='Iris-virginica'] = 2\n",
    "# one_hotへ変換\n",
    "y = y.astype(np.int)\n",
    "y = np.identity(3)[y]\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units=n_hidden1, activation='relu', input_shape=(n_input, )))\n",
    "model.add(tf.keras.layers.Dense(units=n_hidden2, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=n_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96 samples, validate on 24 samples\n",
      "Epoch 1/10\n",
      "96/96 - 1s - loss: 1.3029 - categorical_accuracy: 0.4062 - val_loss: 0.8320 - val_categorical_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "96/96 - 0s - loss: 0.7323 - categorical_accuracy: 0.6146 - val_loss: 0.5309 - val_categorical_accuracy: 0.7083\n",
      "Epoch 3/10\n",
      "96/96 - 0s - loss: 0.4969 - categorical_accuracy: 0.7083 - val_loss: 0.4656 - val_categorical_accuracy: 0.7083\n",
      "Epoch 4/10\n",
      "96/96 - 0s - loss: 0.4558 - categorical_accuracy: 0.7083 - val_loss: 0.4520 - val_categorical_accuracy: 0.7083\n",
      "Epoch 5/10\n",
      "96/96 - 0s - loss: 0.3514 - categorical_accuracy: 0.8542 - val_loss: 0.3457 - val_categorical_accuracy: 0.8750\n",
      "Epoch 6/10\n",
      "96/96 - 0s - loss: 0.2507 - categorical_accuracy: 0.9375 - val_loss: 0.2953 - val_categorical_accuracy: 0.9167\n",
      "Epoch 7/10\n",
      "96/96 - 0s - loss: 0.2029 - categorical_accuracy: 0.9479 - val_loss: 0.3144 - val_categorical_accuracy: 0.8333\n",
      "Epoch 8/10\n",
      "96/96 - 0s - loss: 0.1955 - categorical_accuracy: 0.9271 - val_loss: 0.2386 - val_categorical_accuracy: 0.8750\n",
      "Epoch 9/10\n",
      "96/96 - 0s - loss: 0.1681 - categorical_accuracy: 0.9375 - val_loss: 0.4285 - val_categorical_accuracy: 0.7500\n",
      "Epoch 10/10\n",
      "96/96 - 0s - loss: 0.1080 - categorical_accuracy: 0.9583 - val_loss: 0.2580 - val_categorical_accuracy: 0.9167\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=2,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】House PricesをKerasで学習\n",
    "TensorFlowによるHouse Pricesデータセットに対する回帰をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの読み込み\n",
    "dataset_path =\"/Users/ikeda/Desktop/dive/diveintocode-ml/Downlowd_data/train.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "objective_variable = 'SalePrice'\n",
    "features = ['GrLivArea', 'YearBuilt']\n",
    "\n",
    "y = df[objective_variable]\n",
    "X = df.loc[:, features]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# Xを標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std = StandardScaler()\n",
    "X_train = std.fit_transform(X_train)\n",
    "X_val = std.transform(X_val)\n",
    "X_test = std.transform(X_test)\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train\n",
    "n_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 決定係数\n",
    "def det_coeff(y_true, y_pred):\n",
    "    u = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    v = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    return tf.ones_like(v) - (u / v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units=n_hidden1, activation='relu', input_shape=(n_input, )))\n",
    "model.add(tf.keras.layers.Dense(units=n_hidden2, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=n_classes, activation=None))\n",
    "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), metrics=[det_coeff])\n",
    "# metrics=['root_mean_squared_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 934 samples, validate on 234 samples\n",
      "Epoch 1/10\n",
      "934/934 - 1s - loss: 38721423605.5846 - det_coeff: -9.8510e+00 - val_loss: 34478913623.5214 - val_det_coeff: -1.3195e+01\n",
      "Epoch 2/10\n",
      "934/934 - 0s - loss: 27139086219.7859 - det_coeff: -6.4996e+00 - val_loss: 12068350704.6838 - val_det_coeff: -3.9466e+00\n",
      "Epoch 3/10\n",
      "934/934 - 0s - loss: 5580760591.5546 - det_coeff: -2.4556e-01 - val_loss: 1618795526.2906 - val_det_coeff: 0.0918\n",
      "Epoch 4/10\n",
      "934/934 - 0s - loss: 2135357513.2505 - det_coeff: 0.5495 - val_loss: 1509555307.7607 - val_det_coeff: 0.1375\n",
      "Epoch 5/10\n",
      "934/934 - 0s - loss: 2073594045.0535 - det_coeff: 0.5398 - val_loss: 1537164481.6410 - val_det_coeff: 0.1008\n",
      "Epoch 6/10\n",
      "934/934 - 0s - loss: 2062457132.2998 - det_coeff: 0.5299 - val_loss: 1493145544.2051 - val_det_coeff: 0.1582\n",
      "Epoch 7/10\n",
      "934/934 - 0s - loss: 2053844001.9529 - det_coeff: 0.5780 - val_loss: 1491126389.6068 - val_det_coeff: 0.1594\n",
      "Epoch 8/10\n",
      "934/934 - 0s - loss: 2026991208.9422 - det_coeff: 0.5461 - val_loss: 1472986889.2991 - val_det_coeff: 0.1675\n",
      "Epoch 9/10\n",
      "934/934 - 0s - loss: 2024286109.9443 - det_coeff: 0.5803 - val_loss: 1463771148.8547 - val_det_coeff: 0.1906\n",
      "Epoch 10/10\n",
      "934/934 - 0s - loss: 2018616816.2398 - det_coeff: 0.5626 - val_loss: 1518172714.6667 - val_det_coeff: 0.1311\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=2,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題6】MNISTをKerasで学習\n",
    "TensorFlowによるMNISTデータセットによる画像の多値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの読み込み\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# 平滑化\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "# 前処理\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# one_hotへ変換\n",
    "y_test = np.identity(10)[y_test]\n",
    "y_train = np.identity(10)[y_train]\n",
    "\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 20\n",
    "num_epochs = 20\n",
    "n_hidden1 = 400\n",
    "n_hidden2 = 200\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units=n_hidden1, activation='relu', input_shape=(n_input, )))\n",
    "model.add(tf.keras.layers.Dense(units=n_hidden2, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=n_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 - 16s - loss: 0.3375 - categorical_accuracy: 0.9080 - val_loss: 0.2575 - val_categorical_accuracy: 0.9335\n",
      "Epoch 2/20\n",
      "48000/48000 - 15s - loss: 0.2241 - categorical_accuracy: 0.9415 - val_loss: 0.2034 - val_categorical_accuracy: 0.9496\n",
      "Epoch 3/20\n",
      "48000/48000 - 15s - loss: 0.1915 - categorical_accuracy: 0.9515 - val_loss: 0.2668 - val_categorical_accuracy: 0.9520\n",
      "Epoch 4/20\n",
      "48000/48000 - 15s - loss: 0.1805 - categorical_accuracy: 0.9561 - val_loss: 0.1835 - val_categorical_accuracy: 0.9567\n",
      "Epoch 5/20\n",
      "48000/48000 - 14s - loss: 0.1623 - categorical_accuracy: 0.9597 - val_loss: 0.1595 - val_categorical_accuracy: 0.9642\n",
      "Epoch 6/20\n",
      "48000/48000 - 14s - loss: 0.1518 - categorical_accuracy: 0.9640 - val_loss: 0.1804 - val_categorical_accuracy: 0.9609\n",
      "Epoch 7/20\n",
      "48000/48000 - 15s - loss: 0.1593 - categorical_accuracy: 0.9635 - val_loss: 0.1870 - val_categorical_accuracy: 0.9640\n",
      "Epoch 8/20\n",
      "48000/48000 - 14s - loss: 0.1511 - categorical_accuracy: 0.9656 - val_loss: 0.2310 - val_categorical_accuracy: 0.9538\n",
      "Epoch 9/20\n",
      "48000/48000 - 14s - loss: 0.1482 - categorical_accuracy: 0.9675 - val_loss: 0.2526 - val_categorical_accuracy: 0.9575\n",
      "Epoch 10/20\n",
      "48000/48000 - 14s - loss: 0.1414 - categorical_accuracy: 0.9684 - val_loss: 0.2644 - val_categorical_accuracy: 0.9601\n",
      "Epoch 11/20\n",
      "48000/48000 - 14s - loss: 0.1392 - categorical_accuracy: 0.9686 - val_loss: 0.1993 - val_categorical_accuracy: 0.9631\n",
      "Epoch 12/20\n",
      "48000/48000 - 14s - loss: 0.1325 - categorical_accuracy: 0.9700 - val_loss: 0.1993 - val_categorical_accuracy: 0.9646\n",
      "Epoch 13/20\n",
      "48000/48000 - 14s - loss: 0.1489 - categorical_accuracy: 0.9676 - val_loss: 0.2786 - val_categorical_accuracy: 0.9460\n",
      "Epoch 14/20\n",
      "48000/48000 - 14s - loss: 0.1340 - categorical_accuracy: 0.9694 - val_loss: 0.3045 - val_categorical_accuracy: 0.9515\n",
      "Epoch 15/20\n",
      "48000/48000 - 14s - loss: 0.1421 - categorical_accuracy: 0.9688 - val_loss: 0.2709 - val_categorical_accuracy: 0.9573\n",
      "Epoch 16/20\n",
      "48000/48000 - 14s - loss: 0.1568 - categorical_accuracy: 0.9671 - val_loss: 0.2683 - val_categorical_accuracy: 0.9559\n",
      "Epoch 17/20\n",
      "48000/48000 - 14s - loss: 0.1422 - categorical_accuracy: 0.9674 - val_loss: 0.2697 - val_categorical_accuracy: 0.9519\n",
      "Epoch 18/20\n",
      "48000/48000 - 14s - loss: 0.1507 - categorical_accuracy: 0.9670 - val_loss: 0.2792 - val_categorical_accuracy: 0.9544\n",
      "Epoch 19/20\n",
      "48000/48000 - 14s - loss: 0.1465 - categorical_accuracy: 0.9674 - val_loss: 0.3384 - val_categorical_accuracy: 0.9572\n",
      "Epoch 20/20\n",
      "48000/48000 - 14s - loss: 0.1488 - categorical_accuracy: 0.9687 - val_loss: 0.3609 - val_categorical_accuracy: 0.9448\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=2,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題7】（アドバンス課題）PyTorchへの書き換え\n",
    "4種類の問題をPyTorchに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iris（2値分類）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# データセットの読み込み\n",
    "dataset_path =\"/Users/ikeda/Desktop/dive/diveintocode-ml/Downlowd_data/datasets_19_420_Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "# y = y.astype(np.int)[:, np.newaxis]\n",
    "y = y.astype(np.int)\n",
    "# one_hotへ変換\n",
    "# y = y.astype(np.int)\n",
    "# y = np.identity(2)[y]\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形を直してバッチ情報を取得\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_val = torch.from_numpy(X_val).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "y_val = torch.from_numpy(y_val).long()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "ds_train = TensorDataset(X_train, y_train)\n",
    "loader_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=4, out_features=50, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルを構築\n",
    "model = nn.Sequential()\n",
    "model.add_module('fc1', nn.Linear(n_input, n_hidden1))\n",
    "model.add_module('relu1', nn.ReLU())\n",
    "model.add_module('fc2', nn.Linear(n_hidden1, n_hidden2))\n",
    "model.add_module('relu2', nn.ReLU())\n",
    "model.add_module('fc3', nn.Linear(n_hidden2, n_classes))\n",
    "# model.add_module('softmax', nn.Softmax(dim=1))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化手法のパラメータ設定\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "# loss関数の定義\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, total_loss : 0.5664, val_loss : 0.8181, total_acc : 0.438, val_acc : 0.375\n",
      "Epoch 2, total_loss : 0.4783, val_loss : 0.7012, total_acc : 0.531, val_acc : 0.375\n",
      "Epoch 3, total_loss : 0.4781, val_loss : 0.7411, total_acc : 0.516, val_acc : 0.375\n",
      "Epoch 4, total_loss : 0.4679, val_loss : 0.6536, total_acc : 0.531, val_acc : 0.625\n",
      "Epoch 5, total_loss : 0.4617, val_loss : 0.6653, total_acc : 0.516, val_acc : 0.625\n",
      "Epoch 6, total_loss : 0.4714, val_loss : 0.6419, total_acc : 0.484, val_acc : 0.625\n",
      "Epoch 7, total_loss : 0.4733, val_loss : 0.7789, total_acc : 0.672, val_acc : 0.375\n",
      "Epoch 8, total_loss : 0.4525, val_loss : 0.7574, total_acc : 0.625, val_acc : 0.375\n",
      "Epoch 9, total_loss : 0.4520, val_loss : 0.6476, total_acc : 0.531, val_acc : 0.625\n",
      "Epoch 10, total_loss : 0.4479, val_loss : 0.6579, total_acc : 0.625, val_acc : 0.375\n"
     ]
    }
   ],
   "source": [
    "# バッチ正規化等、学習時と推論時で振る舞いの違うモジュールの振る舞いを学習時の振る舞いに\n",
    "# model.eval()で推論時の振る舞いに変更可能\n",
    "model.train()\n",
    "# 学習ループ\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    total_acc = 0\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    # ミニバッチ毎ににループ\n",
    "    for mini_batch_x, mini_batch_y in loader_train:    \n",
    "        # 勾配の初期化\n",
    "        optimizer.zero_grad()\n",
    "        # 順伝播\n",
    "        out = model(mini_batch_x)\n",
    "        # 推論する\n",
    "        pred = out.data.max(1, keepdim=True)[1] # 出力ラベルを求める\n",
    "        correct = pred.eq(mini_batch_y.data.view_as(pred)).sum()  # 正解と一緒だったらカウントアップ\n",
    "        # print(pred, correct)\n",
    "        total_correct += correct\n",
    "        # print(total_correct)\n",
    "        # ロスの計算\n",
    "        loss = criterion(out, mini_batch_y)\n",
    "        total_loss += loss\n",
    "        # print(total_loss)\n",
    "        # 勾配の計算\n",
    "        loss.backward()\n",
    "        # パラメータの更新\n",
    "        optimizer.step()\n",
    "    total_loss /= batch_size\n",
    "    # print(total_loss)\n",
    "    total_acc = total_correct.numpy() / n_samples\n",
    "    # print(total_acc)\n",
    "    val_out = model(X_val)\n",
    "    val_pred = val_out.data.max(1, keepdim=True)[1] # 出力ラベルを求める\n",
    "    val_correct = val_pred.eq(y_val.data.view_as(val_pred)).sum()  # 正解と一緒だったらカウントアップ\n",
    "    val_acc = val_correct.numpy() / len(y_val.data)\n",
    "    \n",
    "    val_loss = criterion(val_out, y_val)\n",
    "    print(\"Epoch {}, total_loss : {:.4f}, val_loss : {:.4f}, total_acc : {:.3f}, val_acc : {:.3f}\".format(epoch, total_loss, val_loss, total_acc, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iris（多値分類）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの読み込み\n",
    "dataset_path =\"/Users/ikeda/Desktop/dive/diveintocode-ml/Downlowd_data/datasets_19_420_Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-setosa'] = 0\n",
    "y[y=='Iris-versicolor'] = 1\n",
    "y[y=='Iris-virginica'] = 2\n",
    "# one_hotへ変換\n",
    "y = y.astype(np.int)\n",
    "y = np.identity(3)[y]\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形を直してバッチ情報を取得\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_val = torch.from_numpy(X_val).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "y_val = torch.from_numpy(y_val).long()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "ds_train = TensorDataset(X_train, y_train)\n",
    "loader_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=4, out_features=50, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=100, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルを構築\n",
    "model = nn.Sequential()\n",
    "model.add_module('fc1', nn.Linear(n_input, n_hidden1))\n",
    "model.add_module('relu1', nn.ReLU())\n",
    "model.add_module('fc2', nn.Linear(n_hidden1, n_hidden2))\n",
    "model.add_module('relu2', nn.ReLU())\n",
    "model.add_module('fc3', nn.Linear(n_hidden2, n_classes))\n",
    "# model.add_module('softmax', nn.Softmax(dim=1))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化手法のパラメータ設定\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "# loss関数の定義\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[10, 1]' is invalid for input of size 30",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-388-9414efacf7d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# 推論する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 出力ラベルを求める\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 正解と一緒だったらカウントアップ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m# print(pred, correct)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtotal_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[10, 1]' is invalid for input of size 30"
     ]
    }
   ],
   "source": [
    "# バッチ正規化等、学習時と推論時で振る舞いの違うモジュールの振る舞いを学習時の振る舞いに\n",
    "# model.eval()で推論時の振る舞いに変更可能\n",
    "model.train()\n",
    "# 学習ループ\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    total_acc = 0\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    # ミニバッチ毎ににループ\n",
    "    for mini_batch_x, mini_batch_y in loader_train:    \n",
    "        # 勾配の初期化\n",
    "        optimizer.zero_grad()\n",
    "        # 順伝播\n",
    "        out = model(mini_batch_x)\n",
    "        # 推論する\n",
    "        pred = out.data.max(1, keepdim=True)[1] # 出力ラベルを求める\n",
    "        correct = pred.eq(mini_batch_y.data.view_as(pred)).sum()  # 正解と一緒だったらカウントアップ\n",
    "        # print(pred, correct)\n",
    "        total_correct += correct\n",
    "        # print(total_correct)\n",
    "        # ロスの計算\n",
    "        loss = criterion(out, mini_batch_y)\n",
    "        total_loss += loss\n",
    "        # print(total_loss)\n",
    "        # 勾配の計算\n",
    "        loss.backward()\n",
    "        # パラメータの更新\n",
    "        optimizer.step()\n",
    "    total_loss /= batch_size\n",
    "    # print(total_loss)\n",
    "    total_acc = total_correct.numpy() / n_samples\n",
    "    # print(total_acc)\n",
    "    val_out = model(X_val)\n",
    "    val_pred = val_out.data.max(1, keepdim=True)[1] # 出力ラベルを求める\n",
    "    val_correct = val_pred.eq(y_val.data.view_as(val_pred)).sum()  # 正解と一緒だったらカウントアップ\n",
    "    val_acc = val_correct.numpy() / len(y_val.data)\n",
    "    \n",
    "    val_loss = criterion(val_out, y_val)\n",
    "    print(\"Epoch {}, total_loss : {:.4f}, val_loss : {:.4f}, total_acc : {:.3f}, val_acc : {:.3f}\".format(epoch, total_loss, val_loss, total_acc, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題8】（アドバンス課題）フレームワークの比較\n",
    "それぞれのフレームワークにはどのような違いがあるかをまとめてください。\n",
    "\n",
    "*《視点例》*\n",
    "\n",
    "- 計算速度\n",
    "- コードの行数・可読性\n",
    "- 用意されている機能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ThensorFlow\n",
    "    - ハイレベルな機能を実装可能で、計算をデータフローやグラフで表すことができまるため、実践で、複雑な問題に対処できる\n",
    "    - Googleの音声検索や言語翻訳、画像検索に使用\n",
    "-　Keras\n",
    "    - 比較的短いコードで実装可能、最新手法を素早く試すことができる\n",
    "    - CPUとGPU上でシームレスな動作\n",
    "- Chainer\n",
    "    - 記法がシンプルで国産のフレームワークのため、学習が容易\n",
    "    - 画像分類や物体検出などで利用されている\n",
    "    - 2019年12月に開発を終了し、PyTorchへ移行\n",
    "- PyTorch\n",
    "    - Chainer, Numpyと似たような構文で操作可能\n",
    "    - 計算速度も早く、ソースコードが扱いやすい\n",
    "    - 計算グラフを動的で構築可能\n",
    "- MXNet\n",
    "    - 命令的プラグラムと宣言的プログラムを併用することができ、柔軟なフレームワーク\n",
    "    - CNN、LSTM、RCNN、Deep Q Networkなど様々な深層学習モデルをサポートしており、画像認識、自然言語処理、レコメンデーションなど様々な場面で使われている\n",
    "- Deeplearning4j（DL4j）\n",
    "    - Javaで開発されているため、JVM上で動くという特徴があり、既存の情報システムと組み合わせ運用できます。そのため、商用に使われやすく、サポートが提供されているということが利点\n",
    "    - Javaでディープラーニングを行う際に利用されており、例えば、金融分野の不正検知や異常検知、電子商取引や広告のレコメンドシステム、 製造業の不良品検知や画像認識などで使用されている\n",
    "- Microsoft Cognitive Toolkit\n",
    "    - 巨大データセット処理時のパフォーマンス低下を最小化するためのアルゴリズムが組み込まれているため、複数マシンで巨大データセットを扱う場合において、他のツールキットに対する優位性があるとされている\n",
    "    - Skypeのリアルタイム翻訳、国内では三井住友銀行が自動応答システムで利用している\n",
    "- PaddlePaddle\n",
    "    - クラウドだけでなく、分散コンピューティングのクラスタで高速に稼働する\n",
    "    - 中国で強い\n",
    "- Caffe\n",
    "    - C++で実装され、GPUに対応しているため、高速な計算処理が可能\n",
    "    - 開発コミュニティーが活発にGitHubを更新していたり、サンプルコードも多く提供されているため、初心者にもおすすめ\n",
    "    - 大規模画像認識のコンテスト「ILSVRC」で2012年に首位を獲得した「畳み込みニューラルネットワークの画像分類モデル」があり、直ぐに利用できるというのも利点"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_anaconda3-2020.02)",
   "language": "python",
   "name": "conda_anaconda3-2020.02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
