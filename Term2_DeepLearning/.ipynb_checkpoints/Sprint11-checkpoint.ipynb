{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 深層学習スクラッチ 畳み込みニューラルネットワーク1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.このSprintについて\n",
    "### Sprintの目的\n",
    "- スクラッチを通してCNNの基礎を理解する\n",
    "\n",
    "### どのように学ぶか\n",
    "スクラッチで1次元用畳み込みニューラルネットワークを実装した後、学習と検証を行なっていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1次元の畳み込みニューラルネットワークスクラッチ\n",
    "\n",
    "**畳み込みニューラルネットワーク（CNN）** のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "このSprintでは1次元の **畳み込み層** を作成し、畳み込みの基礎を理解することを目指します。次のSprintでは2次元畳み込み層とプーリング層を作成することで、一般的に画像に対して利用されるCNNを完成させます。\n",
    "\n",
    "クラスの名前はScratch1dCNNClassifierとしてください。クラスの構造などは前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1次元畳み込み層とは\n",
    "CNNでは画像に対しての2次元畳み込み層が定番ですが、ここでは理解しやすくするためにまずは1次元畳み込み層を実装します。1次元畳み込みは実用上は自然言語や波形データなどの 系列データ で使われることが多いです。\n",
    "\n",
    "畳み込みは任意の次元に対して考えることができ、立体データに対しての3次元畳み込みまではフレームワークで一般的に用意されています。\n",
    "\n",
    "### データセットの用意\n",
    "検証には引き続きMNISTデータセットを使用します。1次元畳み込みでは全結合のニューラルネットワークと同様に平滑化されたものを入力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# MNISTデータ　ロード\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# 平滑化\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "# 前処理\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.max()) # 1.0\n",
    "print(X_train.min()) # 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成\n",
    "チャンネル数を1に限定した1次元畳み込み層のクラスSimpleConv1dを作成してください。基本構造は前のSprintで作成した全結合層のFCクラスと同じになります。なお、重みの初期化に関するクラスは必要に応じて作り変えてください。Xavierの初期値などを使う点は全結合層と同様です。\n",
    "\n",
    "ここでは **パディング** は考えず、**ストライド** も1に固定します。また、複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応してください。この部分の拡張はアドバンス課題とします。\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。\n",
    "$$\n",
    "a_i = \\sum_{s=0}^{F-1}x_{(i+s)}w_s+b\n",
    "$$\n",
    "\n",
    "$a_i$  : 出力される配列のi番目の値\n",
    "\n",
    "$F$ : フィルタのサイズ\n",
    "\n",
    "$x_(i+s)$ : 入力の配列の(i+s)番目の値\n",
    "\n",
    "$w_s$ : 重みの配列のs番目の値\n",
    "\n",
    "$b$ : バイアス項\n",
    "\n",
    "全てスカラーです。\n",
    "\n",
    "次に更新式です。ここがAdaGradなどに置き換えられる点は全結合層と同様です。\n",
    "\n",
    "$\\alpha$ : 学習率\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_s}$ : $w_s$ に関する損失 $L$ の勾配\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b}$ : $b$ に関する損失 $L$ の勾配\n",
    "\n",
    "勾配 $\\frac{\\partial L}{\\partial w_s}$ や $\\frac{\\partial L}{\\partial b}$ を求めるためのバックプロパゲーションの数式が以下です。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_s} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}x_{(i+s)}\\\\\n",
    "\\frac{\\partial L}{\\partial b} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial a_i}$ : 勾配の配列のi番目の値\n",
    "\n",
    "$N_{out}$ : 出力のサイズ\n",
    "\n",
    "前の層に流す誤差の数式は以下です。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_j} = \\sum_{s=0}^{F-1} \\frac{\\partial L}{\\partial a_{(j-s)}}w_s\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_j}$ : 前の層に流す誤差の配列のj番目の値\n",
    "\n",
    "ただし、 $j-s<0$ または $j-s>N_{out}-1$ のとき $\\frac{\\partial L}{\\partial a_{(j-s)}} =0$ です。\n",
    "\n",
    "全結合層との大きな違いは、重みが複数の特徴量に対して共有されていることです。この場合は共有されている分の誤差を全て足すことで勾配を求めます。計算グラフ上での分岐はバックプロパゲーションの際に誤差の足し算をすれば良いことになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d:\n",
    "    \"\"\"\n",
    "    チャンネル数を1に限定した1次元畳み込み層\n",
    "    Parameters\n",
    "    ----------\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    input_channels : int\n",
    "      入力のチャンネル数\n",
    "    filter _sizes : int\n",
    "      フィルタのサイズ\n",
    "    stride_sizes : int\n",
    "      スライドのサイズ\n",
    "    output_channels : int\n",
    "      出力のチャンネル数\n",
    "    \"\"\"\n",
    "    # def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "    def __init__(self, input_channels, filter_sizes, stride_sizes, initializer, optimizer):\n",
    "        self.input_channels = input_channels\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.stride_sizes = stride_sizes\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        # self.W = initializer.W(input_channels, output_channels, strides)\n",
    "        # self.B = initializer.B(output_channels)\n",
    "        if initializer is None:\n",
    "            self.W = np.array([3, 5, 7])\n",
    "            self.B = np.array([1])\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (input_channels, n_features)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (output_channels, filter_sizes-1)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        # A = np.dot(X, self.W) + self.B\n",
    "        if self.stride_sizes == 1:\n",
    "            A = np.convolve(X, self.W[::-1], mode='valid') + self.B\n",
    "        return A\n",
    "    def backward(self, dA, X):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            一層前の活性化関数の出力結果\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # dZを計算\n",
    "        # dZ = np.dot(dA, self.W.T)\n",
    "        \n",
    "        # 更新\n",
    "        #  self = self.optimizer.update(self, dA, Z)\n",
    "        # return dZ\n",
    "        self = self.optimizer.update(self, dA, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- optimizer_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.1):\n",
    "        self.lr = lr\n",
    "    def update(self, layer, dA, X):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        dA : 後ろから流れてきた勾配\n",
    "        \"\"\"\n",
    "        if len(dA.shape) == 1:\n",
    "            # 各勾配の計算\n",
    "            # dB = dA.sum(axis=0)\n",
    "            dB = np.convolve(dA, np.array([1]*dA.shape[0]), mode='valid')\n",
    "            print(dB.shape, dB)\n",
    "            \n",
    "            dW = np.convolve(X, dA[::-1], mode='valid')\n",
    "            print(dW.shape, dW)\n",
    "            self.dW = dW\n",
    "            \n",
    "            dX = np.zeros(X.shape[0])\n",
    "            diff = X.shape[0] - dA.shape[0]\n",
    "            if diff > 0:\n",
    "                dA_trans = np.concatenate([np.array([0]*diff), dA, np.array([0]*diff)], axis=0)\n",
    "                # 重みの数\n",
    "                w_num = layer.W.shape[0]\n",
    "#                 print(diff)\n",
    "#                 print(w_num)\n",
    "#                 print(dA_trans)\n",
    "                for i in range(X.shape[0]):\n",
    "                    dX[i] = np.dot(dA_trans[i: w_num+i], layer.W[::-1])\n",
    "                print(dX.shape, dX)\n",
    "            \n",
    "            # バイアス、重みの更新\n",
    "            # layer.B -= dB * self.lr\n",
    "            layer.B = layer.B - dB * self.lr\n",
    "            # layer.W -= dW * self.lr\n",
    "            layer.W = layer.W - dW * self.lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】1次元畳み込み後の出力サイズの計算\n",
    "畳み込みを行うと特徴量の数が変化します。どのように変化するかは以下の数式から求められます。パディングやストライドも含めています。この計算を行う関数を作成してください。\n",
    "$$\n",
    "N_{out} =  \\frac{N_{in}+2P-F}{S} + 1\\\\\n",
    "$$\n",
    "\n",
    "$N_{out}$ : 出力のサイズ（特徴量の数）\n",
    "\n",
    "$N_{in}$ : 入力のサイズ（特徴量の数）\n",
    "\n",
    "$P$ : ある方向へのパディングの数\n",
    "\n",
    "$F$ : フィルタのサイズ\n",
    "\n",
    "$S$ : ストライドのサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_output_nums(input_nums, paddings, filter_sizes, stride_sizes):\n",
    "    output_nums = (input_nums + 2*paddings - filter_sizes) // stride_sizes + 1\n",
    "    return output_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】小さな配列での1次元畳み込み層の実験\n",
    "次に示す小さな配列でフォワードプロパゲーションとバックプロパゲーションが正しく行えているか確認してください。\n",
    "\n",
    "入力x、重みw、バイアスbを次のようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.array([1,2,3,4])\n",
    "# w = np.array([3, 5, 7])\n",
    "# b = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer=None\n",
    "optimizer=SGD()\n",
    "input_channels =1\n",
    "filter_sizes=3\n",
    "stride_sizes = 1\n",
    "simple_conv1d = SimpleConv1d(input_channels, filter_sizes, stride_sizes, initializer, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3, 5, 7]), array([1]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_conv1d.W, simple_conv1d.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "フォワードプロパゲーションをすると出力は次のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.array([35, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35, 50])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# フォワードプロぱゲーション\n",
    "simple_conv1d.forward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にバックプロパゲーションを考えます。誤差は次のようであったとします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta_a = np.array([10, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バックプロパゲーションをすると次のような値になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta_b = np.array([30])\n",
    "# delta_w = np.array([50, 80, 110])\n",
    "# delta_x = np.array([30, 110, 170, 140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,) [30]\n",
      "(3,) [ 50  80 110]\n",
      "(4,) [ 30. 110. 170. 140.]\n"
     ]
    }
   ],
   "source": [
    "# バックプロパゲーション \n",
    "dA = np.array([10, 20])\n",
    "simple_conv1d.backward(dA, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装上の工夫\n",
    "畳み込みを実装する場合は、まずはfor文を重ねていく形で構いません。しかし、できるだけ計算は効率化させたいため、以下の式を一度に計算する方法を考えることにします。\n",
    "$$\n",
    "a_i = \\sum_{s=0}^{F-1}x_{(i+s)}w_s+b\n",
    "$$\n",
    "バイアス項は単純な足し算のため、重みの部分を見ます。\n",
    "$$\n",
    "\\sum_{s=0}^{F-1}x_{(i+s)}w_s\n",
    "$$\n",
    "これは、xの一部を取り出した配列とwの配列の内積です。具体的な状況を考えると、以下のようなコードで計算できます。この例では流れを分かりやすくするために、各要素同士でアダマール積を計算してから合計を計算しています。これは結果的に内積と同様です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.array([1, 2, 3, 4])\n",
    "# w = np.array([3, 5, 7])\n",
    "# a = np.empty((2, 3))\n",
    "# indexes0 = np.array([0, 1, 2]).astype(np.int)\n",
    "# indexes1 = np.array([1, 2, 3]).astype(np.int)\n",
    "# a[0] = x[indexes0]*w # x[indexes0]は([1, 2, 3])である\n",
    "# a[1] = x[indexes1]*w # x[indexes1]は([2, 3, 4])である\n",
    "# a = a.sum(axis=1) # array([34., 49.,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ndarrayは配列を使ったインデックス指定ができることを利用した方法です。\n",
    "\n",
    "また、二次元配列を使えば一次元配列から二次元配列が取り出せます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.array([1, 2, 3, 4])\n",
    "# indexes = np.array([[0, 1, 2], [1, 2, 3]]).astype(np.int)\n",
    "# print(x[indexes]) # ([[1, 2, 3], [2, 3, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このこととブロードキャストなどをうまく組み合わせることで、一度にまとめて計算することも可能です。\n",
    "\n",
    "畳み込みの計算方法に正解はないので、自分なりに効率化していってください。\n",
    "\n",
    "**《参考》**\n",
    "\n",
    "以下のページのInteger array indexingの部分がこの方法についての記述です。\n",
    "\n",
    "[Indexing — NumPy v1.17 Manual](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成\n",
    "チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    \"\"\"\n",
    "    チャンネル数を限定しない1次元畳み込み層\n",
    "    Parameters\n",
    "    ----------\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    input_channels : int\n",
    "      入力のチャンネル数\n",
    "    output_channels : int\n",
    "      出力のチャンネル数\n",
    "    filter _sizes : int\n",
    "      フィルタのサイズ\n",
    "    stride_sizes : int\n",
    "      スライドのサイズ\n",
    "    paddings : int\n",
    "      ある方向へのパディングする数\n",
    "    \"\"\"\n",
    "    \n",
    "    # def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "    def __init__(self, input_channels, output_channels, filter_sizes, stride_sizes, paddings, initializer, optimizer):\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.stride_sizes = stride_sizes\n",
    "        self.paddings = paddings\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        # self.W = initializer.W(input_channels, output_channels, strides)\n",
    "        # self.B = initializer.B(output_channels)\n",
    "        if initializer is None:\n",
    "            self.W = np.ones((self.output_channels, self.input_channels, self.filter_sizes))\n",
    "            self.B = np.arange(1, self.output_channels+1)\n",
    "            \n",
    "    def calc_output_nums(self, input_nums, paddings, filter_sizes, stride_sizes):\n",
    "        output_nums = (input_nums + 2*paddings - filter_sizes) // stride_sizes + 1\n",
    "        return output_nums\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (input_nums, n_features)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (output_channels, output_nums)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        print('フォワードプロパゲーション')\n",
    "        _, input_nums = X.shape\n",
    "        # 出力する特徴量数を計算\n",
    "        output_nums = self.calc_output_nums(input_nums, self.paddings, self.filter_sizes, self.stride_sizes)\n",
    "        \n",
    "        # インデックスを計算\n",
    "        indexes = np.array([[i for i in range(j * self.stride_sizes , self.filter_sizes + j * self.stride_sizes)]for j in range(output_nums)])\n",
    "        print(indexes)\n",
    "        # パディングの処理、Xにゼロを追加する\n",
    "        if self.paddings > 0:\n",
    "            X = np.pad(X, [(0, 0), (self.paddings, self.paddings)], 'constant')\n",
    "            print(X)\n",
    "            \n",
    "        # 畳み込みを実施\n",
    "        A = (X[:,indexes] * self.W[:, :, np.newaxis]).sum(axis=1).sum(axis=2) + self.B[:, np.newaxis]\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA, X):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (self.output_channels, output_nums)\n",
    "            後ろから流れてきた勾配\n",
    "        X : 次の形のndarray, shape (self.input_channles, input_nums)\n",
    "            入力データ\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # dZを計算\n",
    "        # dZ = np.dot(dA, self.W.T)\n",
    "        \n",
    "        # 更新\n",
    "        #  self = self.optimizer.update(self, dA, Z)\n",
    "        # return dZ\n",
    "        self = self.optimizer.update(self, dA, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD_kai:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.001):\n",
    "        self.lr = lr\n",
    "    def update(self, layer, dA, X):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        dA : 次の形のndarray, shape (self.output_channels, output_nums)\n",
    "            後ろから流れてきた勾配\n",
    "        X : 次の形のndarray, shape (self.input_channles, input_nums)\n",
    "            入力データ\n",
    "        \"\"\"\n",
    "        print('バックプロパゲーション ')\n",
    "        # 各勾配の計算\n",
    "        axis = dA.ndim-1\n",
    "        dB = dA.sum(axis=axis)\n",
    "        print('dB', dB.shape, dB)\n",
    "        \n",
    "        # 3　はfilter_size, 2は出力のサイズ(N_out)\n",
    "        N_out = dA.shape[-1]\n",
    "        print(N_out)\n",
    "        indexes = np.array([[i for i in range(j * layer.stride_sizes, (N_out+j) * layer.stride_sizes)] for j in range(layer.filter_sizes)])\n",
    "        print(indexes)\n",
    "        \n",
    "        # パディングの処理？　行わないとXの形が変わっている\n",
    "        if layer.paddings > 0:\n",
    "            X = np.pad(X, [(0, 0), (layer.paddings, layer.paddings)], 'constant')\n",
    "        \n",
    "        dW = np.dot(dA, X[:, indexes.T])\n",
    "        print('dW', dW.shape, dW)\n",
    "        \n",
    "        # 2はinput_channels, 4はfilter_sizes + N_out - 1\n",
    "        dX = np.zeros((layer.input_channels, layer.filter_sizes + N_out - 1))\n",
    "        # 重みを転置して、dAとの行列積をとり、再度転置\n",
    "        calc = np.dot(layer.W.T, dA).T\n",
    "        # 2はN_out\n",
    "        for i in range(N_out):\n",
    "            dX[:, i:i+layer.filter_sizes] += calc[i]\n",
    "        print('dX', dX.shape, dX)\n",
    "\n",
    "        \n",
    "        '''\n",
    "            dX = np.zeros(X.shape[0])\n",
    "            diff = X.shape[0] - dA.shape[0]\n",
    "            if diff > 0:\n",
    "                dA_trans = np.concatenate([np.array([0]*diff), dA, np.array([0]*diff)], axis=0)\n",
    "                # 重みの数\n",
    "                w_num = layer.W.shape[0]\n",
    "#                 print(diff)\n",
    "#                 print(w_num)\n",
    "#                 print(dA_trans)\n",
    "                for i in range(X.shape[0]):\n",
    "                    dX[i] = np.dot(dA_trans[i: w_num+i], layer.W[::-1])\n",
    "                print(dX.shape, dX)\n",
    "            \n",
    "            # バイアス、重みの更新\n",
    "            # layer.B -= dB * self.lr\n",
    "            layer.B = layer.B - dB * self.lr\n",
    "            # layer.W -= dW * self.lr\n",
    "            layer.W = layer.W - dW * self.lr\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = 2\n",
    "output_channels = 3\n",
    "filter_sizes = 3\n",
    "stride_sizes = 1\n",
    "paddings = 0\n",
    "initializer = None \n",
    "optimizer = SGD_kai()\n",
    "conv1d = Conv1d(input_channels, output_channels, filter_sizes, stride_sizes, paddings, initializer, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 初期化した重みの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       " \n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       " \n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.]]]),\n",
       " array([1, 2, 3]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d.W, conv1d.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1, 2, 3, 4], [2, 3, 4, 5]])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- パディングの処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4],\n",
       "       [2, 3, 4, 5]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if paddings > 0:\n",
    "    local_X = np.pad(X, [(0, 0), (paddings, paddings)], 'constant')\n",
    "else:\n",
    "    local_X = X\n",
    "local_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 出力数の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_output_nums(input_nums, paddings, filter_sizes, stride_sizes):\n",
    "    output_nums = (input_nums + 2*paddings - filter_sizes) // stride_sizes + 1\n",
    "    return output_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_out = calc_output_nums(input_nums=4, paddings=paddings, filter_sizes=3, stride_sizes=1)\n",
    "N_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 畳み込み計算するインデックスの取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [1, 2, 3]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes = np.array([[i for i in range(j, filter_sizes+j)] for j in range(N_out)])\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[1, 2, 3],\n",
       "         [2, 3, 4]],\n",
       " \n",
       "        [[2, 3, 4],\n",
       "         [3, 4, 5]]]),\n",
       " (2, 2, 3))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_X[:,indexes], local_X[:,indexes].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d.W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 畳み込み計算の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[[1., 2., 3.],\n",
       "          [2., 3., 4.]],\n",
       " \n",
       "         [[2., 3., 4.],\n",
       "          [3., 4., 5.]]],\n",
       " \n",
       " \n",
       "        [[[1., 2., 3.],\n",
       "          [2., 3., 4.]],\n",
       " \n",
       "         [[2., 3., 4.],\n",
       "          [3., 4., 5.]]],\n",
       " \n",
       " \n",
       "        [[[1., 2., 3.],\n",
       "          [2., 3., 4.]],\n",
       " \n",
       "         [[2., 3., 4.],\n",
       "          [3., 4., 5.]]]]),\n",
       " (3, 2, 2, 3))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(local_X[:,indexes] * conv1d.W[:, :, np.newaxis]), (local_X[:,indexes] * conv1d.W[:, :, np.newaxis]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 畳み込み計算で残したい軸以外を合計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16., 22.],\n",
       "       [17., 23.],\n",
       "       [18., 24.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(local_X[:,indexes] * conv1d.W[:, :, np.newaxis]).sum(axis=1).sum(axis=2) + conv1d.B[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "フォワードプロパゲーション\n",
      "[[0 1 2]\n",
      " [1 2 3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[16., 22.],\n",
       "       [17., 23.],\n",
       "       [18., 24.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# クラスで実行確認\n",
    "A = conv1d.forward(X)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dAはとりあえず上記で計算したAを使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "フォワードプロパゲーション\n",
      "[[0 1 2]\n",
      " [1 2 3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[16., 22.],\n",
       "        [17., 23.],\n",
       "        [18., 24.]]),\n",
       " (3, 2))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dA = conv1d.forward(X)\n",
    "dA, dA.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dBを計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "フォワードプロパゲーション\n",
      "[[0 1 2]\n",
      " [1 2 3]]\n",
      "フォワードプロパゲーション\n",
      "[[0 1 2]\n",
      " [1 2 3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([38., 40., 42.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dAの最後の軸に沿って合計する\n",
    "axis = conv1d.forward(X).ndim-1\n",
    "dB = conv1d.forward(X).sum(axis=axis)\n",
    "dB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dWを計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 2, 3, 4],\n",
       "        [2, 3, 4, 5]]),\n",
       " (2, 4))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_X, local_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]]),\n",
       " (3, 2))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3　はfilter_size, 2は出力のサイズ(N_out)\n",
    "indexes = np.array([[i for i in range(j, N_out+j)] for j in range(filter_sizes)])\n",
    "indexes, indexes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 1, 2],\n",
       "        [1, 2, 3]]),\n",
       " (2, 3))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes.T, indexes.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[1, 2, 3],\n",
       "         [2, 3, 4]],\n",
       " \n",
       "        [[2, 3, 4],\n",
       "         [3, 4, 5]]]),\n",
       " (2, 2, 3))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_X[:, indexes.T], (local_X[:, indexes.T]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 60.,  98., 136.],\n",
       "         [ 98., 136., 174.]],\n",
       " \n",
       "        [[ 63., 103., 143.],\n",
       "         [103., 143., 183.]],\n",
       " \n",
       "        [[ 66., 108., 150.],\n",
       "         [108., 150., 192.]]]),\n",
       " (3, 2, 3))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dW　の計算\n",
    "np.dot(dA, local_X[:, indexes.T]), np.dot(dA, local_X[:, indexes.T]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "バックプロパゲーション \n",
      "dB (3,) [38. 40. 42.]\n",
      "2\n",
      "[[0 1]\n",
      " [1 2]\n",
      " [2 3]]\n",
      "dW (3, 2, 3) [[[ 60.  98. 136.]\n",
      "  [ 98. 136. 174.]]\n",
      "\n",
      " [[ 63. 103. 143.]\n",
      "  [103. 143. 183.]]\n",
      "\n",
      " [[ 66. 108. 150.]\n",
      "  [108. 150. 192.]]]\n",
      "dX (2, 4) [[ 51. 120. 120.  69.]\n",
      " [ 51. 120. 120.  69.]]\n"
     ]
    }
   ],
   "source": [
    "conv1d.backward(dA, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dXを計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5]],\n",
       " \n",
       "        [[ 6,  7,  8],\n",
       "         [ 9, 10, 11]],\n",
       " \n",
       "        [[12, 13, 14],\n",
       "         [15, 16, 17]]]),\n",
       " (3, 2, 3))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters = np.arange(output_channels*input_channels*filter_sizes).reshape(output_channels, input_channels, filter_sizes)\n",
    "filters, filters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[16., 22.],\n",
       "        [17., 23.],\n",
       "        [18., 24.]]),\n",
       " (3, 2))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dA, dA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  6, 12],\n",
       "        [ 3,  9, 15]],\n",
       "\n",
       "       [[ 1,  7, 13],\n",
       "        [ 4, 10, 16]],\n",
       "\n",
       "       [[ 2,  8, 14],\n",
       "        [ 5, 11, 17]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[318., 369., 420.],\n",
       "        [471., 522., 573.]],\n",
       "\n",
       "       [[426., 495., 564.],\n",
       "        [633., 702., 771.]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc = np.dot(filters.T, dA).T\n",
    "calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 318.,  795.,  915.,  564.],\n",
       "       [ 471., 1155., 1275.,  771.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2はinput_channels, 4はfilter_sizes + N_out - 1\n",
    "dX = np.zeros((input_channels, filter_sizes+N_out-1))\n",
    "# 重みを転置して、dAとの行列積をとり、再度転置\n",
    "calc = np.dot(filters.T, dA).T\n",
    "# 2はN_out\n",
    "for i in range(N_out):\n",
    "    dX[:, i:i+filter_sizes] += calc[i]\n",
    "dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 51., 120., 120.,  69.],\n",
       "       [ 51., 120., 120.,  69.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2はinput_channels, 4はfilter_sizes + N_out - 1\n",
    "dX = np.zeros((input_channels, filter_sizes+N_out-1))\n",
    "# 重みを転置して、dAとの行列積をとり、再度転置\n",
    "calc = np.dot(conv1d.W.T, dA).T\n",
    "# 2はN_out\n",
    "for i in range(N_out):\n",
    "    dX[:, i:i+filter_sizes] += calc[i]\n",
    "dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 30,  50,  70],\n",
       "       [ 60, 100, 140]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([3, 5, 7])\n",
    "dA_ = np.array([10, 20])\n",
    "w * dA_[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 30,  50,  70],\n",
       "       [ 60, 100, 140]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = w[:, np.newaxis] @ dA_[np.newaxis]\n",
    "tmp.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3, 4])\n",
    "w = np.array([3, 5, 7])\n",
    "a = np.empty((2, 3))\n",
    "indexes0 = np.array([0, 1, 2]).astype(np.int)\n",
    "indexes1 = np.array([1, 2, 3]).astype(np.int)\n",
    "a[0] = x[indexes0]*w # x[indexes0]は([1, 2, 3])である\n",
    "a[1] = x[indexes1]*w # x[indexes1]は([2, 3, 4])である\n",
    "# a = a.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3., 10., 21.],\n",
       "       [ 6., 15., 28.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[1., 2., 3.],\n",
       "         [2., 3., 4.]],\n",
       "\n",
       "        [[1., 2., 3.],\n",
       "         [2., 3., 4.]],\n",
       "\n",
       "        [[1., 2., 3.],\n",
       "         [2., 3., 4.]]],\n",
       "\n",
       "\n",
       "       [[[2., 3., 4.],\n",
       "         [3., 4., 5.]],\n",
       "\n",
       "        [[2., 3., 4.],\n",
       "         [3., 4., 5.]],\n",
       "\n",
       "        [[2., 3., 4.],\n",
       "         [3., 4., 5.]]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d.W * (X[:, 0:3][np.newaxis], X[:, 1:4][np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15., 21.],\n",
       "       [15., 21.],\n",
       "       [15., 21.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = (conv1d.W * (X[:, 0:3][np.newaxis], X[:, 1:4][np.newaxis])).sum(axis=0).sum(axis=2)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 19,  88],\n",
       "       [109, 214],\n",
       "       [199, 340]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp2 = (filters * (X[:, 0:3][np.newaxis], X[:, 1:4][np.newaxis])).sum(axis=0).sum(axis=2)\n",
    "tmp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "class ScratchDeepNeuralNetworkClassifier():\n",
    "    \"\"\"\n",
    "    多層なニューラルネットワーク分類器\n",
    "    Parameters\n",
    "    ----------\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=False, random_state=None, activation='relu', epoch=10, batch_size=20, lr=0.0001, hidden_layer_sizes=(400, 200,), initialize=None, sigma=None, optimize='adagrad'):\n",
    "        self.verbose = verbose\n",
    "        # 学習率\n",
    "        self.lr = lr\n",
    "        # 層のノード数のタプル\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        # バッチサイズ\n",
    "        self.batch_size = batch_size\n",
    "        # epochの回数\n",
    "        self.epoch = epoch\n",
    "        # 活性化関数の名前　'sigmoid', 'tanh', 'relu'\n",
    "        self.activation = activation\n",
    "        # 乱数の設定\n",
    "        self.random_state = random_state\n",
    "        # 初期値の設定方法 'gauss', 'xavier', 'he'\n",
    "        self.initialize = initialize\n",
    "        # 初期値をガウス分布で定める際の標準偏差\n",
    "        self.sigma = sigma\n",
    "        # 最適化手法 'sgd', 'adagrad'\n",
    "        self.optimize = optimize\n",
    "        \n",
    "        # 出力クラス数\n",
    "        self.n_output = None\n",
    "        # 出力カテゴリの配列\n",
    "        self.categories_ = None\n",
    "        # 重みの情報リスト\n",
    "        self.coefs_ = None\n",
    "        self.intercepts_ = None\n",
    "        # epoch毎に損失を記録\n",
    "        self.losses = np.zeros(self.epoch)\n",
    "        self.val_losses = None\n",
    "        # fitの時にvalデータがあるフラグ\n",
    "        self.val_flag = None    \n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y_train : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        # random_stateが設定されている場合はseed設定\n",
    "        if type(self.random_state) == int:\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        # 変数情報\n",
    "        self.n_samples, self.n_features = X_train.shape\n",
    "        # valデータの確認\n",
    "        if type(X_val) == np.ndarray and type(y_val) == np.ndarray:\n",
    "            if X_val.shape == (y_val.shape[0], self.n_features):\n",
    "                self.val_flag = True\n",
    "                self.val_losses = np.zeros(self.epoch)\n",
    "        \n",
    "        # 目的変数をone_hot_encoding\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "        if self.val_flag:\n",
    "            y_val_one_hot = enc.transform(y_val[:, np.newaxis])\n",
    "        \n",
    "        # 出力カテゴリの配列\n",
    "        self.categories_ = enc.categories_[0]\n",
    "        # 出力クラス数を特定\n",
    "        self.n_output = len(self.categories_)\n",
    "        \n",
    "        # 初期化のインスタンス作成\n",
    "        # initializeで指定がない時、sigmoid, tanhではxavier, reluではhe\n",
    "        if self.initialize is None:\n",
    "            if self.activation == 'relu':\n",
    "                self.initialize = 'he'\n",
    "            elif self.activation == 'sigmoid' or self.activation == 'tanh':\n",
    "                self.initialize = 'xavier'\n",
    "        # self.initializeに応じてinitializerのインスタンスを作成\n",
    "        if self.initialize == 'he':\n",
    "            initializer = HeInitializer()\n",
    "        elif self.initialize == 'xavier':\n",
    "            initializer = XavierInitializer()\n",
    "        elif self.initialize == 'gauss':\n",
    "            if self.sigma is None:\n",
    "                self.sigma = 0.01\n",
    "            initializer = SimpleInitializer(self.sigma)\n",
    "        \n",
    "        # 最適化のインスタンス作成\n",
    "        if self.optimize == 'adagrad':\n",
    "            optimizer = AdaGrad(self.lr)\n",
    "        elif self.optimize == 'sgd':\n",
    "            optimizer = SGD(self.lr)\n",
    "        \n",
    "        # 重みを初期化し、インスタンスをリストへ\n",
    "        fc_lst = []\n",
    "        row = self.n_features\n",
    "        for n_nodes in list(self.hidden_layer_sizes) + [self.n_output]:\n",
    "            fc = FC(row, n_nodes, initializer, optimizer)\n",
    "            \n",
    "            # self.optimize　が 'adagrad' の際、インスタンスへH_W, H_Bを持たせる\n",
    "            if self.optimize == 'adagrad':\n",
    "                fc.adagrad_initialize(row, n_nodes)\n",
    "            \n",
    "            fc_lst.append(fc)\n",
    "            row = n_nodes\n",
    "            \n",
    "        # 活性化関数のインスタンス\n",
    "        if self.activation == 'relu':\n",
    "            self.act = ReLU()\n",
    "        elif self.activation == 'sigmoid':\n",
    "            self.act = Sigmoid()\n",
    "        elif self.activation == 'tanh':\n",
    "            self.act = Tanh()\n",
    "        else:\n",
    "            print('not proper activation name')\n",
    "        \n",
    "        # epoch　でループ\n",
    "        for n_epoch in range(self.epoch):\n",
    "            # 各々のバッチの損失の記録\n",
    "            batches_losses = np.zeros(self.n_samples // self.batch_size, dtype=np.float64)\n",
    "            if self.val_flag:\n",
    "                batches_val_losses = np.zeros(self.n_samples // self.batch_size, dtype=np.float64)\n",
    "            \n",
    "            # バッチを取り出し、バッチでループ\n",
    "            get_mini_batch = GetMiniBatch(X_train, y_train_one_hot, batch_size=self.batch_size)\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                \n",
    "                # forward propagation\n",
    "                # 全結合層の結果をリストで管理、ex [0, A_1, A_2]\n",
    "                A_lst = [0]\n",
    "                # 、活性化の結果をリストで管理、ex [X, Z_1, Z_2]\n",
    "                Z_lst = [mini_X_train]\n",
    "                Z = mini_X_train\n",
    "                if self.val_flag:\n",
    "                    Z_val = X_val\n",
    "                \n",
    "                # 各層をループ\n",
    "                for i in range(len(self.hidden_layer_sizes)):\n",
    "                    \n",
    "                    A = fc_lst[i].forward(Z)\n",
    "                    # actでZを計算\n",
    "                    Z = self.act.forward(A)\n",
    "                    \n",
    "                    if self.val_flag:\n",
    "                        A_val = fc_lst[i].forward(Z_val)\n",
    "                        Z_val = self.act.forward(A_val)\n",
    "                    \n",
    "                    # A, Z　をdeepcopyしてリストへ保管　（deepcopy必要？）\n",
    "                    A_lst.append(deepcopy(A))\n",
    "                    Z_lst.append(deepcopy(Z))\n",
    "                \n",
    "                # 最後の出力層、forward_propagation\n",
    "                # 最終層の活性化関数のインスタンス\n",
    "                self.last_act = Softmax(mini_y_train)\n",
    "                A = fc_lst[i+1].forward(Z)\n",
    "                Z = self.last_act.forward(A)\n",
    "                \n",
    "                if self.val_flag:\n",
    "                    last_act_val = Softmax(y_val_one_hot)\n",
    "                    A_val = fc_lst[i+1].forward(Z_val)\n",
    "                    Z_val = last_act_val.forward(A_val) \n",
    "\n",
    "                # 最後の出力層、back_propagation\n",
    "                # 最終層のAをもとに、dA　と　損失を求める（損失は記録）\n",
    "                dA, batches_losses[get_mini_batch._counter - 1] = self.last_act.backward(A)\n",
    "                if self.val_flag:\n",
    "                    _, batches_val_losses[get_mini_batch._counter - 1] = last_act_val.backward(A_val)\n",
    "                \n",
    "                # back propagation\n",
    "                # fc, A, Zを逆順で取り出す\n",
    "                for fc, A, Z in zip(fc_lst[::-1], A_lst[::-1], Z_lst[::-1]):\n",
    "                    # dZ の勾配を求める、関数内でW、Bを更新\n",
    "                    dZ = fc.backward(dA, Z)\n",
    "                    # dAを求める、dZ　と　活性化関数のアダマール積、最後のdAはゼロになる\n",
    "                    dA = dZ * self.act.backward(A)\n",
    "  \n",
    "            #　バッチのループが終わったら損失を合計し、記録\n",
    "            self.losses[n_epoch] = batches_losses.sum()\n",
    "            if self.val_flag:\n",
    "                self.val_losses[n_epoch] = batches_val_losses.sum()\n",
    "        \n",
    "            #verboseをTrueにした際は学習過程などを出力する\n",
    "            if self.verbose is True:\n",
    "                print('epoch : {} finished'.format(n_epoch))\n",
    "                print('train_loss : {}'.format(self.losses[n_epoch]))\n",
    "                if self.val_flag:\n",
    "                    print('val_loss : {}'.format(self.val_losses[n_epoch]))\n",
    "        \n",
    "        # 学習が終わったら重み、バイアスを変数へ記録\n",
    "        self.coefs_ = [fc.W for fc in fc_lst]\n",
    "        self.intercepts_ = [fc.B for fc in fc_lst]\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        # 活性化関数の出力結果をpredで管理\n",
    "        pred = X\n",
    "\n",
    "        # 各層をループ\n",
    "        for n_layer in range(len(self.hidden_layer_sizes)):\n",
    "            A = np.dot(pred, self.coefs_[n_layer]) + self.intercepts_[n_layer]\n",
    "            pred = self.act.forward(A)\n",
    "\n",
    "        # 最後の出力層\n",
    "        A = np.dot(pred, self.coefs_[-1]) + self.intercepts_[-1]\n",
    "        # 最後の活性化関数\n",
    "        pred = self.last_act.forward(A)\n",
    "\n",
    "        # 列、横方向に最大のインデックスを取得し、出力カテゴリのself.categories_の値を返す\n",
    "        return self.categories_[np.argmax(pred, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma=0.01):\n",
    "        self.sigma = sigma\n",
    "    def W(self, input_channels, output_channels, strides):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(output_channels, input_channels, self.n_features-1)\n",
    "        return W\n",
    "    def B(self, output_channels):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        output_channels : int\n",
    "          出力のチャンネル数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(output_channels)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_channel : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "    \n",
    "    # optimazer がadagradの際に呼び出す\n",
    "    def adagrad_initialize(self, n_nodes1, n_nodes2):\n",
    "        self.H_W = np.ones(n_nodes1 * n_nodes2).reshape(n_nodes1, -1)\n",
    "        self.H_B = np.ones(n_nodes2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        return A\n",
    "    def backward(self, dA, Z):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            一層前の活性化関数の出力結果\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # dZを計算\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self, dA, Z)\n",
    "        return dZ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
